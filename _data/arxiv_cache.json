{
  "timestamp": 1754358394,
  "papers": [
    {
      "title": "Formal Bayesian Transfer Learning via the Total Risk Prior",
      "authors": [
        "Nathan Wycoff",
        "Ali Arab",
        "Lisa O. Singh"
      ],
      "abstract": "In analyses with severe data-limitations, augmenting the target dataset with\ninformation from ancillary datasets in the application domain, called source\ndatasets, can lead to significantly improved statistical procedures. However,\nexisting methods for this transfer learning struggle to deal with situations\nwhere the source datasets are also limited and not guaranteed to be\nwell-aligned with the target dataset. A typical strategy is to use the\nempirical loss minimizer on the source data as a prior mean for the target\nparameters, which places the estimation of source parameters outside of the\nBayesian formalism. Our key conceptual contribution is to use a risk minimizer\nconditional on source parameters instead. This allows us to construct a single\njoint prior distribution for all parameters from the source datasets as well as\nthe target dataset. As a consequence, we benefit from full Bayesian uncertainty\nquantification and can perform model averaging via Gibbs sampling over\nindicator variables governing the inclusion of each source dataset. We show how\na particular instantiation of our prior leads to a Bayesian Lasso in a\ntransformed coordinate system and discuss computational techniques to scale our\napproach to moderately sized datasets. We also demonstrate that recently\nproposed minimax-frequentist transfer learning techniques may be viewed as an\napproximate Maximum a Posteriori approach to our model. Finally, we demonstrate\nsuperior predictive performance relative to the frequentist baseline on a\ngenetics application, especially when the source data are limited.",
      "published": "July 31, 2025",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "pdf_link": "http://arxiv.org/pdf/2507.23768v1",
      "arxiv_url": "http://arxiv.org/abs/2507.23768v1"
    },
    {
      "title": "On the Interaction of Compressibility and Adversarial Robustness",
      "authors": [
        "Melih Barsbey",
        "Ant\u00f4nio H. Ribeiro",
        "Umut \u015eim\u015fekli",
        "Tolga Birdal"
      ],
      "abstract": "Modern neural networks are expected to simultaneously satisfy a host of\ndesirable properties: accurate fitting to training data, generalization to\nunseen inputs, parameter and computational efficiency, and robustness to\nadversarial perturbations. While compressibility and robustness have each been\nstudied extensively, a unified understanding of their interaction still remains\nelusive. In this work, we develop a principled framework to analyze how\ndifferent forms of compressibility - such as neuron-level sparsity and spectral\ncompressibility - affect adversarial robustness. We show that these forms of\ncompression can induce a small number of highly sensitive directions in the\nrepresentation space, which adversaries can exploit to construct effective\nperturbations. Our analysis yields a simple yet instructive robustness bound,\nrevealing how neuron and spectral compressibility impact $L_\\infty$ and $L_2$\nrobustness via their effects on the learned representations. Crucially, the\nvulnerabilities we identify arise irrespective of how compression is achieved -\nwhether via regularization, architectural bias, or implicit learning dynamics.\nThrough empirical evaluations across synthetic and realistic tasks, we confirm\nour theoretical predictions, and further demonstrate that these vulnerabilities\npersist under adversarial training and transfer learning, and contribute to the\nemergence of universal adversarial perturbations. Our findings show a\nfundamental tension between structured compressibility and robustness, and\nsuggest new pathways for designing models that are both efficient and secure.",
      "published": "July 23, 2025",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "stat.ML"
      ],
      "pdf_link": "http://arxiv.org/pdf/2507.17725v1",
      "arxiv_url": "http://arxiv.org/abs/2507.17725v1"
    },
    {
      "title": "Causal Mechanism Estimation in Multi-Sensor Systems Across Multiple\n  Domains",
      "authors": [
        "Jingyi Yu",
        "Tim Pychynski",
        "Marco F. Huber"
      ],
      "abstract": "To gain deeper insights into a complex sensor system through the lens of\ncausality, we present common and individual causal mechanism estimation\n(CICME), a novel three-step approach to inferring causal mechanisms from\nheterogeneous data collected across multiple domains. By leveraging the\nprinciple of Causal Transfer Learning (CTL), CICME is able to reliably detect\ndomain-invariant causal mechanisms when provided with sufficient samples. The\nidentified common causal mechanisms are further used to guide the estimation of\nthe remaining causal mechanisms in each domain individually. The performance of\nCICME is evaluated on linear Gaussian models under scenarios inspired from a\nmanufacturing process. Building upon existing continuous optimization-based\ncausal discovery methods, we show that CICME leverages the benefits of applying\ncausal discovery on the pooled data and repeatedly on data from individual\ndomains, and it even outperforms both baseline methods under certain scenarios.",
      "published": "July 23, 2025",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_link": "http://arxiv.org/pdf/2507.17792v2",
      "arxiv_url": "http://arxiv.org/abs/2507.17792v2"
    },
    {
      "title": "Sufficiency-principled Transfer Learning via Model Averaging",
      "authors": [
        "Xiyuan Zhang",
        "Huihang Liu",
        "Xinyu Zhang"
      ],
      "abstract": "When the transferable set is unknowable, transfering informative knowledge as\nmuch as possible\\textemdash a principle we refer to as \\emph{sufficiency},\nbecomes crucial for enhancing transfer learning effectiveness. However,\nexisting transfer learning methods not only overlook the sufficiency principle,\nbut also rely on restrictive single-similarity assumptions (\\eg individual or\ncombinatorial similarity), leading to suboptimal performance. To address these\nlimitations, we propose a sufficiency-principled transfer learning framework\nvia unified model averaging algorithms, accommodating both individual and\ncombinatorial similarities. Theoretically, we establish the\nasymptotic/high-probability optimality, enhanced convergence rate and\nasymptotic normality for multi-source linear regression models with a diverging\nnumber of parameters, achieving sufficiency, robustness to negative transfer,\nprivacy protection and feasible statistical inference. Extensive simulations\nand an empirical data analysis of Beijing housing rental data demonstrate the\npromising superiority of our framework over conventional alternatives.",
      "published": "July 21, 2025",
      "categories": [
        "stat.ME",
        "math.ST",
        "stat.TH"
      ],
      "pdf_link": "http://arxiv.org/pdf/2507.15416v1",
      "arxiv_url": "http://arxiv.org/abs/2507.15416v1"
    },
    {
      "title": "Parameter-transfer in spatial autoregressive models via model averaging",
      "authors": [
        "Fen Jiang",
        "Wenhui Li",
        "Xinyu Zhang"
      ],
      "abstract": "Econometric modeling in spatial autoregressive models often suffers from\ninsufficient samples in practice, such as spatial analysis of infectious\ndiseases at the country level with limited data. Transfer learning offers a\npromising solution by leveraging information from regions or domains with\nsimilar spatial spillover effects to improve the analysis of the target data.\nIn this paper, we propose a parameter-transfer approach based on Mallows model\naveraging for spatial autoregressive models to improve the prediction accuracy.\nOur approach does not require sharing multi-source spatial data and can be\ncombined with various parameter estimation methods, such as the maximum\nlikelihood and the two-stage least squares. Theoretical analyses demonstrate\nthat our method achieves asymptotic optimality and ensures weight convergence\nwith an explicit convergence rate. Simulation studies and the application of\ninfection count prediction in Africa further demonstrate the effectiveness of\nour approach.",
      "published": "July 19, 2025",
      "categories": [
        "stat.ME"
      ],
      "pdf_link": "http://arxiv.org/pdf/2507.14453v1",
      "arxiv_url": "http://arxiv.org/abs/2507.14453v1"
    }
  ]
}