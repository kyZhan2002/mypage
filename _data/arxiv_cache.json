{
  "timestamp": 1745976149,
  "papers": [
    {
      "title": "Transfer Learning Under High-Dimensional Network Convolutional\n  Regression Model",
      "authors": [
        "Liyuan Wang",
        "Jiachen Chen",
        "Kathryn L. Lunetta",
        "Danyang Huang",
        "Huimin Cheng",
        "Debarghya Mukherjee"
      ],
      "abstract": "Transfer learning enhances model performance by utilizing knowledge from\nrelated domains, particularly when labeled data is scarce. While existing\nresearch addresses transfer learning under various distribution shifts in\nindependent settings, handling dependencies in networked data remains\nchallenging. To address this challenge, we propose a high-dimensional transfer\nlearning framework based on network convolutional regression (NCR), inspired by\nthe success of graph convolutional networks (GCNs). The NCR model incorporates\nrandom network structure by allowing each node's response to depend on its\nfeatures and the aggregated features of its neighbors, capturing local\ndependencies effectively. Our methodology includes a two-step transfer learning\nalgorithm that addresses domain shift between source and target networks, along\nwith a source detection mechanism to identify informative domains.\nTheoretically, we analyze the lasso estimator in the context of a random graph\nbased on the Erdos-Renyi model assumption, demonstrating that transfer learning\nimproves convergence rates when informative sources are present. Empirical\nevaluations, including simulations and a real-world application using Sina\nWeibo data, demonstrate substantial improvements in prediction accuracy,\nparticularly when labeled data in the target domain is limited.",
      "published": "April 28, 2025",
      "categories": [
        "cs.LG",
        "stat.ME"
      ],
      "pdf_link": "http://arxiv.org/pdf/2504.19979v2",
      "arxiv_url": "http://arxiv.org/abs/2504.19979v2"
    },
    {
      "title": "Comments on the minimal training set for CNN: a case study of the\n  frustrated $J_1$-$J_2$ Ising model on the square lattice",
      "authors": [
        "Shang-Wei Li",
        "Yuan-Heng Tseng",
        "Ming-Che Hsieh",
        "Fu-Jiun Jiang"
      ],
      "abstract": "The minimal training set to train a working CNN is explored in detail. The\nconsidered model is the frustrated $J_1$-$J_2$ Ising model on the square\nlattice. Here $J_1 < 0$ and $J_2 > 0$ are the nearest and next-to-nearest\nneighboring couplings, respectively. We train the CNN using the configurations\nof $g \\stackrel{\\text{def}}{=} J_2/|J_1| = 0.7$ and employ the resulting CNN to\nstudy the phase transition of $g = 0.8$. We find that this transfer learning is\nsuccessful. In particular, only configurations of two temperatures, one is\nbelow and one is above the critical temperature $T_c$ of $g=0.7$, are needed to\nreach accurately determination of the $T_c$ of $g=0.8$. However, it may be\nsubtle to use this strategy for the training. Specifically, for the considered\nmodel, due to the inefficiency of the single spin flip algorithm used in\nsampling the configurations at the low-temperature region, the two temperatures\nassociated with the training set should not be too far away from the $T_c$ of\n$g=0.7$, otherwise, the performance of the obtained CNN is not of high quality,\nhence cannot determine the $T_c$ of $g=0.8$ accurately. For the considered\nmodel, we also uncover the condition for training a successful CNN when only\nconfigurations of two temperatures are considered as the training set.",
      "published": "April 28, 2025",
      "categories": [
        "hep-lat",
        "cond-mat.stat-mech"
      ],
      "pdf_link": "http://arxiv.org/pdf/2504.19795v1",
      "arxiv_url": "http://arxiv.org/abs/2504.19795v1"
    },
    {
      "title": "Post-Transfer Learning Statistical Inference in High-Dimensional\n  Regression",
      "authors": [
        "Nguyen Vu Khai Tam",
        "Cao Huyen My",
        "Vo Nguyen Le Duy"
      ],
      "abstract": "Transfer learning (TL) for high-dimensional regression (HDR) is an important\nproblem in machine learning, particularly when dealing with limited sample size\nin the target task. However, there currently lacks a method to quantify the\nstatistical significance of the relationship between features and the response\nin TL-HDR settings. In this paper, we introduce a novel statistical inference\nframework for assessing the reliability of feature selection in TL-HDR, called\nPTL-SI (Post-TL Statistical Inference). The core contribution of PTL-SI is its\nability to provide valid $p$-values to features selected in TL-HDR, thereby\nrigorously controlling the false positive rate (FPR) at desired significance\nlevel $\\alpha$ (e.g., 0.05). Furthermore, we enhance statistical power by\nincorporating a strategic divide-and-conquer approach into our framework. We\ndemonstrate the validity and effectiveness of the proposed PTL-SI through\nextensive experiments on both synthetic and real-world high-dimensional\ndatasets, confirming its theoretical properties and utility in testing the\nreliability of feature selection in TL scenarios.",
      "published": "April 25, 2025",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "pdf_link": "http://arxiv.org/pdf/2504.18212v1",
      "arxiv_url": "http://arxiv.org/abs/2504.18212v1"
    },
    {
      "title": "Transfer Learning for High-dimensional Reduced Rank Time Series Models",
      "authors": [
        "Mingliang Ma Abolfazl Safikhani"
      ],
      "abstract": "The objective of transfer learning is to enhance estimation and inference in\na target data by leveraging knowledge gained from additional sources. Recent\nstudies have explored transfer learning for independent observations in\ncomplex, high-dimensional models assuming sparsity, yet research on time series\nmodels remains limited. Our focus is on transfer learning for sequences of\nobservations with temporal dependencies and a more intricate model parameter\nstructure. Specifically, we investigate the vector autoregressive model (VAR),\na widely recognized model for time series data, where the transition matrix can\nbe deconstructed into a combination of a sparse matrix and a low-rank one. We\npropose a new transfer learning algorithm tailored for estimating\nhigh-dimensional VAR models characterized by low-rank and sparse structures.\nAdditionally, we present a novel approach for selecting informative\nobservations from auxiliary datasets. Theoretical guarantees are established,\nencompassing model parameter consistency, informative set selection, and the\nasymptotic distribution of estimators under mild conditions. The latter\nfacilitates the construction of entry-wise confidence intervals for model\nparameters. Finally, we demonstrate the empirical efficacy of our methodologies\nthrough both simulated and real-world datasets.",
      "published": "April 22, 2025",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "pdf_link": "http://arxiv.org/pdf/2504.15691v1",
      "arxiv_url": "http://arxiv.org/abs/2504.15691v1"
    },
    {
      "title": "TransST: Transfer Learning Embedded Spatial Factor Modeling of Spatial\n  Transcriptomics Data",
      "authors": [
        "Shuo Shuo Liu",
        "Shikun Wang",
        "Yuxuan Chen",
        "Anil K. Rustgi",
        "Ming Yuan",
        "Jianhua Hu"
      ],
      "abstract": "Background: Spatial transcriptomics have emerged as a powerful tool in\nbiomedical research because of its ability to capture both the spatial contexts\nand abundance of the complete RNA transcript profile in organs of interest.\nHowever, limitations of the technology such as the relatively low resolution\nand comparatively insufficient sequencing depth make it difficult to reliably\nextract real biological signals from these data. To alleviate this challenge,\nwe propose a novel transfer learning framework, referred to as TransST, to\nadaptively leverage the cell-labeled information from external sources in\ninferring cell-level heterogeneity of a target spatial transcriptomics data.\n  Results: Applications in several real studies as well as a number of\nsimulation settings show that our approach significantly improves existing\ntechniques. For example, in the breast cancer study, TransST successfully\nidentifies five biologically meaningful cell clusters, including the two\nsubgroups of cancer in situ and invasive cancer; in addition, only TransST is\nable to separate the adipose tissues from the connective issues among all the\nstudied methods.\n  Conclusions: In summary, the proposed method TransST is both effective and\nrobust in identifying cell subclusters and detecting corresponding driving\nbiomarkers in spatial transcriptomics data.",
      "published": "April 15, 2025",
      "categories": [
        "q-bio.GN",
        "cs.LG",
        "stat.AP",
        "stat.ML"
      ],
      "pdf_link": "http://arxiv.org/pdf/2504.12353v1",
      "arxiv_url": "http://arxiv.org/abs/2504.12353v1"
    },
    {
      "title": "Rank-based transfer learning for high-dimensional survival data with\n  application to sepsis data",
      "authors": [
        "Nan Qiao",
        "Haowei Jiang",
        "Cunjie Lin"
      ],
      "abstract": "Sepsis remains a critical challenge due to its high mortality and complex\nprognosis. To address data limitations in studying MSSA sepsis, we extend\nexisting transfer learning frameworks to accommodate transformation models for\nhigh-dimensional survival data. Specifically, we construct a measurement index\nbased on C-index for intelligently identifying the helpful source datasets, and\nthe target model performance is improved by leveraging information from the\nidentified source datasets via performing the transfer step and debiasing step.\nWe further provide an algorithm to construct confidence intervals for each\ncoefficient component. Another significant development is that statistical\nproperties are rigorously established, including $\\ell_1/\\ell_2$-estimation\nerror bounds of the transfer learning algorithm, detection consistency property\nof the transferable source detection algorithm and asymptotic theories for the\nconfidence interval construction. Extensive simulations and analysis of\nMIMIC-IV sepsis data demonstrate the estimation and prediction accuracy, and\npractical advantages of our approach, providing significant improvements in\nsurvival estimates for MSSA sepsis patients.",
      "published": "April 15, 2025",
      "categories": [
        "stat.AP"
      ],
      "pdf_link": "http://arxiv.org/pdf/2504.11270v1",
      "arxiv_url": "http://arxiv.org/abs/2504.11270v1"
    }
  ]
}