{
  "timestamp": 1743124719,
  "papers": [
    {
      "title": "Wasserstein Distributionally Robust Bayesian Optimization with\n  Continuous Context",
      "authors": [
        "Francesco Micheli",
        "Efe C. Balta",
        "Anastasios Tsiamis",
        "John Lygeros"
      ],
      "abstract": "We address the challenge of sequential data-driven decision-making under\ncontext distributional uncertainty. This problem arises in numerous real-world\nscenarios where the learner optimizes black-box objective functions in the\npresence of uncontrollable contextual variables. We consider the setting where\nthe context distribution is uncertain but known to lie within an ambiguity set\ndefined as a ball in the Wasserstein distance. We propose a novel algorithm for\nWasserstein Distributionally Robust Bayesian Optimization that can handle\ncontinuous context distributions while maintaining computational tractability.\nOur theoretical analysis combines recent results in self-normalized\nconcentration in Hilbert spaces and finite-sample bounds for distributionally\nrobust optimization to establish sublinear regret bounds that match\nstate-of-the-art results. Through extensive comparisons with existing\napproaches on both synthetic and real-world problems, we demonstrate the\nsimplicity, effectiveness, and practical applicability of our proposed method.",
      "published": "March 26, 2025",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "pdf_link": "http://arxiv.org/pdf/2503.20341v1",
      "arxiv_url": "http://arxiv.org/abs/2503.20341v1"
    },
    {
      "title": "Similarity-Informed Transfer Learning for Multivariate Functional\n  Censored Quantile Regression",
      "authors": [
        "Hua Liu",
        "Jiaqi Men",
        "Shouxia Wang",
        "Jinhong You",
        "Jiguo Cao"
      ],
      "abstract": "To address the challenge of utilizing patient data from other organ\ntransplant centers (source cohorts) to improve survival time estimation and\ninference for a target center (target cohort) with limited samples and strict\ndata-sharing privacy constraints, we propose the Similarity-Informed Transfer\nLearning (SITL) method. This approach estimates multivariate functional\ncensored quantile regression by flexibly leveraging information from each\nsource cohort based on its similarity to the target cohort. Furthermore, the\nmethod is adaptable to continuously updated real-time data. We establish the\nasymptotic properties of the estimators obtained using the SITL method,\ndemonstrating improved convergence rates. Additionally, we develop an enhanced\napproach that combines the SITL method with a resampling technique to construct\nmore accurate confidence intervals for functional coefficients, backed by\ntheoretical guarantees. Extensive simulation studies and an application to\nkidney transplant data illustrate the significant advantages of the SITL\nmethod. Compared to methods that rely solely on the target cohort or\nindiscriminately pool data across source and target cohorts, the SITL method\nsubstantially improves both estimation and inference performance.",
      "published": "March 24, 2025",
      "categories": [
        "stat.ME"
      ],
      "pdf_link": "http://arxiv.org/pdf/2503.18437v1",
      "arxiv_url": "http://arxiv.org/abs/2503.18437v1"
    },
    {
      "title": "An Optimization Framework for Differentially Private Sparse Fine-Tuning",
      "authors": [
        "Mehdi Makni",
        "Kayhan Behdin",
        "Gabriel Afriat",
        "Zheng Xu",
        "Sergei Vassilvitskii",
        "Natalia Ponomareva",
        "Hussein Hazimeh",
        "Rahul Mazumder"
      ],
      "abstract": "Differentially private stochastic gradient descent (DP-SGD) is broadly\nconsidered to be the gold standard for training and fine-tuning neural networks\nunder differential privacy (DP). With the increasing availability of\nhigh-quality pre-trained model checkpoints (e.g., vision and language models),\nfine-tuning has become a popular strategy. However, despite recent progress in\nunderstanding and applying DP-SGD for private transfer learning tasks,\nsignificant challenges remain -- most notably, the performance gap between\nmodels fine-tuned with DP-SGD and their non-private counterparts. Sparse\nfine-tuning on private data has emerged as an alternative to full-model\nfine-tuning; recent work has shown that privately fine-tuning only a small\nsubset of model weights and keeping the rest of the weights fixed can lead to\nbetter performance. In this work, we propose a new approach for sparse\nfine-tuning of neural networks under DP. Existing work on private sparse\nfinetuning often used fixed choice of trainable weights (e.g., updating only\nthe last layer), or relied on public model's weights to choose the subset of\nweights to modify. Such choice of weights remains suboptimal. In contrast, we\nexplore an optimization-based approach, where our selection method makes use of\nthe private gradient information, while using off the shelf privacy accounting\ntechniques. Our numerical experiments on several computer vision models and\ndatasets show that our selection method leads to better prediction accuracy,\ncompared to full-model private fine-tuning or existing private sparse\nfine-tuning approaches.",
      "published": "March 17, 2025",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_link": "http://arxiv.org/pdf/2503.12822v1",
      "arxiv_url": "http://arxiv.org/abs/2503.12822v1"
    }
  ]
}