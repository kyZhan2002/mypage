{
  "timestamp": 1754444702,
  "papers": [
    {
      "title": "Model Recycling Framework for Multi-Source Data-Free Supervised Transfer\n  Learning",
      "authors": [
        "Sijia Wang",
        "Ricardo Henao"
      ],
      "abstract": "Increasing concerns for data privacy and other difficulties associated with\nretrieving source data for model training have created the need for source-free\ntransfer learning, in which one only has access to pre-trained models instead\nof data from the original source domains. This setting introduces many\nchallenges, as many existing transfer learning methods typically rely on access\nto source data, which limits their direct applicability to scenarios where\nsource data is unavailable. Further, practical concerns make it more difficult,\nfor instance efficiently selecting models for transfer without information on\nsource data, and transferring without full access to the source models. So\nmotivated, we propose a model recycling framework for parameter-efficient\ntraining of models that identifies subsets of related source models to reuse in\nboth white-box and black-box settings. Consequently, our framework makes it\npossible for Model as a Service (MaaS) providers to build libraries of\nefficient pre-trained models, thus creating an opportunity for multi-source\ndata-free supervised transfer learning.",
      "published": "August 04, 2025",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_link": "http://arxiv.org/pdf/2508.02039v1",
      "arxiv_url": "http://arxiv.org/abs/2508.02039v1"
    },
    {
      "title": "Formal Bayesian Transfer Learning via the Total Risk Prior",
      "authors": [
        "Nathan Wycoff",
        "Ali Arab",
        "Lisa O. Singh"
      ],
      "abstract": "In analyses with severe data-limitations, augmenting the target dataset with\ninformation from ancillary datasets in the application domain, called source\ndatasets, can lead to significantly improved statistical procedures. However,\nexisting methods for this transfer learning struggle to deal with situations\nwhere the source datasets are also limited and not guaranteed to be\nwell-aligned with the target dataset. A typical strategy is to use the\nempirical loss minimizer on the source data as a prior mean for the target\nparameters, which places the estimation of source parameters outside of the\nBayesian formalism. Our key conceptual contribution is to use a risk minimizer\nconditional on source parameters instead. This allows us to construct a single\njoint prior distribution for all parameters from the source datasets as well as\nthe target dataset. As a consequence, we benefit from full Bayesian uncertainty\nquantification and can perform model averaging via Gibbs sampling over\nindicator variables governing the inclusion of each source dataset. We show how\na particular instantiation of our prior leads to a Bayesian Lasso in a\ntransformed coordinate system and discuss computational techniques to scale our\napproach to moderately sized datasets. We also demonstrate that recently\nproposed minimax-frequentist transfer learning techniques may be viewed as an\napproximate Maximum a Posteriori approach to our model. Finally, we demonstrate\nsuperior predictive performance relative to the frequentist baseline on a\ngenetics application, especially when the source data are limited.",
      "published": "July 31, 2025",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "pdf_link": "http://arxiv.org/pdf/2507.23768v1",
      "arxiv_url": "http://arxiv.org/abs/2507.23768v1"
    },
    {
      "title": "On the Interaction of Compressibility and Adversarial Robustness",
      "authors": [
        "Melih Barsbey",
        "Ant\u00f4nio H. Ribeiro",
        "Umut \u015eim\u015fekli",
        "Tolga Birdal"
      ],
      "abstract": "Modern neural networks are expected to simultaneously satisfy a host of\ndesirable properties: accurate fitting to training data, generalization to\nunseen inputs, parameter and computational efficiency, and robustness to\nadversarial perturbations. While compressibility and robustness have each been\nstudied extensively, a unified understanding of their interaction still remains\nelusive. In this work, we develop a principled framework to analyze how\ndifferent forms of compressibility - such as neuron-level sparsity and spectral\ncompressibility - affect adversarial robustness. We show that these forms of\ncompression can induce a small number of highly sensitive directions in the\nrepresentation space, which adversaries can exploit to construct effective\nperturbations. Our analysis yields a simple yet instructive robustness bound,\nrevealing how neuron and spectral compressibility impact $L_\\infty$ and $L_2$\nrobustness via their effects on the learned representations. Crucially, the\nvulnerabilities we identify arise irrespective of how compression is achieved -\nwhether via regularization, architectural bias, or implicit learning dynamics.\nThrough empirical evaluations across synthetic and realistic tasks, we confirm\nour theoretical predictions, and further demonstrate that these vulnerabilities\npersist under adversarial training and transfer learning, and contribute to the\nemergence of universal adversarial perturbations. Our findings show a\nfundamental tension between structured compressibility and robustness, and\nsuggest new pathways for designing models that are both efficient and secure.",
      "published": "July 23, 2025",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "stat.ML"
      ],
      "pdf_link": "http://arxiv.org/pdf/2507.17725v1",
      "arxiv_url": "http://arxiv.org/abs/2507.17725v1"
    },
    {
      "title": "Causal Mechanism Estimation in Multi-Sensor Systems Across Multiple\n  Domains",
      "authors": [
        "Jingyi Yu",
        "Tim Pychynski",
        "Marco F. Huber"
      ],
      "abstract": "To gain deeper insights into a complex sensor system through the lens of\ncausality, we present common and individual causal mechanism estimation\n(CICME), a novel three-step approach to inferring causal mechanisms from\nheterogeneous data collected across multiple domains. By leveraging the\nprinciple of Causal Transfer Learning (CTL), CICME is able to reliably detect\ndomain-invariant causal mechanisms when provided with sufficient samples. The\nidentified common causal mechanisms are further used to guide the estimation of\nthe remaining causal mechanisms in each domain individually. The performance of\nCICME is evaluated on linear Gaussian models under scenarios inspired from a\nmanufacturing process. Building upon existing continuous optimization-based\ncausal discovery methods, we show that CICME leverages the benefits of applying\ncausal discovery on the pooled data and repeatedly on data from individual\ndomains, and it even outperforms both baseline methods under certain scenarios.",
      "published": "July 23, 2025",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_link": "http://arxiv.org/pdf/2507.17792v2",
      "arxiv_url": "http://arxiv.org/abs/2507.17792v2"
    },
    {
      "title": "Sufficiency-principled Transfer Learning via Model Averaging",
      "authors": [
        "Xiyuan Zhang",
        "Huihang Liu",
        "Xinyu Zhang"
      ],
      "abstract": "When the transferable set is unknowable, transfering informative knowledge as\nmuch as possible\\textemdash a principle we refer to as \\emph{sufficiency},\nbecomes crucial for enhancing transfer learning effectiveness. However,\nexisting transfer learning methods not only overlook the sufficiency principle,\nbut also rely on restrictive single-similarity assumptions (\\eg individual or\ncombinatorial similarity), leading to suboptimal performance. To address these\nlimitations, we propose a sufficiency-principled transfer learning framework\nvia unified model averaging algorithms, accommodating both individual and\ncombinatorial similarities. Theoretically, we establish the\nasymptotic/high-probability optimality, enhanced convergence rate and\nasymptotic normality for multi-source linear regression models with a diverging\nnumber of parameters, achieving sufficiency, robustness to negative transfer,\nprivacy protection and feasible statistical inference. Extensive simulations\nand an empirical data analysis of Beijing housing rental data demonstrate the\npromising superiority of our framework over conventional alternatives.",
      "published": "July 21, 2025",
      "categories": [
        "stat.ME",
        "math.ST",
        "stat.TH"
      ],
      "pdf_link": "http://arxiv.org/pdf/2507.15416v1",
      "arxiv_url": "http://arxiv.org/abs/2507.15416v1"
    }
  ]
}