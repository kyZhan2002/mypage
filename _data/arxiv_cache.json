{
  "timestamp": 1749086741,
  "papers": [
    {
      "title": "Distributionally Robust Learning in Survival Analysis",
      "authors": [
        "Yeping Jin",
        "Lauren Wise",
        "Ioannis Paschalidis"
      ],
      "abstract": "We introduce an innovative approach that incorporates a Distributionally\nRobust Learning (DRL) approach into Cox regression to enhance the robustness\nand accuracy of survival predictions. By formulating a DRL framework with a\nWasserstein distance-based ambiguity set, we develop a variant Cox model that\nis less sensitive to assumptions about the underlying data distribution and\nmore resilient to model misspecification and data perturbations. By leveraging\nWasserstein duality, we reformulate the original min-max DRL problem into a\ntractable regularized empirical risk minimization problem, which can be\ncomputed by exponential conic programming. We provide guarantees on the finite\nsample behavior of our DRL-Cox model. Moreover, through extensive simulations\nand real world case studies, we demonstrate that our regression model achieves\nsuperior performance in terms of prediction accuracy and robustness compared\nwith traditional methods.",
      "published": "June 02, 2025",
      "categories": [
        "cs.LG",
        "math.OC",
        "stat.ML"
      ],
      "pdf_link": "http://arxiv.org/pdf/2506.01348v1",
      "arxiv_url": "http://arxiv.org/abs/2506.01348v1"
    },
    {
      "title": "Density Ratio Permutation Tests with connections to distributional\n  shifts and conditional two-sample testing",
      "authors": [
        "Alberto Bordino",
        "Thomas B. Berrett"
      ],
      "abstract": "We introduce novel hypothesis tests to allow for statistical inference for\ndensity ratios. More precisely, we introduce the Density Ratio Permutation Test\n(DRPT) for testing $H_0: g \\propto r f$ based on independent data drawn from\ndistributions with densities $f$ and $g$, where the hypothesised density ratio\n$r$ is a fixed function. The proposed test employs an efficient Markov Chain\nMonte Carlo algorithm to draw permutations of the combined dataset according to\na distribution determined by $r$, producing exchangeable versions of the whole\nsample and thereby establishing finite-sample validity. Regarding the test's\nbehaviour under the alternative hypothesis, we begin by demonstrating that if\nthe test statistic is chosen as an Integral Probability Metric (IPM), the DRPT\nis consistent under mild assumptions on the function class that defines the\nIPM. We then narrow our focus to the setting where the function class is a\nReproducing Kernel Hilbert Space, and introduce a generalisation of the\nclassical Maximum Mean Discrepancy (MMD), which we term Shifted-MMD. For\ncontinuous data, assuming that a normalised version of $g - rf$ lies in a\nSobolev ball, we establish the minimax optimality of the DRPT based on the\nShifted-MMD. We further extend our approach to scenarios with an unknown shift\nfactor $r$, estimating it from part of the data using Density Ratio Estimation\ntechniques, and derive Type-I error bounds based on estimation error.\nAdditionally, we demonstrate how the DRPT can be adapted for conditional\ntwo-sample testing, establishing it as a versatile tool for assessing modelling\nassumptions on importance weights, covariate shifts and related scenarios,\nwhich frequently arise in contexts such as transfer learning and causal\ninference. Finally, we validate our theoretical findings through experiments on\nboth simulated and real-world datasets.",
      "published": "May 30, 2025",
      "categories": [
        "stat.ME",
        "math.ST",
        "stat.TH",
        "62G09 62G10"
      ],
      "pdf_link": "http://arxiv.org/pdf/2505.24529v1",
      "arxiv_url": "http://arxiv.org/abs/2505.24529v1"
    },
    {
      "title": "Attractor learning for spatiotemporally chaotic dynamical systems using\n  echo state networks with transfer learning",
      "authors": [
        "Mohammad Shah Alam",
        "William Ott",
        "Ilya Timofeyev"
      ],
      "abstract": "In this paper, we explore the predictive capabilities of echo state networks\n(ESNs) for the generalized Kuramoto-Sivashinsky (gKS) equation, an archetypal\nnonlinear PDE that exhibits spatiotemporal chaos. We introduce a novel\nmethodology that integrates ESNs with transfer learning, aiming to enhance\npredictive performance across various parameter regimes of the gKS model. Our\nresearch focuses on predicting changes in long-term statistical patterns of the\ngKS model that result from varying the dispersion relation or the length of the\nspatial domain. We use transfer learning to adapt ESNs to different parameter\nsettings and successfully capture changes in the underlying chaotic attractor.",
      "published": "May 30, 2025",
      "categories": [
        "math.DS",
        "cs.AI",
        "cs.LG",
        "nlin.CD",
        "stat.ML",
        "37N99, 68T30"
      ],
      "pdf_link": "http://arxiv.org/pdf/2505.24099v1",
      "arxiv_url": "http://arxiv.org/abs/2505.24099v1"
    },
    {
      "title": "Epistemic Errors of Imperfect Multitask Learners When Distributions\n  Shift",
      "authors": [
        "Sabina J. Sloman",
        "Michele Caprio",
        "Samuel Kaski"
      ],
      "abstract": "When data are noisy, a statistical learner's goal is to resolve epistemic\nuncertainty about the data it will encounter at test-time, i.e., to identify\nthe distribution of test (target) data. Many real-world learning settings\nintroduce sources of epistemic uncertainty that can not be resolved on the\nbasis of training (source) data alone: The source data may arise from multiple\ntasks (multitask learning), the target data may differ systematically from the\nsource data tasks (distribution shift), and/or the learner may not arrive at an\naccurate characterization of the source data (imperfect learning). We introduce\na principled definition of epistemic error, and provide a generic,\ndecompositional epistemic error bound. Our error bound is the first to (i)\nconsider epistemic error specifically, (ii) accommodate all the sources of\nepistemic uncertainty above, and (iii) separately attribute the error to each\nof multiple aspects of the learning procedure and environment. As corollaries\nof the generic result, we provide (i) epistemic error bounds specialized to the\nsettings of Bayesian transfer learning and distribution shift within\n$\\epsilon$-neighborhoods, and (ii) a set of corresponding generalization\nbounds. Finally, we provide a novel definition of negative transfer, and\nvalidate its insights in a synthetic experimental setting.",
      "published": "May 29, 2025",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_link": "http://arxiv.org/pdf/2505.23496v1",
      "arxiv_url": "http://arxiv.org/abs/2505.23496v1"
    },
    {
      "title": "GLAMP: An Approximate Message Passing Framework for Transfer Learning\n  with Applications to Lasso-based Estimators",
      "authors": [
        "Longlin Wang",
        "Yanke Song",
        "Kuanhao Jiang",
        "Pragya Sur"
      ],
      "abstract": "Approximate Message Passing (AMP) algorithms enable precise characterization\nof certain classes of random objects in the high-dimensional limit, and have\nfound widespread applications in fields such as statistics, deep learning,\ngenetics, and communications. However, existing AMP frameworks cannot\nsimultaneously handle matrix-valued iterates and non-separable denoising\nfunctions. This limitation prevents them from precisely characterizing\nestimators that draw information from multiple data sources with distribution\nshifts. In this work, we introduce Generalized Long Approximate Message Passing\n(GLAMP), a novel extension of AMP that addresses this limitation. We rigorously\nprove state evolution for GLAMP. GLAMP significantly broadens the scope of AMP,\nenabling the analysis of transfer learning estimators that were previously out\nof reach. We demonstrate the utility of GLAMP by precisely characterizing the\nrisk of three Lasso-based transfer learning estimators: the Stacked Lasso, the\nModel Averaging Estimator, and the Second Step Estimator. We also demonstrate\nthe remarkable finite sample accuracy of our theory via extensive simulations.",
      "published": "May 28, 2025",
      "categories": [
        "math.ST",
        "stat.ML",
        "stat.TH"
      ],
      "pdf_link": "http://arxiv.org/pdf/2505.22594v1",
      "arxiv_url": "http://arxiv.org/abs/2505.22594v1"
    },
    {
      "title": "Distributionally Robust Deep Q-Learning",
      "authors": [
        "Chung I Lu",
        "Julian Sester",
        "Aijia Zhang"
      ],
      "abstract": "We propose a novel distributionally robust $Q$-learning algorithm for the\nnon-tabular case accounting for continuous state spaces where the state\ntransition of the underlying Markov decision process is subject to model\nuncertainty. The uncertainty is taken into account by considering the\nworst-case transition from a ball around a reference probability measure. To\ndetermine the optimal policy under the worst-case state transition, we solve\nthe associated non-linear Bellman equation by dualising and regularising the\nBellman operator with the Sinkhorn distance, which is then parameterized with\ndeep neural networks. This approach allows us to modify the Deep Q-Network\nalgorithm to optimise for the worst case state transition.\n  We illustrate the tractability and effectiveness of our approach through\nseveral applications, including a portfolio optimisation task based on\nS\\&{P}~500 data.",
      "published": "May 25, 2025",
      "categories": [
        "cs.LG",
        "math.OC",
        "q-fin.PM",
        "stat.ML"
      ],
      "pdf_link": "http://arxiv.org/pdf/2505.19058v1",
      "arxiv_url": "http://arxiv.org/abs/2505.19058v1"
    },
    {
      "title": "Linear Mixture Distributionally Robust Markov Decision Processes",
      "authors": [
        "Zhishuai Liu",
        "Pan Xu"
      ],
      "abstract": "Many real-world decision-making problems face the off-dynamics challenge: the\nagent learns a policy in a source domain and deploys it in a target domain with\ndifferent state transitions. The distributionally robust Markov decision\nprocess (DRMDP) addresses this challenge by finding a robust policy that\nperforms well under the worst-case environment within a pre-specified\nuncertainty set of transition dynamics. Its effectiveness heavily hinges on the\nproper design of these uncertainty sets, based on prior knowledge of the\ndynamics. In this work, we propose a novel linear mixture DRMDP framework,\nwhere the nominal dynamics is assumed to be a linear mixture model. In contrast\nwith existing uncertainty sets directly defined as a ball centered around the\nnominal kernel, linear mixture DRMDPs define the uncertainty sets based on a\nball around the mixture weighting parameter. We show that this new framework\nprovides a more refined representation of uncertainties compared to\nconventional models based on $(s,a)$-rectangularity and $d$-rectangularity,\nwhen prior knowledge about the mixture model is present. We propose a meta\nalgorithm for robust policy learning in linear mixture DRMDPs with general\n$f$-divergence defined uncertainty sets, and analyze its sample complexities\nunder three divergence metrics instantiations: total variation,\nKullback-Leibler, and $\\chi^2$ divergences. These results establish the\nstatistical learnability of linear mixture DRMDPs, laying the theoretical\nfoundation for future research on this new setting.",
      "published": "May 23, 2025",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO",
        "stat.ML"
      ],
      "pdf_link": "http://arxiv.org/pdf/2505.18044v1",
      "arxiv_url": "http://arxiv.org/abs/2505.18044v1"
    }
  ]
}