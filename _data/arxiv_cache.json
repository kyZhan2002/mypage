{
  "timestamp": 1758330944,
  "papers": [
    {
      "title": "Representation-Aware Distributionally Robust Optimization: A Knowledge\n  Transfer Framework",
      "authors": [
        "Zitao Wang",
        "Nian Si",
        "Molei Liu"
      ],
      "abstract": "We propose REpresentation-Aware Distributionally Robust Estimation (READ), a\nnovel framework for Wasserstein distributionally robust learning that accounts\nfor predictive representations when guarding against distributional shifts.\nUnlike classical approaches that treat all feature perturbations equally, READ\nembeds a multidimensional alignment parameter into the transport cost, allowing\nthe model to differentially discourage perturbations along directions\nassociated with informative representations. This yields robustness to feature\nvariation while preserving invariant structure. Our first contribution is a\ntheoretical foundation: we show that seminorm regularizations for linear\nregression and binary classification arise as Wasserstein distributionally\nrobust objectives, thereby providing tractable reformulations of READ and\nunifying a broad class of regularized estimators under the DRO lens. Second, we\nadopt a principled procedure for selecting the Wasserstein radius using the\ntechniques of robust Wasserstein profile inference. This further enables the\nconstruction of valid, representation-aware confidence regions for model\nparameters with distinct geometric features. Finally, we analyze the geometry\nof READ estimators as the alignment parameters vary and propose an optimization\nalgorithm to estimate the projection of the global optimum onto this solution\nsurface. This procedure selects among equally robust estimators while optimally\nconstructing a representation structure. We conclude by demonstrating the\neffectiveness of our framework through extensive simulations and a real-world\nstudy, providing a powerful robust estimation grounded in learning\nrepresentation.",
      "published": "September 11, 2025",
      "categories": [
        "stat.ME",
        "cs.LG"
      ],
      "pdf_link": "http://arxiv.org/pdf/2509.09371v1",
      "arxiv_url": "http://arxiv.org/abs/2509.09371v1"
    },
    {
      "title": "Robust and Adaptive Spectral Method for Representation Multi-Task\n  Learning with Contamination",
      "authors": [
        "Yian Huang",
        "Yang Feng",
        "Zhiliang Ying"
      ],
      "abstract": "Representation-based multi-task learning (MTL) improves efficiency by\nlearning a shared structure across tasks, but its practical application is\noften hindered by contamination, outliers, or adversarial tasks. Most existing\nmethods and theories assume a clean or near-clean setting, failing when\ncontamination is significant. This paper tackles representation MTL with an\nunknown and potentially large contamination proportion, while also allowing for\nheterogeneity among inlier tasks. We introduce a Robust and Adaptive Spectral\nmethod (RAS) that can distill the shared inlier representation effectively and\nefficiently, while requiring no prior knowledge of the contamination level or\nthe true representation dimension. Theoretically, we provide non-asymptotic\nerror bounds for both the learned representation and the per-task parameters.\nThese bounds adapt to inlier task similarity and outlier structure, and\nguarantee that RAS performs at least as well as single-task learning, thus\npreventing negative transfer. We also extend our framework to transfer learning\nwith corresponding theoretical guarantees for the target task. Extensive\nexperiments confirm our theory, showcasing the robustness and adaptivity of\nRAS, and its superior performance in regimes with up to 80\\% task\ncontamination.",
      "published": "September 08, 2025",
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.ME"
      ],
      "pdf_link": "http://arxiv.org/pdf/2509.06575v1",
      "arxiv_url": "http://arxiv.org/abs/2509.06575v1"
    },
    {
      "title": "Minimax optimal transfer learning for high-dimensional additive\n  regression",
      "authors": [
        "Seung Hyun Moon"
      ],
      "abstract": "This paper studies high-dimensional additive regression under the transfer\nlearning framework, where one observes samples from a target population\ntogether with auxiliary samples from different but potentially related\nregression models. We first introduce a target-only estimation procedure based\non the smooth backfitting estimator with local linear smoothing. In contrast to\nprevious work, we establish general error bounds under sub-Weibull($\\alpha$)\nnoise, thereby accommodating heavy-tailed error distributions. In the\nsub-exponential case ($\\alpha=1$), we show that the estimator attains the\nminimax lower bound under regularity conditions, which requires a substantial\ndeparture from existing proof strategies. We then develop a novel two-stage\nestimation method within a transfer learning framework, and provide theoretical\nguarantees at both the population and empirical levels. Error bounds are\nderived for each stage under general tail conditions, and we further\ndemonstrate that the minimax optimal rate is achieved when the auxiliary and\ntarget distributions are sufficiently close. All theoretical results are\nsupported by simulation studies and real data analysis.",
      "published": "September 08, 2025",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.ST",
        "stat.TH"
      ],
      "pdf_link": "http://arxiv.org/pdf/2509.06308v2",
      "arxiv_url": "http://arxiv.org/abs/2509.06308v2"
    }
  ]
}