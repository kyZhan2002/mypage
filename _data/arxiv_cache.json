{
  "timestamp": 1759195185,
  "papers": [
    {
      "title": "Transfer Learning under Group-Label Shift: A Semiparametric Exponential\n  Tilting Approach",
      "authors": [
        "Manli Cheng",
        "Subha Maity",
        "Qinglong Tian",
        "Pengfei Li"
      ],
      "abstract": "We propose a new framework for binary classification in transfer learning\nsettings where both covariate and label distributions may shift between source\nand target domains. Unlike traditional covariate shift or label shift\nassumptions, we introduce a group-label shift assumption that accommodates\nsubpopulation imbalance and mitigates spurious correlations, thereby improving\nrobustness to real-world distributional changes. To model the joint\ndistribution difference, we adopt a flexible exponential tilting formulation\nand establish mild, verifiable identification conditions via an instrumental\nvariable strategy. We develop a computationally efficient two-step\nlikelihood-based estimation procedure that combines logistic regression for the\nsource outcome model with conditional likelihood estimation using both source\nand target covariates. We derive consistency and asymptotic normality for the\nresulting estimators, and extend the theory to receiver operating\ncharacteristic curves, the area under the curve, and other target functionals,\naddressing the nonstandard challenges posed by plug-in classifiers. Simulation\nstudies demonstrate that our method outperforms existing alternatives under\nsubpopulation shift scenarios. A semi-synthetic application using the\nwaterbirds dataset further confirms the proposed method's ability to transfer\ninformation effectively and improve target-domain classification accuracy.",
      "published": "September 26, 2025",
      "categories": [
        "stat.ME",
        "math.ST",
        "stat.TH"
      ],
      "pdf_link": "http://arxiv.org/pdf/2509.22268v1",
      "arxiv_url": "http://arxiv.org/abs/2509.22268v1"
    },
    {
      "title": "Transfer Learning in Regression with Influential Points",
      "authors": [
        "Bingbing Wang",
        "Jiaqi Wang",
        "Yu Tang"
      ],
      "abstract": "Regression prediction plays a crucial role in practical applications and\nstrongly relies on data annotation. However, due to prohibitive annotation\ncosts or domain-specific constraints, labeled data in the target domain is\noften scarce, making transfer learning a critical solution by leveraging\nknowledge from resource-rich source domains. In the practical target scenario,\nalthough transfer learning has been widely applied, influential points can\nsignificantly distort parameter estimation for the target domain model. This\nissue is further compounded when influential points are also present in source\ndomains, leading to aggravated performance degradation and posing critical\nrobustness challenges for existing transfer learning frameworks. In this study,\nwe innovatively introduce a transfer learning collaborative optimization\n(Trans-CO) framework for influential point detection and regression model\nfitting. Extensive simulation experiments demonstrate that the proposed\nTrans-CO algorithm outperforms competing methods in terms of model fitting\nperformance and influential point identification accuracy. Furthermore, it\nachieves superior predictive accuracy on real-world datasets, providing a novel\nsolution for transfer learning in regression with influential points",
      "published": "September 24, 2025",
      "categories": [
        "stat.ME"
      ],
      "pdf_link": "http://arxiv.org/pdf/2509.20272v1",
      "arxiv_url": "http://arxiv.org/abs/2509.20272v1"
    },
    {
      "title": "A Realistic Evaluation of Cross-Frequency Transfer Learning and\n  Foundation Forecasting Models",
      "authors": [
        "Kin G. Olivares",
        "Malcolm Wolff",
        "Tatiana Konstantinova",
        "Shankar Ramasubramanian",
        "Andrew Gordon Wilson",
        "Andres Potapczynski",
        "Willa Potosnak",
        "Mengfei Cao",
        "Boris Oreshkin",
        "Dmitry Efimov"
      ],
      "abstract": "Cross-frequency transfer learning (CFTL) has emerged as a popular framework\nfor curating large-scale time series datasets to pre-train foundation\nforecasting models (FFMs). Although CFTL has shown promise, current\nbenchmarking practices fall short of accurately assessing its performance. This\nshortcoming stems from many factors: an over-reliance on small-scale evaluation\ndatasets; inadequate treatment of sample size when computing summary\nstatistics; reporting of suboptimal statistical models; and failing to account\nfor non-negligible risks of overlap between pre-training and test datasets. To\naddress these limitations, we introduce a unified reimplementation of\nwidely-adopted neural forecasting networks, adapting them for the CFTL setup;\nwe pre-train only on proprietary and synthetic data, being careful to prevent\ntest leakage; and we evaluate on 15 large, diverse public forecast competition\ndatasets. Our empirical analysis reveals that statistical models' accuracy is\nfrequently underreported. Notably, we confirm that statistical models and their\nensembles consistently outperform existing FFMs by more than 8.2% in sCRPS, and\nby more than 20% MASE, across datasets. However, we also find that synthetic\ndataset pre-training does improve the accuracy of a FFM by 7% percent.",
      "published": "September 23, 2025",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.AP"
      ],
      "pdf_link": "http://arxiv.org/pdf/2509.19465v1",
      "arxiv_url": "http://arxiv.org/abs/2509.19465v1"
    },
    {
      "title": "DRO-REBEL: Distributionally Robust Relative-Reward Regression for Fast\n  and Efficient LLM Alignment",
      "authors": [
        "Sharan Sahu",
        "Martin T. Wells"
      ],
      "abstract": "Reinforcement learning with human feedback (RLHF) has become crucial for\naligning Large Language Models (LLMs) with human intent. However, existing\noffline RLHF approaches suffer from overoptimization, where models overfit to\nreward misspecification and drift from preferred behaviors observed during\ntraining. We introduce DRO-REBEL, a unified family of robust REBEL updates with\ntype-$p$ Wasserstein, KL, and $\\chi^2$ ambiguity sets. Using Fenchel duality,\neach update reduces to a simple relative-reward regression, preserving\nscalability and avoiding PPO-style clipping or auxiliary value networks. Under\nstandard linear-reward and log-linear policy classes with a data-coverage\ncondition, we establish $O(n^{-1/4})$ estimation bounds with tighter constants\nthan prior DRO-DPO approaches, and recover the minimax-optimal $O(n^{-1/2})$\nrate via a localized Rademacher complexity analysis. The same analysis closes\nthe gap for Wasserstein-DPO and KL-DPO, showing both also attain optimal\nparametric rates. We derive practical SGD algorithms for all three divergences:\ngradient regularization (Wasserstein), importance weighting (KL), and a fast\n1-D dual solve ($\\chi^2$). Experiments on Emotion Alignment, the large-scale\nArmoRM multi-objective benchmark, and HH-Alignment demonstrate strong\nworst-case robustness across unseen preference mixtures, model sizes, and data\nscales, with $\\chi^2$-REBEL showing consistently strong empirical performance.\nA controlled radius--coverage study validates a no-free-lunch trade-off: radii\nshrinking faster than empirical divergence concentration rates achieve\nminimax-optimal parametric rates but forfeit coverage, while\ncoverage-guaranteeing radii incur $O(n^{-1/4})$ rates.",
      "published": "September 23, 2025",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_link": "http://arxiv.org/pdf/2509.19104v1",
      "arxiv_url": "http://arxiv.org/abs/2509.19104v1"
    },
    {
      "title": "Asymptotically Optimal Problem-Dependent Bandit Policies for Transfer\n  Learning",
      "authors": [
        "Adrien Prevost",
        "Timothee Mathieu",
        "Odalric-Ambrym Maillard"
      ],
      "abstract": "We study the non-contextual multi-armed bandit problem in a transfer learning\nsetting: before any pulls, the learner is given N'_k i.i.d. samples from each\nsource distribution nu'_k, and the true target distributions nu_k lie within a\nknown distance bound d_k(nu_k, nu'_k) <= L_k. In this framework, we first\nderive a problem-dependent asymptotic lower bound on cumulative regret that\nextends the classical Lai-Robbins result to incorporate the transfer parameters\n(d_k, L_k, N'_k). We then propose KL-UCB-Transfer, a simple index policy that\nmatches this new bound in the Gaussian case. Finally, we validate our approach\nvia simulations, showing that KL-UCB-Transfer significantly outperforms the\nno-prior baseline when source and target distributions are sufficiently close.",
      "published": "September 23, 2025",
      "categories": [
        "cs.LG",
        "math.ST",
        "stat.TH"
      ],
      "pdf_link": "http://arxiv.org/pdf/2509.19098v1",
      "arxiv_url": "http://arxiv.org/abs/2509.19098v1"
    },
    {
      "title": "A Generative Conditional Distribution Equality Testing Framework and Its\n  Minimax Analysis",
      "authors": [
        "Siming Zheng",
        "Meifang Lan",
        "Tong Wang",
        "Yuanyuan Lin"
      ],
      "abstract": "In this paper, we propose a general framework for testing the equality of the\nconditional distributions in a two-sample problem. This problem is most\nrelevant to transfer learning under covariate shift. Our framework is built on\nneural network-based generative methods and sample splitting techniques by\ntransforming the conditional distribution testing problem into an unconditional\none. We introduce two special tests: the generative permutation-based\nconditional distribution equality test and the generative classification\naccuracy-based conditional distribution equality test. Theoretically, we\nestablish a minimax lower bound for statistical inference in testing the\nequality of two conditional distributions under certain smoothness conditions.\nWe demonstrate that the generative permutation-based conditional distribution\nequality test and its modified version can attain this lower bound precisely or\nup to some iterated logarithmic factor. Moreover, we prove the testing\nconsistency of the generative classification accuracy-based conditional\ndistribution equality test. We also establish the convergence rate for the\nlearned conditional generator by deriving new results related to the\nrecently-developed offset Rademacher complexity and approximation properties\nusing neural networks. Empirically, we conduct numerical studies including\nsynthetic datasets and two real-world datasets, demonstrating the effectiveness\nof our approach.",
      "published": "September 22, 2025",
      "categories": [
        "cs.LG",
        "math.ST",
        "stat.ME",
        "stat.TH"
      ],
      "pdf_link": "http://arxiv.org/pdf/2509.17729v2",
      "arxiv_url": "http://arxiv.org/abs/2509.17729v2"
    },
    {
      "title": "What is a good matching of probability measures? A counterfactual lens\n  on transport maps",
      "authors": [
        "Lucas De Lara",
        "Luca Ganassali"
      ],
      "abstract": "Coupling probability measures lies at the core of many problems in statistics\nand machine learning, from domain adaptation to transfer learning and causal\ninference. Yet, even when restricted to deterministic transports, such\ncouplings are not identifiable: two atomless marginals admit infinitely many\ntransport maps. The common recourse to optimal transport, motivated by cost\nminimization and cyclical monotonicity, obscures the fact that several distinct\nnotions of multivariate monotone matchings coexist. In this work, we first\ncarry a comparative analysis of three constructions of transport maps:\ncyclically monotone, quantile-preserving and triangular monotone maps. We\nestablish necessary and sufficient conditions for their equivalence, thereby\nclarifying their respective structural properties. In parallel, we formulate\ncounterfactual reasoning within the framework of structural causal models as a\nproblem of selecting transport maps between fixed marginals, which makes\nexplicit the role of untestable assumptions in counterfactual reasoning. Then,\nwe are able to connect these two perspectives by identifying conditions on\ncausal graphs and structural equations under which counterfactual maps coincide\nwith classical statistical transports. In this way, we delineate the\ncircumstances in which causal assumptions support the use of a specific\nstructure of transport map. Taken together, our results aim to enrich the\ntheoretical understanding of families of transport maps and to clarify their\npossible causal interpretations. We hope this work contributes to establishing\nnew bridges between statistical transport and causal inference.",
      "published": "September 19, 2025",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.ST",
        "stat.ME",
        "stat.TH"
      ],
      "pdf_link": "http://arxiv.org/pdf/2509.16027v1",
      "arxiv_url": "http://arxiv.org/abs/2509.16027v1"
    },
    {
      "title": "Transfer learning under latent space model",
      "authors": [
        "Kuangnan Fang",
        "Ruixuan Qin",
        "Xinyan Fan"
      ],
      "abstract": "Latent space model plays a crucial role in network analysis, and accurate\nestimation of latent variables is essential for downstream tasks such as link\nprediction. However, the large number of parameters to be estimated presents a\nchallenge, especially when the latent space dimension is not exceptionally\nsmall. In this paper, we propose a transfer learning method that leverages\ninformation from networks with latent variables similar to those in the target\nnetwork, thereby improving the estimation accuracy for the target. Given\ntransferable source networks, we introduce a two-stage transfer learning\nalgorithm that accommodates differences in node numbers between source and\ntarget networks. In each stage, we derive sufficient identification conditions\nand design tailored projected gradient descent algorithms for estimation.\nTheoretical properties of the resulting estimators are established. When the\ntransferable networks are unknown, a detection algorithm is introduced to\nidentify suitable source networks. Simulation studies and analyses of two real\ndatasets demonstrate the effectiveness of the proposed methods.",
      "published": "September 19, 2025",
      "categories": [
        "stat.ME",
        "stat.ML"
      ],
      "pdf_link": "http://arxiv.org/pdf/2509.15797v1",
      "arxiv_url": "http://arxiv.org/abs/2509.15797v1"
    },
    {
      "title": "SETrLUSI: Stochastic Ensemble Multi-Source Transfer Learning Using\n  Statistical Invariant",
      "authors": [
        "Chunna Li",
        "Yiwei Song",
        "Yuanhai Shao"
      ],
      "abstract": "In transfer learning, a source domain often carries diverse knowledge, and\ndifferent domains usually emphasize different types of knowledge. Different\nfrom handling only a single type of knowledge from all domains in traditional\ntransfer learning methods, we introduce an ensemble learning framework with a\nweak mode of convergence in the form of Statistical Invariant (SI) for\nmulti-source transfer learning, formulated as Stochastic Ensemble Multi-Source\nTransfer Learning Using Statistical Invariant (SETrLUSI). The proposed SI\nextracts and integrates various types of knowledge from both source and target\ndomains, which not only effectively utilizes diverse knowledge but also\naccelerates the convergence process. Further, SETrLUSI incorporates stochastic\nSI selection, proportional source domain sampling, and target domain\nbootstrapping, which improves training efficiency while enhancing model\nstability. Experiments show that SETrLUSI has good convergence and outperforms\nrelated methods with a lower time cost.",
      "published": "September 19, 2025",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "pdf_link": "http://arxiv.org/pdf/2509.15593v1",
      "arxiv_url": "http://arxiv.org/abs/2509.15593v1"
    }
  ]
}