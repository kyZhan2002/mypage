{
  "timestamp": 1763601769,
  "papers": [
    {
      "title": "SCOPE: Spectral Concentration by Distributionally Robust Joint Covariance-Precision Estimation",
      "authors": [
        "Renjie Chen",
        "Viet Anh Nguyen",
        "Huifu Xu"
      ],
      "abstract": "We propose a distributionally robust formulation for simultaneously estimating the covariance matrix and the precision matrix of a random vector.The proposed model minimizes the worst-case weighted sum of the Frobenius loss of the covariance estimator and Stein's loss of the precision matrix estimator against all distributions from an ambiguity set centered at the nominal distribution. The radius of the ambiguity set is measured via convex spectral divergence. We demonstrate that the proposed distributionally robust estimation model can be reduced to a convex optimization problem, thereby yielding quasi-analytical estimators. The joint estimators are shown to be nonlinear shrinkage estimators. The eigenvalues of the estimators are shrunk nonlinearly towards a positive scalar, where the scalar is determined by the weight coefficient of the loss terms. By tuning the coefficient carefully, the shrinkage corrects the spectral bias of the empirical covariance/precision matrix estimator. By this property, we call the proposed joint estimator the Spectral concentrated COvariance and Precision matrix Estimator (SCOPE). We demonstrate that the shrinkage effect improves the condition number of the estimator. We provide a parameter-tuning scheme that adjusts the shrinkage target and intensity that is asymptotically optimal. Numerical experiments on synthetic and real data show that our shrinkage estimators perform competitively against state-of-the-art estimators in practical applications.",
      "published": "November 18, 2025",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.OC"
      ],
      "pdf_link": "https://arxiv.org/pdf/2511.14146v1",
      "arxiv_url": "https://arxiv.org/abs/2511.14146v1"
    },
    {
      "title": "SmallML: Bayesian Transfer Learning for Small-Data Predictive Analytics",
      "authors": [
        "Semen Leontev"
      ],
      "abstract": "Small and medium-sized enterprises (SMEs) represent 99.9% of U.S. businesses yet remain systematically excluded from AI due to a mismatch between their operational scale and modern machine learning's data requirements. This paper introduces SmallML, a Bayesian transfer learning framework achieving enterprise-level prediction accuracy with datasets as small as 50-200 observations.\n  We develop a three-layer architecture integrating transfer learning, hierarchical Bayesian modeling, and conformal prediction. Layer 1 extracts informative priors from 22,673 public records using a SHAP-based procedure transferring knowledge from gradient boosting to logistic regression. Layer 2 implements hierarchical pooling across J=5-50 SMEs with adaptive shrinkage, balancing population patterns with entity-specific characteristics. Layer 3 provides conformal sets with finite-sample coverage guarantees P(y in C(x)) >= 1-alpha for distribution-free uncertainty quantification.\n  Validation on customer churn data demonstrates 96.7% +/- 4.2% AUC with 100 observations per business -- a +24.2 point improvement over independent logistic regression (72.5% +/- 8.1%), with p < 0.000001. Conformal prediction achieves 92% empirical coverage at 90% target. Training completes in 33 minutes on standard CPU hardware. By enabling enterprise-grade predictions for 33 million U.S. SMEs previously excluded from machine learning, SmallML addresses a critical gap in AI democratization.\n  Keywords: Bayesian transfer learning, hierarchical models, conformal prediction, small-data analytics, SME machine learning",
      "published": "November 18, 2025",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_link": "https://arxiv.org/pdf/2511.14049v1",
      "arxiv_url": "https://arxiv.org/abs/2511.14049v1"
    },
    {
      "title": "Transfer learning for high-dimensional Factor-augmented sparse linear model",
      "authors": [
        "Bo Fu",
        "Dandan Jiang"
      ],
      "abstract": "In this paper, we study transfer learning for high-dimensional factor-augmented sparse linear models, motivated by applications in economics and finance where strongly correlated predictors and latent factor structures pose major challenges for reliable estimation. Our framework simultaneously mitigates the impact of high correlation and removes the additional contributions of latent factors, thereby reducing potential model misspecification in conventional linear modeling. In such settings, the target dataset is often limited, but multiple heterogeneous auxiliary sources may provide additional information. We develop transfer learning procedures that effectively leverage these auxiliary datasets to improve estimation accuracy, and establish non-asymptotic $\\ell_1$- and $\\ell_2$-error bounds for the proposed estimators. To prevent negative transfer, we introduce a data-driven source detection algorithm capable of identifying informative auxiliary datasets and prove its consistency. In addition, we provide a hypothesis testing framework for assessing the adequacy of the factor model, together with a procedure for constructing simultaneous confidence intervals for the regression coefficients of interest. Numerical studies demonstrate that our methods achieve substantial gains in estimation accuracy and remain robust under heterogeneity across datasets. Overall, our framework offers a theoretical foundation and a practically scalable solution for incorporating heterogeneous auxiliary information in settings with highly correlated features and latent factor structures.",
      "published": "November 16, 2025",
      "categories": [
        "stat.ME"
      ],
      "pdf_link": "https://arxiv.org/pdf/2511.12435v2",
      "arxiv_url": "https://arxiv.org/abs/2511.12435v2"
    }
  ]
}