{
  "timestamp": 1741482424,
  "papers": [
    {
      "title": "Provable Robust Overfitting Mitigation in Wasserstein Distributionally\n  Robust Optimization",
      "authors": [
        "Shuang Liu",
        "Yihan Wang",
        "Yifan Zhu",
        "Yibo Miao",
        "Xiao-Shan Gao"
      ],
      "abstract": "Wasserstein distributionally robust optimization (WDRO) optimizes against\nworst-case distributional shifts within a specified uncertainty set, leading to\nenhanced generalization on unseen adversarial examples, compared to standard\nadversarial training which focuses on pointwise adversarial perturbations.\nHowever, WDRO still suffers fundamentally from the robust overfitting problem,\nas it does not consider statistical error. We address this gap by proposing a\nnovel robust optimization framework under a new uncertainty set for adversarial\nnoise via Wasserstein distance and statistical error via Kullback-Leibler\ndivergence, called the Statistically Robust WDRO. We establish a robust\ngeneralization bound for the new optimization framework, implying that\nout-of-distribution adversarial performance is at least as good as the\nstatistically robust training loss with high probability. Furthermore, we\nderive conditions under which Stackelberg and Nash equilibria exist between the\nlearner and the adversary, giving an optimal robust model in certain sense.\nFinally, through extensive experiments, we demonstrate that our method\nsignificantly mitigates robust overfitting and enhances robustness within the\nframework of WDRO.",
      "published": "March 06, 2025",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.ST",
        "stat.TH"
      ],
      "pdf_link": "http://arxiv.org/pdf/2503.04315v1",
      "arxiv_url": "http://arxiv.org/abs/2503.04315v1"
    },
    {
      "title": "The Distributionally Robust Optimization Model of Sparse Principal\n  Component Analysis",
      "authors": [
        "Lei Wang",
        "Xin Liu",
        "Xiaojun Chen"
      ],
      "abstract": "We consider sparse principal component analysis (PCA) under a stochastic\nsetting where the underlying probability distribution of the random parameter\nis uncertain. This problem is formulated as a distributionally robust\noptimization (DRO) model based on a constructive approach to capturing\nuncertainty in the covariance matrix, which constitutes a nonsmooth constrained\nmin-max optimization problem. We further prove that the inner maximization\nproblem admits a closed-form solution, reformulating the original DRO model\ninto an equivalent minimization problem on the Stiefel manifold. This\ntransformation leads to a Riemannian optimization problem with intricate\nnonsmooth terms, a challenging formulation beyond the reach of existing\nalgorithms. To address this issue, we devise an efficient smoothing manifold\nproximal gradient algorithm. We prove the Riemannian gradient consistency and\nglobal convergence of our algorithm to a stationary point of the nonsmooth\nminimization problem. Moreover, we establish the iteration complexity of our\nalgorithm. Finally, numerical experiments are conducted to validate the\neffectiveness and scalability of our algorithm, as well as to highlight the\nnecessity and rationality of adopting the DRO model for sparse PCA.",
      "published": "March 04, 2025",
      "categories": [
        "math.OC",
        "cs.LG",
        "stat.ML"
      ],
      "pdf_link": "http://arxiv.org/pdf/2503.02494v1",
      "arxiv_url": "http://arxiv.org/abs/2503.02494v1"
    },
    {
      "title": "Diagnosis of Patients with Viral, Bacterial, and Non-Pneumonia Based on\n  Chest X-Ray Images Using Convolutional Neural Networks",
      "authors": [
        "Carlos Arizmendi",
        "Jorge Pinto",
        "Alejandro Arboleda",
        "Hernando Gonz\u00e1lez"
      ],
      "abstract": "According to the World Health Organization (WHO), pneumonia is a disease that\ncauses a significant number of deaths each year. In response to this issue, the\ndevelopment of a decision support system for the classification of patients\ninto those without pneumonia and those with viral or bacterial pneumonia is\nproposed. This is achieved by implementing transfer learning (TL) using\npre-trained convolutional neural network (CNN) models on chest x-ray (CXR)\nimages. The system is further enhanced by integrating Relief and Chi-square\nmethods as dimensionality reduction techniques, along with support vector\nmachines (SVM) for classification. The performance of a series of experiments\nwas evaluated to build a model capable of distinguishing between patients\nwithout pneumonia and those with viral or bacterial pneumonia. The obtained\nresults include an accuracy of 91.02%, precision of 97.73%, recall of 98.03%,\nand an F1 Score of 97.88% for discriminating between patients without pneumonia\nand those with pneumonia. In addition, accuracy of 93.66%, precision of 94.26%,\nrecall of 92.66%, and an F1 Score of 93.45% were achieved for discriminating\nbetween patients with viral pneumonia and those with bacterial pneumonia.",
      "published": "March 03, 2025",
      "categories": [
        "eess.IV",
        "cs.CV",
        "cs.LG",
        "stat.CO"
      ],
      "pdf_link": "http://arxiv.org/pdf/2503.02906v1",
      "arxiv_url": "http://arxiv.org/abs/2503.02906v1"
    },
    {
      "title": "Optimal Transfer Learning for Missing Not-at-Random Matrix Completion",
      "authors": [
        "Akhil Jalan",
        "Yassir Jedra",
        "Arya Mazumdar",
        "Soumendu Sundar Mukherjee",
        "Purnamrita Sarkar"
      ],
      "abstract": "We study transfer learning for matrix completion in a Missing Not-at-Random\n(MNAR) setting that is motivated by biological problems. The target matrix $Q$\nhas entire rows and columns missing, making estimation impossible without side\ninformation. To address this, we use a noisy and incomplete source matrix $P$,\nwhich relates to $Q$ via a feature shift in latent space. We consider both the\nactive and passive sampling of rows and columns. We establish minimax lower\nbounds for entrywise estimation error in each setting. Our computationally\nefficient estimation framework achieves this lower bound for the active\nsetting, which leverages the source data to query the most informative rows and\ncolumns of $Q$. This avoids the need for incoherence assumptions required for\nrate optimality in the passive sampling setting. We demonstrate the\neffectiveness of our approach through comparisons with existing algorithms on\nreal-world biological datasets.",
      "published": "February 28, 2025",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_link": "http://arxiv.org/pdf/2503.00174v1",
      "arxiv_url": "http://arxiv.org/abs/2503.00174v1"
    },
    {
      "title": "A Principled Approach to Bayesian Transfer Learning",
      "authors": [
        "Adam Bretherton",
        "Joshua J. Bon",
        "David J. Warne",
        "Kerrie Mengersen",
        "Christopher Drovandi"
      ],
      "abstract": "Updating $\\textit{a priori}$ information given some observed data is the core\ntenet of Bayesian inference. Bayesian transfer learning extends this idea by\nincorporating information from a related dataset to improve the inference on\nthe observed data which may have been collected under slightly different\nsettings. The use of related information can be useful when the observed data\nis scarce, for example. Current Bayesian transfer learning methods that are\nbased on the so-called $\\textit{power prior}$ can adaptively transfer\ninformation from related data. Unfortunately, it is not always clear under\nwhich scenario Bayesian transfer learning performs best or even if it will\nimprove Bayesian inference. Additionally, current power prior methods rely on\nconjugacy to evaluate the posterior of interest. We propose using leave-one-out\ncross validation on the target dataset as a means of evaluating Bayesian\ntransfer learning methods. Further, we introduce a new framework,\n$\\textit{transfer sequential Monte Carlo}$, for power prior approaches that\nefficiently chooses the transfer parameter while avoiding the need for\nconjugate priors. We assess the performance of our proposed methods in two\ncomprehensive simulation studies.",
      "published": "February 27, 2025",
      "categories": [
        "stat.ME",
        "stat.CO",
        "62F15, 62F07 (Primary) 62-08 (Secondary)"
      ],
      "pdf_link": "http://arxiv.org/pdf/2502.19796v1",
      "arxiv_url": "http://arxiv.org/abs/2502.19796v1"
    },
    {
      "title": "Conformal Prediction Under Generalized Covariate Shift with Posterior\n  Drift",
      "authors": [
        "Baozhen Wang",
        "Xingye Qiao"
      ],
      "abstract": "In many real applications of statistical learning, collecting sufficiently\nmany training data is often expensive, time-consuming, or even unrealistic. In\nthis case, a transfer learning approach, which aims to leverage knowledge from\na related source domain to improve the learning performance in the target\ndomain, is more beneficial. There have been many transfer learning methods\ndeveloped under various distributional assumptions. In this article, we study a\nparticular type of classification problem, called conformal prediction, under a\nnew distributional assumption for transfer learning. Classifiers under the\nconformal prediction framework predict a set of plausible labels instead of one\nsingle label for each data instance, affording a more cautious and safer\ndecision. We consider a generalization of the \\textit{covariate shift with\nposterior drift} setting for transfer learning. Under this setting, we propose\na weighted conformal classifier that leverages both the source and target\nsamples, with a coverage guarantee in the target domain. Theoretical studies\ndemonstrate favorable asymptotic properties. Numerical studies further\nillustrate the usefulness of the proposed method.",
      "published": "February 25, 2025",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.ST",
        "stat.TH"
      ],
      "pdf_link": "http://arxiv.org/pdf/2502.17744v1",
      "arxiv_url": "http://arxiv.org/abs/2502.17744v1"
    },
    {
      "title": "Distributionally Robust Active Learning for Gaussian Process Regression",
      "authors": [
        "Shion Takeno",
        "Yoshito Okura",
        "Yu Inatsu",
        "Aoyama Tatsuya",
        "Tomonari Tanaka",
        "Akahane Satoshi",
        "Hiroyuki Hanada",
        "Noriaki Hashimoto",
        "Taro Murayama",
        "Hanju Lee",
        "Shinya Kojima",
        "Ichiro Takeuchi"
      ],
      "abstract": "Gaussian process regression (GPR) or kernel ridge regression is a widely used\nand powerful tool for nonlinear prediction. Therefore, active learning (AL) for\nGPR, which actively collects data labels to achieve an accurate prediction with\nfewer data labels, is an important problem. However, existing AL methods do not\ntheoretically guarantee prediction accuracy for target distribution.\nFurthermore, as discussed in the distributionally robust learning literature,\nspecifying the target distribution is often difficult. Thus, this paper\nproposes two AL methods that effectively reduce the worst-case expected error\nfor GPR, which is the worst-case expectation in target distribution candidates.\nWe show an upper bound of the worst-case expected squared error, which suggests\nthat the error will be arbitrarily small by a finite number of data labels\nunder mild conditions. Finally, we demonstrate the effectiveness of the\nproposed methods through synthetic and real-world datasets.",
      "published": "February 24, 2025",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_link": "http://arxiv.org/pdf/2502.16870v1",
      "arxiv_url": "http://arxiv.org/abs/2502.16870v1"
    },
    {
      "title": "Provable Benefits of Unsupervised Pre-training and Transfer Learning via\n  Single-Index Models",
      "authors": [
        "Taj Jones-McCormick",
        "Aukosh Jagannath",
        "Subhabrata Sen"
      ],
      "abstract": "Unsupervised pre-training and transfer learning are commonly used techniques\nto initialize training algorithms for neural networks, particularly in settings\nwith limited labeled data. In this paper, we study the effects of unsupervised\npre-training and transfer learning on the sample complexity of high-dimensional\nsupervised learning. Specifically, we consider the problem of training a\nsingle-layer neural network via online stochastic gradient descent. We\nestablish that pre-training and transfer learning (under concept shift) reduce\nsample complexity by polynomial factors (in the dimension) under very general\nassumptions. We also uncover some surprising settings where pre-training grants\nexponential improvement over random initialization in terms of sample\ncomplexity.",
      "published": "February 24, 2025",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "pdf_link": "http://arxiv.org/pdf/2502.16849v1",
      "arxiv_url": "http://arxiv.org/abs/2502.16849v1"
    }
  ]
}