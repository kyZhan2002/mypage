{
  "timestamp": 1761182511,
  "papers": [
    {
      "title": "Transfer Learning for Benign Overfitting in High-Dimensional Linear\n  Regression",
      "authors": [
        "Yeichan Kim",
        "Ilmun Kim",
        "Seyoung Park"
      ],
      "abstract": "Transfer learning is a key component of modern machine learning, enhancing\nthe performance of target tasks by leveraging diverse data sources.\nSimultaneously, overparameterized models such as the minimum-$\\ell_2$-norm\ninterpolator (MNI) in high-dimensional linear regression have garnered\nsignificant attention for their remarkable generalization capabilities, a\nproperty known as benign overfitting. Despite their individual importance, the\nintersection of transfer learning and MNI remains largely unexplored. Our\nresearch bridges this gap by proposing a novel two-step Transfer MNI approach\nand analyzing its trade-offs. We characterize its non-asymptotic excess risk\nand identify conditions under which it outperforms the target-only MNI. Our\nanalysis reveals free-lunch covariate shift regimes, where leveraging\nheterogeneous data yields the benefit of knowledge transfer at limited cost. To\noperationalize our findings, we develop a data-driven procedure to detect\ninformative sources and introduce an ensemble method incorporating multiple\ninformative Transfer MNIs. Finite-sample experiments demonstrate the robustness\nof our methods to model and data heterogeneity, confirming their advantage.",
      "published": "October 17, 2025",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.ST",
        "stat.TH"
      ],
      "pdf_link": "http://arxiv.org/pdf/2510.15337v1",
      "arxiv_url": "http://arxiv.org/abs/2510.15337v1"
    },
    {
      "title": "Stress-Aware Learning under KL Drift via Trust-Decayed Mirror Descent",
      "authors": [
        "Gabriel Nixon Raj"
      ],
      "abstract": "We study sequential decision-making under distribution drift. We propose\nentropy-regularized trust-decay, which injects stress-aware exponential tilting\ninto both belief updates and mirror-descent decisions. On the simplex, a\nFenchel-dual equivalence shows that belief tilt and decision tilt coincide. We\nformalize robustness via fragility (worst-case excess risk in a KL ball),\nbelief bandwidth (radius sustaining a target excess), and a decision-space\nFragility Index (drift tolerated at $O(\\sqrt{T})$ regret). We prove\nhigh-probability sensitivity bounds and establish dynamic-regret guarantees of\n$\\tilde{O}(\\sqrt{T})$ under KL-drift path length $S_T = \\sum_{t\\ge2}\\sqrt{{\\rm\nKL}(D_t|D_{t-1})/2}$. In particular, trust-decay achieves $O(1)$ per-switch\nregret, while stress-free updates incur $\\Omega(1)$ tails. A parameter-free\nhedge adapts the tilt to unknown drift, whereas persistent over-tilting yields\nan $\\Omega(\\lambda^2 T)$ stationary penalty. We further obtain\ncalibrated-stress bounds and extensions to second-order updates, bandit\nfeedback, outliers, stress variation, distributed optimization, and plug-in\nKL-drift estimation. The framework unifies dynamic-regret analysis,\ndistributionally robust objectives, and KL-regularized control within a single\nstress-adaptive update.",
      "published": "October 17, 2025",
      "categories": [
        "cs.LG",
        "math.ST",
        "stat.TH"
      ],
      "pdf_link": "http://arxiv.org/pdf/2510.15222v1",
      "arxiv_url": "http://arxiv.org/abs/2510.15222v1"
    },
    {
      "title": "Evaluating Policy Effects under Network Interference without Network\n  Information: A Transfer Learning Approach",
      "authors": [
        "Tadao Hoshino"
      ],
      "abstract": "This paper develops a sensitivity analysis framework that transfers the\naverage total treatment effect (ATTE) from source data with a fully observed\nnetwork to target data whose network is completely unknown. The ATTE represents\nthe average social impact of a policy that assigns the treatment to every\nindividual in the dataset. We postulate a covariate-shift type assumption that\nboth source and target datasets share the same conditional mean outcome.\nHowever, because the target network is unobserved, this assumption alone is not\nsufficient to pin down the ATTE for the target data. To address this issue, we\nconsider a sensitivity analysis based on the uncertainty of the target\nnetwork's degree distribution, where the extent of uncertainty is measured by\nthe Wasserstein distance from a given reference degree distribution. We then\nconstruct bounds on the target ATTE using a linear programming-based estimator.\nThe limiting distribution of the bound estimator is derived via the functional\ndelta method, and we develop a wild bootstrap approach to approximate the\ndistribution. As an empirical illustration, we revisit the social network\nexperiment on farmers' weather insurance adoption in China by Cai et al.\n(2015).",
      "published": "October 16, 2025",
      "categories": [
        "stat.ME",
        "econ.EM",
        "math.ST",
        "stat.TH"
      ],
      "pdf_link": "http://arxiv.org/pdf/2510.14415v1",
      "arxiv_url": "http://arxiv.org/abs/2510.14415v1"
    },
    {
      "title": "Policy Regularized Distributionally Robust Markov Decision Processes\n  with Linear Function Approximation",
      "authors": [
        "Jingwen Gu",
        "Yiting He",
        "Zhishuai Liu",
        "Pan Xu"
      ],
      "abstract": "Decision-making under distribution shift is a central challenge in\nreinforcement learning (RL), where training and deployment environments differ.\nWe study this problem through the lens of robust Markov decision processes\n(RMDPs), which optimize performance against adversarial transition dynamics.\nOur focus is the online setting, where the agent has only limited interaction\nwith the environment, making sample efficiency and exploration especially\ncritical. Policy optimization, despite its success in standard RL, remains\ntheoretically and empirically underexplored in robust RL. To bridge this gap,\nwe propose \\textbf{D}istributionally \\textbf{R}obust \\textbf{R}egularized\n\\textbf{P}olicy \\textbf{O}ptimization algorithm (DR-RPO), a model-free online\npolicy optimization method that learns robust policies with sublinear regret.\nTo enable tractable optimization within the softmax policy class, DR-RPO\nincorporates reference-policy regularization, yielding RMDP variants that are\ndoubly constrained in both transitions and policies. To scale to large\nstate-action spaces, we adopt the $d$-rectangular linear MDP formulation and\ncombine linear function approximation with an upper confidence bonus for\noptimistic exploration. We provide theoretical guarantees showing that policy\noptimization can achieve polynomial suboptimality bounds and sample efficiency\nin robust RL, matching the performance of value-based approaches. Finally,\nempirical results across diverse domains corroborate our theory and demonstrate\nthe robustness of DR-RPO.",
      "published": "October 16, 2025",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "pdf_link": "http://arxiv.org/pdf/2510.14246v1",
      "arxiv_url": "http://arxiv.org/abs/2510.14246v1"
    }
  ]
}