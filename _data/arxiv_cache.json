{
  "timestamp": 1758590284,
  "papers": [
    {
      "title": "What is a good matching of probability measures? A counterfactual lens\n  on transport maps",
      "authors": [
        "Lucas De Lara",
        "Luca Ganassali"
      ],
      "abstract": "Coupling probability measures lies at the core of many problems in statistics\nand machine learning, from domain adaptation to transfer learning and causal\ninference. Yet, even when restricted to deterministic transports, such\ncouplings are not identifiable: two atomless marginals admit infinitely many\ntransport maps. The common recourse to optimal transport, motivated by cost\nminimization and cyclical monotonicity, obscures the fact that several distinct\nnotions of multivariate monotone matchings coexist. In this work, we first\ncarry a comparative analysis of three constructions of transport maps:\ncyclically monotone, quantile-preserving and triangular monotone maps. We\nestablish necessary and sufficient conditions for their equivalence, thereby\nclarifying their respective structural properties. In parallel, we formulate\ncounterfactual reasoning within the framework of structural causal models as a\nproblem of selecting transport maps between fixed marginals, which makes\nexplicit the role of untestable assumptions in counterfactual reasoning. Then,\nwe are able to connect these two perspectives by identifying conditions on\ncausal graphs and structural equations under which counterfactual maps coincide\nwith classical statistical transports. In this way, we delineate the\ncircumstances in which causal assumptions support the use of a specific\nstructure of transport map. Taken together, our results aim to enrich the\ntheoretical understanding of families of transport maps and to clarify their\npossible causal interpretations. We hope this work contributes to establishing\nnew bridges between statistical transport and causal inference.",
      "published": "September 19, 2025",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.ST",
        "stat.ME",
        "stat.TH"
      ],
      "pdf_link": "http://arxiv.org/pdf/2509.16027v1",
      "arxiv_url": "http://arxiv.org/abs/2509.16027v1"
    },
    {
      "title": "Transfer learning under latent space model",
      "authors": [
        "Kuangnan Fang",
        "Ruixuan Qin",
        "Xinyan Fan"
      ],
      "abstract": "Latent space model plays a crucial role in network analysis, and accurate\nestimation of latent variables is essential for downstream tasks such as link\nprediction. However, the large number of parameters to be estimated presents a\nchallenge, especially when the latent space dimension is not exceptionally\nsmall. In this paper, we propose a transfer learning method that leverages\ninformation from networks with latent variables similar to those in the target\nnetwork, thereby improving the estimation accuracy for the target. Given\ntransferable source networks, we introduce a two-stage transfer learning\nalgorithm that accommodates differences in node numbers between source and\ntarget networks. In each stage, we derive sufficient identification conditions\nand design tailored projected gradient descent algorithms for estimation.\nTheoretical properties of the resulting estimators are established. When the\ntransferable networks are unknown, a detection algorithm is introduced to\nidentify suitable source networks. Simulation studies and analyses of two real\ndatasets demonstrate the effectiveness of the proposed methods.",
      "published": "September 19, 2025",
      "categories": [
        "stat.ME",
        "stat.ML"
      ],
      "pdf_link": "http://arxiv.org/pdf/2509.15797v1",
      "arxiv_url": "http://arxiv.org/abs/2509.15797v1"
    },
    {
      "title": "SETrLUSI: Stochastic Ensemble Multi-Source Transfer Learning Using\n  Statistical Invariant",
      "authors": [
        "Chunna Li",
        "Yiwei Song",
        "Yuanhai Shao"
      ],
      "abstract": "In transfer learning, a source domain often carries diverse knowledge, and\ndifferent domains usually emphasize different types of knowledge. Different\nfrom handling only a single type of knowledge from all domains in traditional\ntransfer learning methods, we introduce an ensemble learning framework with a\nweak mode of convergence in the form of Statistical Invariant (SI) for\nmulti-source transfer learning, formulated as Stochastic Ensemble Multi-Source\nTransfer Learning Using Statistical Invariant (SETrLUSI). The proposed SI\nextracts and integrates various types of knowledge from both source and target\ndomains, which not only effectively utilizes diverse knowledge but also\naccelerates the convergence process. Further, SETrLUSI incorporates stochastic\nSI selection, proportional source domain sampling, and target domain\nbootstrapping, which improves training efficiency while enhancing model\nstability. Experiments show that SETrLUSI has good convergence and outperforms\nrelated methods with a lower time cost.",
      "published": "September 19, 2025",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "pdf_link": "http://arxiv.org/pdf/2509.15593v1",
      "arxiv_url": "http://arxiv.org/abs/2509.15593v1"
    },
    {
      "title": "Representation-Aware Distributionally Robust Optimization: A Knowledge\n  Transfer Framework",
      "authors": [
        "Zitao Wang",
        "Nian Si",
        "Molei Liu"
      ],
      "abstract": "We propose REpresentation-Aware Distributionally Robust Estimation (READ), a\nnovel framework for Wasserstein distributionally robust learning that accounts\nfor predictive representations when guarding against distributional shifts.\nUnlike classical approaches that treat all feature perturbations equally, READ\nembeds a multidimensional alignment parameter into the transport cost, allowing\nthe model to differentially discourage perturbations along directions\nassociated with informative representations. This yields robustness to feature\nvariation while preserving invariant structure. Our first contribution is a\ntheoretical foundation: we show that seminorm regularizations for linear\nregression and binary classification arise as Wasserstein distributionally\nrobust objectives, thereby providing tractable reformulations of READ and\nunifying a broad class of regularized estimators under the DRO lens. Second, we\nadopt a principled procedure for selecting the Wasserstein radius using the\ntechniques of robust Wasserstein profile inference. This further enables the\nconstruction of valid, representation-aware confidence regions for model\nparameters with distinct geometric features. Finally, we analyze the geometry\nof READ estimators as the alignment parameters vary and propose an optimization\nalgorithm to estimate the projection of the global optimum onto this solution\nsurface. This procedure selects among equally robust estimators while optimally\nconstructing a representation structure. We conclude by demonstrating the\neffectiveness of our framework through extensive simulations and a real-world\nstudy, providing a powerful robust estimation grounded in learning\nrepresentation.",
      "published": "September 11, 2025",
      "categories": [
        "stat.ME",
        "cs.LG"
      ],
      "pdf_link": "http://arxiv.org/pdf/2509.09371v1",
      "arxiv_url": "http://arxiv.org/abs/2509.09371v1"
    },
    {
      "title": "Robust and Adaptive Spectral Method for Representation Multi-Task\n  Learning with Contamination",
      "authors": [
        "Yian Huang",
        "Yang Feng",
        "Zhiliang Ying"
      ],
      "abstract": "Representation-based multi-task learning (MTL) improves efficiency by\nlearning a shared structure across tasks, but its practical application is\noften hindered by contamination, outliers, or adversarial tasks. Most existing\nmethods and theories assume a clean or near-clean setting, failing when\ncontamination is significant. This paper tackles representation MTL with an\nunknown and potentially large contamination proportion, while also allowing for\nheterogeneity among inlier tasks. We introduce a Robust and Adaptive Spectral\nmethod (RAS) that can distill the shared inlier representation effectively and\nefficiently, while requiring no prior knowledge of the contamination level or\nthe true representation dimension. Theoretically, we provide non-asymptotic\nerror bounds for both the learned representation and the per-task parameters.\nThese bounds adapt to inlier task similarity and outlier structure, and\nguarantee that RAS performs at least as well as single-task learning, thus\npreventing negative transfer. We also extend our framework to transfer learning\nwith corresponding theoretical guarantees for the target task. Extensive\nexperiments confirm our theory, showcasing the robustness and adaptivity of\nRAS, and its superior performance in regimes with up to 80\\% task\ncontamination.",
      "published": "September 08, 2025",
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.ME"
      ],
      "pdf_link": "http://arxiv.org/pdf/2509.06575v1",
      "arxiv_url": "http://arxiv.org/abs/2509.06575v1"
    }
  ]
}