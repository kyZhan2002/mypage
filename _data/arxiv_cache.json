{
  "timestamp": 1743038313,
  "papers": [
    {
      "title": "Similarity-Informed Transfer Learning for Multivariate Functional\n  Censored Quantile Regression",
      "authors": [
        "Hua Liu",
        "Jiaqi Men",
        "Shouxia Wang",
        "Jinhong You",
        "Jiguo Cao"
      ],
      "abstract": "To address the challenge of utilizing patient data from other organ\ntransplant centers (source cohorts) to improve survival time estimation and\ninference for a target center (target cohort) with limited samples and strict\ndata-sharing privacy constraints, we propose the Similarity-Informed Transfer\nLearning (SITL) method. This approach estimates multivariate functional\ncensored quantile regression by flexibly leveraging information from each\nsource cohort based on its similarity to the target cohort. Furthermore, the\nmethod is adaptable to continuously updated real-time data. We establish the\nasymptotic properties of the estimators obtained using the SITL method,\ndemonstrating improved convergence rates. Additionally, we develop an enhanced\napproach that combines the SITL method with a resampling technique to construct\nmore accurate confidence intervals for functional coefficients, backed by\ntheoretical guarantees. Extensive simulation studies and an application to\nkidney transplant data illustrate the significant advantages of the SITL\nmethod. Compared to methods that rely solely on the target cohort or\nindiscriminately pool data across source and target cohorts, the SITL method\nsubstantially improves both estimation and inference performance.",
      "published": "March 24, 2025",
      "categories": [
        "stat.ME"
      ],
      "pdf_link": "http://arxiv.org/pdf/2503.18437v1",
      "arxiv_url": "http://arxiv.org/abs/2503.18437v1"
    },
    {
      "title": "An Optimization Framework for Differentially Private Sparse Fine-Tuning",
      "authors": [
        "Mehdi Makni",
        "Kayhan Behdin",
        "Gabriel Afriat",
        "Zheng Xu",
        "Sergei Vassilvitskii",
        "Natalia Ponomareva",
        "Hussein Hazimeh",
        "Rahul Mazumder"
      ],
      "abstract": "Differentially private stochastic gradient descent (DP-SGD) is broadly\nconsidered to be the gold standard for training and fine-tuning neural networks\nunder differential privacy (DP). With the increasing availability of\nhigh-quality pre-trained model checkpoints (e.g., vision and language models),\nfine-tuning has become a popular strategy. However, despite recent progress in\nunderstanding and applying DP-SGD for private transfer learning tasks,\nsignificant challenges remain -- most notably, the performance gap between\nmodels fine-tuned with DP-SGD and their non-private counterparts. Sparse\nfine-tuning on private data has emerged as an alternative to full-model\nfine-tuning; recent work has shown that privately fine-tuning only a small\nsubset of model weights and keeping the rest of the weights fixed can lead to\nbetter performance. In this work, we propose a new approach for sparse\nfine-tuning of neural networks under DP. Existing work on private sparse\nfinetuning often used fixed choice of trainable weights (e.g., updating only\nthe last layer), or relied on public model's weights to choose the subset of\nweights to modify. Such choice of weights remains suboptimal. In contrast, we\nexplore an optimization-based approach, where our selection method makes use of\nthe private gradient information, while using off the shelf privacy accounting\ntechniques. Our numerical experiments on several computer vision models and\ndatasets show that our selection method leads to better prediction accuracy,\ncompared to full-model private fine-tuning or existing private sparse\nfine-tuning approaches.",
      "published": "March 17, 2025",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_link": "http://arxiv.org/pdf/2503.12822v1",
      "arxiv_url": "http://arxiv.org/abs/2503.12822v1"
    },
    {
      "title": "Mixed-feature Logistic Regression Robust to Distribution Shifts",
      "authors": [
        "Qingshi Sun",
        "Nathan Justin",
        "Andres Gomez",
        "Phebe Vayanos"
      ],
      "abstract": "Logistic regression models are widely used in the social and behavioral\nsciences and in high-stakes domains, due to their simplicity and\ninterpretability properties. At the same time, such domains are permeated by\ndistribution shifts, where the distribution generating the data changes between\ntraining and deployment. In this paper, we study a distributionally robust\nlogistic regression problem that seeks the model that will perform best against\nadversarial realizations of the data distribution drawn from a suitably\nconstructed Wasserstein ambiguity set. Our model and solution approach differ\nfrom prior work in that we can capture settings where the likelihood of\ndistribution shifts can vary across features, significantly broadening the\napplicability of our model relative to the state-of-the-art. We propose a\ngraph-based solution approach that can be integrated into off-the-shelf\noptimization solvers. We evaluate the performance of our model and algorithms\non numerous publicly available datasets. Our solution achieves a 408x speed-up\nrelative to the state-of-the-art. Additionally, compared to the\nstate-of-the-art, our model reduces average calibration error by up to 36.19%\nand worst-case calibration error by up to 41.70%, while increasing the average\narea under the ROC curve (AUC) by up to 18.02% and worst-case AUC by up to\n48.37%.",
      "published": "March 15, 2025",
      "categories": [
        "cs.LG",
        "math.OC",
        "stat.ML"
      ],
      "pdf_link": "http://arxiv.org/pdf/2503.12012v1",
      "arxiv_url": "http://arxiv.org/abs/2503.12012v1"
    }
  ]
}