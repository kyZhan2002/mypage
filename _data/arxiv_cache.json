{
  "timestamp": 1750814886,
  "papers": [
    {
      "title": "DRO-Augment Framework: Robustness by Synergizing Wasserstein\n  Distributionally Robust Optimization and Data Augmentation",
      "authors": [
        "Jiaming Hu",
        "Debarghya Mukherjee",
        "Ioannis Ch. Paschalidis"
      ],
      "abstract": "In many real-world applications, ensuring the robustness and stability of\ndeep neural networks (DNNs) is crucial, particularly for image classification\ntasks that encounter various input perturbations. While data augmentation\ntechniques have been widely adopted to enhance the resilience of a trained\nmodel against such perturbations, there remains significant room for\nimprovement in robustness against corrupted data and adversarial attacks\nsimultaneously. To address this challenge, we introduce DRO-Augment, a novel\nframework that integrates Wasserstein Distributionally Robust Optimization\n(W-DRO) with various data augmentation strategies to improve the robustness of\nthe models significantly across a broad spectrum of corruptions. Our method\noutperforms existing augmentation methods under severe data perturbations and\nadversarial attack scenarios while maintaining the accuracy on the clean\ndatasets on a range of benchmark datasets, including but not limited to\nCIFAR-10-C, CIFAR-100-C, MNIST, and Fashion-MNIST. On the theoretical side, we\nestablish novel generalization error bounds for neural networks trained using a\ncomputationally efficient, variation-regularized loss function closely related\nto the W-DRO problem.",
      "published": "June 22, 2025",
      "categories": [
        "stat.ML",
        "cs.CV",
        "cs.LG"
      ],
      "pdf_link": "http://arxiv.org/pdf/2506.17874v1",
      "arxiv_url": "http://arxiv.org/abs/2506.17874v1"
    },
    {
      "title": "On Design of Representative Distributionally Robust Formulations for\n  Evaluation of Tail Risk Measures",
      "authors": [
        "Anand Deo"
      ],
      "abstract": "Conditional Value-at-Risk (CVaR) is a risk measure widely used to quantify\nthe impact of extreme losses. Owing to the lack of representative samples CVaR\nis sensitive to the tails of the underlying distribution. In order to combat\nthis sensitivity, Distributionally Robust Optimization (DRO), which evaluates\nthe worst-case CVaR measure over a set of plausible data distributions is often\ndeployed. Unfortunately, an improper choice of the DRO formulation can lead to\na severe underestimation of tail risk. This paper aims at leveraging extreme\nvalue theory to arrive at a DRO formulation which leads to representative\nworst-case CVaR evaluations in that the above pitfall is avoided while\nsimultaneously, the worst case evaluation is not a gross over-estimate of the\ntrue CVaR. We demonstrate theoretically that even when there is paucity of\nsamples in the tail of the distribution, our formulation is readily\nimplementable from data, only requiring calibration of a single scalar\nparameter. We showcase that our formulation can be easily extended to provide\nrobustness to tail risk in multivariate applications as well as in the\nevaluation of other commonly used risk measures. Numerical illustrations on\nsynthetic and real-world data showcase the practical utility of our approach.",
      "published": "June 19, 2025",
      "categories": [
        "q-fin.RM",
        "math.PR",
        "stat.ME",
        "stat.ML"
      ],
      "pdf_link": "http://arxiv.org/pdf/2506.16230v1",
      "arxiv_url": "http://arxiv.org/abs/2506.16230v1"
    },
    {
      "title": "Adjustment for Confounding using Pre-Trained Representations",
      "authors": [
        "Rickmer Schulte",
        "David R\u00fcgamer",
        "Thomas Nagler"
      ],
      "abstract": "There is growing interest in extending average treatment effect (ATE)\nestimation to incorporate non-tabular data, such as images and text, which may\nact as sources of confounding. Neglecting these effects risks biased results\nand flawed scientific conclusions. However, incorporating non-tabular data\nnecessitates sophisticated feature extractors, often in combination with ideas\nof transfer learning. In this work, we investigate how latent features from\npre-trained neural networks can be leveraged to adjust for sources of\nconfounding. We formalize conditions under which these latent features enable\nvalid adjustment and statistical inference in ATE estimation, demonstrating\nresults along the example of double machine learning. We discuss critical\nchallenges inherent to latent feature learning and downstream parameter\nestimation arising from the high dimensionality and non-identifiability of\nrepresentations. Common structural assumptions for obtaining fast convergence\nrates with additive or sparse linear models are shown to be unrealistic for\nlatent features. We argue, however, that neural networks are largely\ninsensitive to these issues. In particular, we show that neural networks can\nachieve fast convergence rates by adapting to intrinsic notions of sparsity and\ndimension of the learning problem.",
      "published": "June 17, 2025",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG",
        "stat.CO",
        "stat.ME"
      ],
      "pdf_link": "http://arxiv.org/pdf/2506.14329v1",
      "arxiv_url": "http://arxiv.org/abs/2506.14329v1"
    },
    {
      "title": "A Transfer Learning Framework for Multilayer Networks via Model\n  Averaging",
      "authors": [
        "Yongqin Qiu",
        "Xinyu Zhang"
      ],
      "abstract": "Link prediction in multilayer networks is a key challenge in applications\nsuch as recommendation systems and protein-protein interaction prediction.\nWhile many techniques have been developed, most rely on assumptions about\nshared structures and require access to raw auxiliary data, limiting their\npracticality. To address these issues, we propose a novel transfer learning\nframework for multilayer networks using a bi-level model averaging method. A\n$K$-fold cross-validation criterion based on edges is used to automatically\nweight inter-layer and intra-layer candidate models. This enables the transfer\nof information from auxiliary layers while mitigating model uncertainty, even\nwithout prior knowledge of shared structures. Theoretically, we prove the\noptimality and weight convergence of our method under mild conditions.\nComputationally, our framework is efficient and privacy-preserving, as it\navoids raw data sharing and supports parallel processing across multiple\nservers. Simulations show our method outperforms others in predictive accuracy\nand robustness. We further demonstrate its practical value through two\nreal-world recommendation system applications.",
      "published": "June 14, 2025",
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.ME"
      ],
      "pdf_link": "http://arxiv.org/pdf/2506.12455v1",
      "arxiv_url": "http://arxiv.org/abs/2506.12455v1"
    },
    {
      "title": "Coefficient Shape Transfer Learning for Functional Linear Regression",
      "authors": [
        "Shuhao Jiao",
        "Ian W. Mckeague",
        "N. -H. Chan"
      ],
      "abstract": "In this paper, we develop a novel transfer learning methodology to tackle the\nchallenge of data scarcity in functional linear models. The methodology\nincorporates samples from the target model (target domain) alongside those from\nauxiliary models (source domains), transferring knowledge of coefficient shape\nfrom the source domains to the target domain. This shape-based knowledge\ntransfer offers two key advantages. First, it is robust to covariate scaling,\nensuring effectiveness despite variations in data distributions across\ndifferent source domains. Second, the notion of coefficient shape homogeneity\nrepresents a meaningful advance beyond traditional coefficient homogeneity,\nallowing the method to exploit a wider range of source domains and achieve\nsignificantly improved model estimation. We rigorously analyze the convergence\nrates of the proposed estimator and examine the minimax optimality. Our\nfindings show that the degree of improvement depends not only on the similarity\nof coefficient shapes between the target and source domains, but also on\ncoefficient magnitudes and the spectral decay rates of the functional\ncovariates covariance operators. To address situations where only a subset of\nauxiliary models is informative for the target model, we further develop a\ndata-driven procedure for identifying such informative sources. The\neffectiveness of the proposed methodology is demonstrated through comprehensive\nsimulation studies and an application to occupation time analysis using\nphysical activity data from the U.S. National Health and Nutrition Examination\nSurvey.",
      "published": "June 13, 2025",
      "categories": [
        "stat.ME",
        "stat.ML"
      ],
      "pdf_link": "http://arxiv.org/pdf/2506.11367v1",
      "arxiv_url": "http://arxiv.org/abs/2506.11367v1"
    }
  ]
}