{
  "last_fetch_timestamp": 1770602854,
  "papers": [
    {
      "arxiv_id": "2602.01825",
      "title": "Learning Sequential Decisions from Multiple Sources via Group-Robust Markov Decision Processes",
      "authors": [
        "Mingyuan Xu",
        "Zongqi Xia",
        "Tianxi Cai",
        "Doudou Zhou",
        "Nian Si"
      ],
      "abstract": "We often collect data from multiple sites (e.g., hospitals) that share common structure but also exhibit heterogeneity. This paper aims to learn robust sequential decision-making policies from such offline, multi-site datasets. To model cross-site uncertainty, we study distributionally robust MDPs with a group-linear structure: all sites share a common feature map, and both the transition kernels and expected reward functions are linear in these shared features. We introduce feature-wise (d-rectangular) uncertainty sets, which preserve tractable robust Bellman recursions while maintaining key cross-site structure. Building on this, we then develop an offline algorithm based on pessimistic value iteration that includes: (i) per-site ridge regression for Bellman targets, (ii) feature-wise worst-case (row-wise minimization) aggregation, and (iii) a data-dependent pessimism penalty computed from the diagonals of the inverse design matrices. We further propose a cluster-level extension that pools similar sites to improve sample efficiency, guided by prior knowledge of site similarity. Under a robust partial coverage assumption, we prove a suboptimality bound for the resulting policy. Overall, our framework addresses multi-site learning with heterogeneous data sources and provides a principled approach to robust planning without relying on strong state-action rectangularity assumptions.",
      "published": "February 02, 2026",
      "published_raw": "2026-02-02T08:58:55Z",
      "categories": [
        "stat.ME",
        "cs.LG",
        "math.OC",
        "stat.ML"
      ],
      "pdf_link": "https://arxiv.org/pdf/2602.01825v1",
      "arxiv_url": "https://arxiv.org/abs/2602.01825v1",
      "added_timestamp": 1770170317
    },
    {
      "arxiv_id": "2602.01427",
      "title": "Robust Generalization with Adaptive Optimal Transport Priors for Decision-Focused Learning",
      "authors": [
        "Haixiang Sun",
        "Andrew L. Liu"
      ],
      "abstract": "Few-shot learning requires models to generalize under limited supervision while remaining robust to distribution shifts. Existing Sinkhorn Distributionally Robust Optimization (DRO) methods provide theoretical guarantees but rely on a fixed reference distribution, which limits their adaptability. We propose a Prototype-Guided Distributionally Robust Optimization (PG-DRO) framework that learns class-adaptive priors from abundant base data via hierarchical optimal transport and embeds them into the Sinkhorn DRO formulation. This design enables few-shot information to be organically integrated into producing class-specific robust decisions that are both theoretically grounded and efficient, and further aligns the uncertainty set with transferable structural knowledge. Experiments show that PG-DRO achieves stronger robust generalization in few-shot scenarios, outperforming both standard learners and DRO baselines.",
      "published": "February 01, 2026",
      "published_raw": "2026-02-01T20:22:41Z",
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.AP"
      ],
      "pdf_link": "https://arxiv.org/pdf/2602.01427v1",
      "arxiv_url": "https://arxiv.org/abs/2602.01427v1",
      "added_timestamp": 1770170317
    },
    {
      "arxiv_id": "2602.00844",
      "title": "Multivariate Time Series Data Imputation via Distributionally Robust Regularization",
      "authors": [
        "Che-Yi Liao",
        "Zheng Dong",
        "Gian-Gabriel Garcia",
        "Kamran Paynabar"
      ],
      "abstract": "Multivariate time series (MTS) imputation is often compromised by mismatch between observed and true data distributions -- a bias exacerbated by non-stationarity and systematic missingness. Standard methods that minimize reconstruction error or encourage distributional alignment risk overfitting these biased observations. We propose the Distributionally Robust Regularized Imputer Objective (DRIO), which jointly minimizes reconstruction error and the divergence between the imputer and a worst-case distribution within a Wasserstein ambiguity set. We derive a tractable dual formulation that reduces infinite-dimensional optimization over measures to adversarial search over sample trajectories, and propose an adversarial learning algorithm compatible with flexible deep learning backbones. Comprehensive experiments on diverse real-world datasets show DRIO consistently improves imputation under both missing-completely-at-random and missing-not-at-random settings, reaching Pareto-optimal trade-offs between reconstruction accuracy and distributional alignment.",
      "published": "January 31, 2026",
      "published_raw": "2026-01-31T18:15:03Z",
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.AP"
      ],
      "pdf_link": "https://arxiv.org/pdf/2602.00844v1",
      "arxiv_url": "https://arxiv.org/abs/2602.00844v1",
      "added_timestamp": 1770084228
    },
    {
      "arxiv_id": "2601.21324",
      "title": "Bulk-Calibrated Credal Ambiguity Sets: Fast, Tractable Decision Making under Out-of-Sample Contamination",
      "authors": [
        "Mengqi Chen",
        "Thomas B. Berrett",
        "Theodoros Damoulas",
        "Michele Caprio"
      ],
      "abstract": "Distributionally robust optimisation (DRO) minimises the worst-case expected loss over an ambiguity set that can capture distributional shifts in out-of-sample environments. While Huber (linear-vacuous) contamination is a classical minimal-assumption model for an $\\varepsilon$-fraction of arbitrary perturbations, including it in an ambiguity set can make the worst-case risk infinite and the DRO objective vacuous unless one imposes strong boundedness or support assumptions. We address these challenges by introducing bulk-calibrated credal ambiguity sets: we learn a high-mass bulk set from data while considering contamination inside the bulk and bounding the remaining tail contribution separately. This leads to a closed-form, finite $\\mathrm{mean}+\\sup$ robust objective and tractable linear or second-order cone programs for common losses and bulk geometries. Through this framework, we highlight and exploit the equivalence between the imprecise probability (IP) notion of upper expectation and the worst-case risk, demonstrating how IP credal sets translate into DRO objectives with interpretable tolerance levels. Experiments on heavy-tailed inventory control, geographically shifted house-price regression, and demographically shifted text classification show competitive robustness-accuracy trade-offs and efficient optimisation times, using Bayesian, frequentist, or empirical reference distributions.",
      "published": "January 29, 2026",
      "published_raw": "2026-01-29T06:37:36Z",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "pdf_link": "https://arxiv.org/pdf/2601.21324v1",
      "arxiv_url": "https://arxiv.org/abs/2601.21324v1",
      "added_timestamp": 1769824413
    },
    {
      "arxiv_id": "2601.11016",
      "title": "Contextual Distributionally Robust Optimization with Causal and Continuous Structure: An Interpretable and Tractable Approach",
      "authors": [
        "Fenglin Zhang",
        "Jie Wang"
      ],
      "abstract": "In this paper, we introduce a framework for contextual distributionally robust optimization (DRO) that considers the causal and continuous structure of the underlying distribution by developing interpretable and tractable decision rules that prescribe decisions using covariates. We first introduce the causal Sinkhorn discrepancy (CSD), an entropy-regularized causal Wasserstein distance that encourages continuous transport plans while preserving the causal consistency. We then formulate a contextual DRO model with a CSD-based ambiguity set, termed Causal Sinkhorn DRO (Causal-SDRO), and derive its strong dual reformulation where the worst-case distribution is characterized as a mixture of Gibbs distributions. To solve the corresponding infinite-dimensional policy optimization, we propose the Soft Regression Forest (SRF) decision rule, which approximates optimal policies within arbitrary measurable function spaces. The SRF preserves the interpretability of classical decision trees while being fully parametric, differentiable, and Lipschitz smooth, enabling intrinsic interpretation from both global and local perspectives. To solve the Causal-SDRO with parametric decision rules, we develop an efficient stochastic compositional gradient algorithm that converges to an $\\varepsilon$-stationary point at a rate of $O(\\varepsilon^{-4})$, matching the convergence rate of standard stochastic gradient descent. Finally, we validate our method through numerical experiments on synthetic and real-world datasets, demonstrating its superior performance and interpretability.",
      "published": "January 16, 2026",
      "published_raw": "2026-01-16T06:18:22Z",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG",
        "math.OC"
      ],
      "pdf_link": "https://arxiv.org/pdf/2601.11016v1",
      "arxiv_url": "https://arxiv.org/abs/2601.11016v1",
      "added_timestamp": 1768873346
    },
    {
      "arxiv_id": "2601.06807",
      "title": "Adversarially Perturbed Precision Matrix Estimation",
      "authors": [
        "Yiling Xie"
      ],
      "abstract": "Precision matrix estimation is a fundamental topic in multivariate statistics and modern machine learning. This paper proposes an adversarially perturbed precision matrix estimation framework, motivated by recent developments in adversarial training. The proposed framework is versatile for the precision matrix problem since, by adapting to different perturbation geometries, the proposed framework can not only recover the existing distributionally robust method but also inspire a novel moment-adaptive approach to precision matrix estimation, proven capable of sparsity recovery and adversarial robustness. Notably, the proposed perturbed precision matrix framework is proven to be asymptotically equivalent to regularized precision matrix estimation, and the asymptotic normality can be established accordingly. The resulting asymptotic distribution highlights the asymptotic bias introduced by perturbation and identifies conditions under which the perturbed estimation can be unbiased in the asymptotic sense. Numerical experiments on both synthetic and real data demonstrate the desirable performance of the proposed adversarially perturbed approach in practice.",
      "published": "January 11, 2026",
      "published_raw": "2026-01-11T08:40:56Z",
      "categories": [
        "stat.ME"
      ],
      "pdf_link": "https://arxiv.org/pdf/2601.06807v1",
      "arxiv_url": "https://arxiv.org/abs/2601.06807v1",
      "added_timestamp": 1768355145
    },
    {
      "arxiv_id": "2601.05975",
      "title": "DeePM: Regime-Robust Deep Learning for Systematic Macro Portfolio Management",
      "authors": [
        "Kieran Wood",
        "Stephen J. Roberts",
        "Stefan Zohren"
      ],
      "abstract": "We propose DeePM (Deep Portfolio Manager), a structured deep-learning macro portfolio manager trained end-to-end to maximize a robust, risk-adjusted utility. DeePM addresses three fundamental challenges in financial learning: (1) it resolves the asynchronous \"ragged filtration\" problem via a Directed Delay (Causal Sieve) mechanism that prioritizes causal impulse-response learning over information freshness; (2) it combats low signal-to-noise ratios via a Macroeconomic Graph Prior, regularizing cross-asset dependence according to economic first principles; and (3) it optimizes a distributionally robust objective where a smooth worst-window penalty serves as a differentiable proxy for Entropic Value-at-Risk (EVaR) - a window-robust utility encouraging strong performance in the most adverse historical subperiods. In large-scale backtests from 2010-2025 on 50 diversified futures with highly realistic transaction costs, DeePM attains net risk-adjusted returns that are roughly twice those of classical trend-following strategies and passive benchmarks, solely using daily closing prices. Furthermore, DeePM improves upon the state-of-the-art Momentum Transformer architecture by roughly fifty percent. The model demonstrates structural resilience across the 2010s \"CTA (Commodity Trading Advisor) Winter\" and the post-2020 volatility regime shift, maintaining consistent performance through the pandemic, inflation shocks, and the subsequent higher-for-longer environment. Ablation studies confirm that strictly lagged cross-sectional attention, graph prior, principled treatment of transaction costs, and robust minimax optimization are the primary drivers of this generalization capability.",
      "published": "January 09, 2026",
      "published_raw": "2026-01-09T17:47:32Z",
      "categories": [
        "q-fin.TR",
        "cs.LG",
        "stat.ML"
      ],
      "pdf_link": "https://arxiv.org/pdf/2601.05975v1",
      "arxiv_url": "https://arxiv.org/abs/2601.05975v1",
      "added_timestamp": 1768268311
    },
    {
      "arxiv_id": "2601.04608",
      "title": "Forecasting the U.S. Treasury Yield Curve: A Distributionally Robust Machine Learning Approach",
      "authors": [
        "Jinjun Liu",
        "Ming-Yen Cheng"
      ],
      "abstract": "We study U.S. Treasury yield curve forecasting under distributional uncertainty and recast forecasting as an operations research and managerial decision problem. Rather than minimizing average forecast error, the forecaster selects a decision rule that minimizes worst case expected loss over an ambiguity set of forecast error distributions. To this end, we propose a distributionally robust ensemble forecasting framework that integrates parametric factor models with high dimensional nonparametric machine learning models through adaptive forecast combinations. The framework consists of three machine learning components. First, a rolling window Factor Augmented Dynamic Nelson Siegel model captures level, slope, and curvature dynamics using principal components extracted from economic indicators. Second, Random Forest models capture nonlinear interactions among macro financial drivers and lagged Treasury yields. Third, distributionally robust forecast combination schemes aggregate heterogeneous forecasts under moment uncertainty, penalizing downside tail risk via expected shortfall and stabilizing second moment estimation through ridge regularized covariance matrices. The severity of the worst case criterion is adjustable, allowing the forecaster to regulate the trade off between robustness and statistical efficiency. Using monthly data, we evaluate out of sample forecasts across maturities and horizons from one to twelve months ahead. Adaptive combinations deliver superior performance at short horizons, while Random Forest forecasts dominate at longer horizons. Extensions to global sovereign bond yields confirm the stability and generalizability of the proposed framework.",
      "published": "January 08, 2026",
      "published_raw": "2026-01-08T05:26:43Z",
      "categories": [
        "q-fin.MF",
        "q-fin.CP",
        "stat.ML"
      ],
      "pdf_link": "https://arxiv.org/pdf/2601.04608v1",
      "arxiv_url": "https://arxiv.org/abs/2601.04608v1",
      "added_timestamp": 1768096201
    }
  ]
}