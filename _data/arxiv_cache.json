{
  "timestamp": 1761009746,
  "papers": [
    {
      "title": "Transfer Learning for Benign Overfitting in High-Dimensional Linear\n  Regression",
      "authors": [
        "Yeichan Kim",
        "Ilmun Kim",
        "Seyoung Park"
      ],
      "abstract": "Transfer learning is a key component of modern machine learning, enhancing\nthe performance of target tasks by leveraging diverse data sources.\nSimultaneously, overparameterized models such as the minimum-$\\ell_2$-norm\ninterpolator (MNI) in high-dimensional linear regression have garnered\nsignificant attention for their remarkable generalization capabilities, a\nproperty known as benign overfitting. Despite their individual importance, the\nintersection of transfer learning and MNI remains largely unexplored. Our\nresearch bridges this gap by proposing a novel two-step Transfer MNI approach\nand analyzing its trade-offs. We characterize its non-asymptotic excess risk\nand identify conditions under which it outperforms the target-only MNI. Our\nanalysis reveals free-lunch covariate shift regimes, where leveraging\nheterogeneous data yields the benefit of knowledge transfer at limited cost. To\noperationalize our findings, we develop a data-driven procedure to detect\ninformative sources and introduce an ensemble method incorporating multiple\ninformative Transfer MNIs. Finite-sample experiments demonstrate the robustness\nof our methods to model and data heterogeneity, confirming their advantage.",
      "published": "October 17, 2025",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.ST",
        "stat.TH"
      ],
      "pdf_link": "http://arxiv.org/pdf/2510.15337v1",
      "arxiv_url": "http://arxiv.org/abs/2510.15337v1"
    },
    {
      "title": "Stress-Aware Learning under KL Drift via Trust-Decayed Mirror Descent",
      "authors": [
        "Gabriel Nixon Raj"
      ],
      "abstract": "We study sequential decision-making under distribution drift. We propose\nentropy-regularized trust-decay, which injects stress-aware exponential tilting\ninto both belief updates and mirror-descent decisions. On the simplex, a\nFenchel-dual equivalence shows that belief tilt and decision tilt coincide. We\nformalize robustness via fragility (worst-case excess risk in a KL ball),\nbelief bandwidth (radius sustaining a target excess), and a decision-space\nFragility Index (drift tolerated at $O(\\sqrt{T})$ regret). We prove\nhigh-probability sensitivity bounds and establish dynamic-regret guarantees of\n$\\tilde{O}(\\sqrt{T})$ under KL-drift path length $S_T = \\sum_{t\\ge2}\\sqrt{{\\rm\nKL}(D_t|D_{t-1})/2}$. In particular, trust-decay achieves $O(1)$ per-switch\nregret, while stress-free updates incur $\\Omega(1)$ tails. A parameter-free\nhedge adapts the tilt to unknown drift, whereas persistent over-tilting yields\nan $\\Omega(\\lambda^2 T)$ stationary penalty. We further obtain\ncalibrated-stress bounds and extensions to second-order updates, bandit\nfeedback, outliers, stress variation, distributed optimization, and plug-in\nKL-drift estimation. The framework unifies dynamic-regret analysis,\ndistributionally robust objectives, and KL-regularized control within a single\nstress-adaptive update.",
      "published": "October 17, 2025",
      "categories": [
        "cs.LG",
        "math.ST",
        "stat.TH"
      ],
      "pdf_link": "http://arxiv.org/pdf/2510.15222v1",
      "arxiv_url": "http://arxiv.org/abs/2510.15222v1"
    },
    {
      "title": "Evaluating Policy Effects under Network Interference without Network\n  Information: A Transfer Learning Approach",
      "authors": [
        "Tadao Hoshino"
      ],
      "abstract": "This paper develops a sensitivity analysis framework that transfers the\naverage total treatment effect (ATTE) from source data with a fully observed\nnetwork to target data whose network is completely unknown. The ATTE represents\nthe average social impact of a policy that assigns the treatment to every\nindividual in the dataset. We postulate a covariate-shift type assumption that\nboth source and target datasets share the same conditional mean outcome.\nHowever, because the target network is unobserved, this assumption alone is not\nsufficient to pin down the ATTE for the target data. To address this issue, we\nconsider a sensitivity analysis based on the uncertainty of the target\nnetwork's degree distribution, where the extent of uncertainty is measured by\nthe Wasserstein distance from a given reference degree distribution. We then\nconstruct bounds on the target ATTE using a linear programming-based estimator.\nThe limiting distribution of the bound estimator is derived via the functional\ndelta method, and we develop a wild bootstrap approach to approximate the\ndistribution. As an empirical illustration, we revisit the social network\nexperiment on farmers' weather insurance adoption in China by Cai et al.\n(2015).",
      "published": "October 16, 2025",
      "categories": [
        "stat.ME",
        "econ.EM",
        "math.ST",
        "stat.TH"
      ],
      "pdf_link": "http://arxiv.org/pdf/2510.14415v1",
      "arxiv_url": "http://arxiv.org/abs/2510.14415v1"
    },
    {
      "title": "Policy Regularized Distributionally Robust Markov Decision Processes\n  with Linear Function Approximation",
      "authors": [
        "Jingwen Gu",
        "Yiting He",
        "Zhishuai Liu",
        "Pan Xu"
      ],
      "abstract": "Decision-making under distribution shift is a central challenge in\nreinforcement learning (RL), where training and deployment environments differ.\nWe study this problem through the lens of robust Markov decision processes\n(RMDPs), which optimize performance against adversarial transition dynamics.\nOur focus is the online setting, where the agent has only limited interaction\nwith the environment, making sample efficiency and exploration especially\ncritical. Policy optimization, despite its success in standard RL, remains\ntheoretically and empirically underexplored in robust RL. To bridge this gap,\nwe propose \\textbf{D}istributionally \\textbf{R}obust \\textbf{R}egularized\n\\textbf{P}olicy \\textbf{O}ptimization algorithm (DR-RPO), a model-free online\npolicy optimization method that learns robust policies with sublinear regret.\nTo enable tractable optimization within the softmax policy class, DR-RPO\nincorporates reference-policy regularization, yielding RMDP variants that are\ndoubly constrained in both transitions and policies. To scale to large\nstate-action spaces, we adopt the $d$-rectangular linear MDP formulation and\ncombine linear function approximation with an upper confidence bonus for\noptimistic exploration. We provide theoretical guarantees showing that policy\noptimization can achieve polynomial suboptimality bounds and sample efficiency\nin robust RL, matching the performance of value-based approaches. Finally,\nempirical results across diverse domains corroborate our theory and demonstrate\nthe robustness of DR-RPO.",
      "published": "October 16, 2025",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "pdf_link": "http://arxiv.org/pdf/2510.14246v1",
      "arxiv_url": "http://arxiv.org/abs/2510.14246v1"
    },
    {
      "title": "Transfer Learning with Distance Covariance for Random Forest: Error\n  Bounds and an EHR Application",
      "authors": [
        "Chenze Li",
        "Subhadeep Paul"
      ],
      "abstract": "Random forest is an important method for ML applications due to its broad\noutperformance over competing methods for structured tabular data. We propose a\nmethod for transfer learning in nonparametric regression using a centered\nrandom forest (CRF) with distance covariance-based feature weights, assuming\nthe unknown source and target regression functions are different for a few\nfeatures (sparsely different). Our method first obtains residuals from\npredicting the response in the target domain using a source domain-trained CRF.\nThen, we fit another CRF to the residuals, but with feature splitting\nprobabilities proportional to the sample distance covariance between the\nfeatures and the residuals in an independent sample. We derive an upper bound\non the mean square error rate of the procedure as a function of sample sizes\nand difference dimension, theoretically demonstrating transfer learning\nbenefits in random forests. In simulations, we show that the results obtained\nfor the CRFs also hold numerically for the standard random forest (SRF) method\nwith data-driven feature split selection. Beyond transfer learning, our results\nalso show the benefit of distance-covariance-based weights on the performance\nof RF in some situations. Our method shows significant gains in predicting the\nmortality of ICU patients in smaller-bed target hospitals using a large\nmulti-hospital dataset of electronic health records for 200,000 ICU patients.",
      "published": "October 13, 2025",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.ST",
        "stat.ME",
        "stat.TH"
      ],
      "pdf_link": "http://arxiv.org/pdf/2510.10870v1",
      "arxiv_url": "http://arxiv.org/abs/2510.10870v1"
    },
    {
      "title": "Quantifying Dataset Similarity to Guide Transfer Learning",
      "authors": [
        "Shudong Sun",
        "Hao Helen Zhang"
      ],
      "abstract": "Transfer learning has become a cornerstone of modern machine learning, as it\ncan empower models by leveraging knowledge from related domains to improve\nlearning effectiveness. However, transferring from poorly aligned data can harm\nrather than help performance, making it crucial to determine whether the\ntransfer will be beneficial before implementation. This work aims to address\nthis challenge by proposing an innovative metric to measure dataset similarity\nand provide quantitative guidance on transferability. In the literature,\nexisting methods largely focus on feature distributions while overlooking label\ninformation and predictive relationships, potentially missing critical\ntransferability insights. In contrast, our proposed metric, the Cross-Learning\nScore (CLS), measures dataset similarity through bidirectional generalization\nperformance between domains. We provide a theoretical justification for CLS by\nestablishing its connection to the cosine similarity between the decision\nboundaries for the target and source datasets. Computationally, CLS is\nefficient and fast to compute as it bypasses the problem of expensive\ndistribution estimation for high-dimensional problems. We further introduce a\ngeneral framework that categorizes source datasets into positive, ambiguous, or\nnegative transfer zones based on their CLS relative to the baseline error,\nenabling informed decisions. Additionally, we extend this approach to\nencoder-head architectures in deep learning to better reflect modern transfer\npipelines. Extensive experiments on diverse synthetic and real-world tasks\ndemonstrate that CLS can reliably predict whether transfer will improve or\ndegrade performance, offering a principled tool for guiding data selection in\ntransfer learning.",
      "published": "October 13, 2025",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "pdf_link": "http://arxiv.org/pdf/2510.10866v1",
      "arxiv_url": "http://arxiv.org/abs/2510.10866v1"
    },
    {
      "title": "Tight Robustness Certificates and Wasserstein Distributional Attacks for\n  Deep Neural Networks",
      "authors": [
        "Bach C. Le",
        "Tung V. Dao",
        "Binh T. Nguyen",
        "Hong T. M. Chu"
      ],
      "abstract": "Wasserstein distributionally robust optimization (WDRO) provides a framework\nfor adversarial robustness, yet existing methods based on global Lipschitz\ncontinuity or strong duality often yield loose upper bounds or require\nprohibitive computation. In this work, we address these limitations by\nintroducing a primal approach and adopting a notion of exact Lipschitz\ncertificate to tighten this upper bound of WDRO. In addition, we propose a\nnovel Wasserstein distributional attack (WDA) that directly constructs a\ncandidate for the worst-case distribution. Compared to existing point-wise\nattack and its variants, our WDA offers greater flexibility in the number and\nlocation of attack points. In particular, by leveraging the piecewise-affine\nstructure of ReLU networks on their activation cells, our approach results in\nan exact tractable characterization of the corresponding WDRO problem.\nExtensive evaluations demonstrate that our method achieves competitive robust\naccuracy against state-of-the-art baselines while offering tighter certificates\nthan existing methods. Our code is available at\nhttps://github.com/OLab-Repo/WDA",
      "published": "October 11, 2025",
      "categories": [
        "cs.LG",
        "math.OC",
        "stat.ML"
      ],
      "pdf_link": "http://arxiv.org/pdf/2510.10000v1",
      "arxiv_url": "http://arxiv.org/abs/2510.10000v1"
    },
    {
      "title": "Distributionally robust approximation property of neural networks",
      "authors": [
        "Mihriban Ceylan",
        "David J. Pr\u00f6mel"
      ],
      "abstract": "The universal approximation property uniformly with respect to weakly compact\nfamilies of measures is established for several classes of neural networks. To\nthat end, we prove that these neural networks are dense in Orlicz spaces,\nthereby extending classical universal approximation theorems even beyond the\ntraditional $L^p$-setting. The covered classes of neural networks include\nwidely used architectures like feedforward neural networks with non-polynomial\nactivation functions, deep narrow networks with ReLU activation functions and\nfunctional input neural networks.",
      "published": "October 10, 2025",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.FA",
        "math.PR"
      ],
      "pdf_link": "http://arxiv.org/pdf/2510.09177v1",
      "arxiv_url": "http://arxiv.org/abs/2510.09177v1"
    },
    {
      "title": "Structured Output Regularization: a framework for few-shot transfer\n  learning",
      "authors": [
        "Nicolas Ewen",
        "Jairo Diaz-Rodriguez",
        "Kelly Ramsay"
      ],
      "abstract": "Traditional transfer learning typically reuses large pre-trained networks by\nfreezing some of their weights and adding task-specific layers. While this\napproach is computationally efficient, it limits the model's ability to adapt\nto domain-specific features and can still lead to overfitting with very limited\ndata. To address these limitations, we propose Structured Output Regularization\n(SOR), a simple yet effective framework that freezes the internal network\nstructures (e.g., convolutional filters) while using a combination of group\nlasso and $L_1$ penalties. This framework tailors the model to specific data\nwith minimal additional parameters and is easily applicable to various network\ncomponents, such as convolutional filters or various blocks in neural networks\nenabling broad applicability for transfer learning tasks. We evaluate SOR on\nthree few shot medical imaging classification tasks and we achieve competitive\nresults using DenseNet121, and EfficientNetB4 bases compared to established\nbenchmarks.",
      "published": "October 09, 2025",
      "categories": [
        "cs.CV",
        "cs.LG",
        "stat.ML"
      ],
      "pdf_link": "http://arxiv.org/pdf/2510.08728v1",
      "arxiv_url": "http://arxiv.org/abs/2510.08728v1"
    },
    {
      "title": "Scalable deep fusion of spaceborne lidar and synthetic aperture radar\n  for global forest structural complexity mapping",
      "authors": [
        "Tiago de Conto",
        "John Armston",
        "Ralph Dubayah"
      ],
      "abstract": "Forest structural complexity metrics integrate multiple canopy attributes\ninto a single value that reflects habitat quality and ecosystem function.\nSpaceborne lidar from the Global Ecosystem Dynamics Investigation (GEDI) has\nenabled mapping of structural complexity in temperate and tropical forests, but\nits sparse sampling limits continuous high-resolution mapping. We present a\nscalable, deep learning framework fusing GEDI observations with multimodal\nSynthetic Aperture Radar (SAR) datasets to produce global, high-resolution (25\nm) wall-to-wall maps of forest structural complexity. Our adapted\nEfficientNetV2 architecture, trained on over 130 million GEDI footprints,\nachieves high performance (global R2 = 0.82) with fewer than 400,000\nparameters, making it an accessible tool that enables researchers to process\ndatasets at any scale without requiring specialized computing infrastructure.\nThe model produces accurate predictions with calibrated uncertainty estimates\nacross biomes and time periods, preserving fine-scale spatial patterns. It has\nbeen used to generate a global, multi-temporal dataset of forest structural\ncomplexity from 2015 to 2022. Through transfer learning, this framework can be\nextended to predict additional forest structural variables with minimal\ncomputational cost. This approach supports continuous, multi-temporal\nmonitoring of global forest structural dynamics and provides tools for\nbiodiversity conservation and ecosystem management efforts in a changing\nclimate.",
      "published": "October 07, 2025",
      "categories": [
        "cs.CV",
        "cs.LG",
        "stat.AP"
      ],
      "pdf_link": "http://arxiv.org/pdf/2510.06299v1",
      "arxiv_url": "http://arxiv.org/abs/2510.06299v1"
    },
    {
      "title": "Transfer Learning on Edge Connecting Probability Estimation under\n  Graphon Model",
      "authors": [
        "Yuyao Wang",
        "Yu-Hung Cheng",
        "Debarghya Mukherjee",
        "Huimin Cheng"
      ],
      "abstract": "Graphon models provide a flexible nonparametric framework for estimating\nlatent connectivity probabilities in networks, enabling a range of downstream\napplications such as link prediction and data augmentation. However, accurate\ngraphon estimation typically requires a large graph, whereas in practice, one\noften only observes a small-sized network. One approach to addressing this\nissue is to adopt a transfer learning framework, which aims to improve\nestimation in a small target graph by leveraging structural information from a\nlarger, related source graph. In this paper, we propose a novel method, namely\nGTRANS, a transfer learning framework that integrates neighborhood smoothing\nand Gromov-Wasserstein optimal transport to align and transfer structural\npatterns between graphs. To prevent negative transfer, GTRANS includes an\nadaptive debiasing mechanism that identifies and corrects for target-specific\ndeviations via residual smoothing. We provide theoretical guarantees on the\nstability of the estimated alignment matrix and demonstrate the effectiveness\nof GTRANS in improving the accuracy of target graph estimation through\nextensive synthetic and real data experiments. These improvements translate\ndirectly to enhanced performance in downstream applications, such as the graph\nclassification task and the link prediction task.",
      "published": "October 07, 2025",
      "categories": [
        "cs.LG",
        "math.ST",
        "stat.TH"
      ],
      "pdf_link": "http://arxiv.org/pdf/2510.05527v1",
      "arxiv_url": "http://arxiv.org/abs/2510.05527v1"
    }
  ]
}