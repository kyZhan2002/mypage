{
  "timestamp": 1740532522,
  "papers": [
    {
      "title": "Distributionally Robust Active Learning for Gaussian Process Regression",
      "authors": [
        "Shion Takeno",
        "Yoshito Okura",
        "Yu Inatsu",
        "Aoyama Tatsuya",
        "Tomonari Tanaka",
        "Akahane Satoshi",
        "Hiroyuki Hanada",
        "Noriaki Hashimoto",
        "Taro Murayama",
        "Hanju Lee",
        "Shinya Kojima",
        "Ichiro Takeuchi"
      ],
      "abstract": "Gaussian process regression (GPR) or kernel ridge regression is a widely used\nand powerful tool for nonlinear prediction. Therefore, active learning (AL) for\nGPR, which actively collects data labels to achieve an accurate prediction with\nfewer data labels, is an important problem. However, existing AL methods do not\ntheoretically guarantee prediction accuracy for target distribution.\nFurthermore, as discussed in the distributionally robust learning literature,\nspecifying the target distribution is often difficult. Thus, this paper\nproposes two AL methods that effectively reduce the worst-case expected error\nfor GPR, which is the worst-case expectation in target distribution candidates.\nWe show an upper bound of the worst-case expected squared error, which suggests\nthat the error will be arbitrarily small by a finite number of data labels\nunder mild conditions. Finally, we demonstrate the effectiveness of the\nproposed methods through synthetic and real-world datasets.",
      "published": "February 24, 2025",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_link": "http://arxiv.org/pdf/2502.16870v1",
      "arxiv_url": "http://arxiv.org/abs/2502.16870v1"
    },
    {
      "title": "Provable Benefits of Unsupervised Pre-training and Transfer Learning via\n  Single-Index Models",
      "authors": [
        "Taj Jones-McCormick",
        "Aukosh Jagannath",
        "Subhabrata Sen"
      ],
      "abstract": "Unsupervised pre-training and transfer learning are commonly used techniques\nto initialize training algorithms for neural networks, particularly in settings\nwith limited labeled data. In this paper, we study the effects of unsupervised\npre-training and transfer learning on the sample complexity of high-dimensional\nsupervised learning. Specifically, we consider the problem of training a\nsingle-layer neural network via online stochastic gradient descent. We\nestablish that pre-training and transfer learning (under concept shift) reduce\nsample complexity by polynomial factors (in the dimension) under very general\nassumptions. We also uncover some surprising settings where pre-training grants\nexponential improvement over random initialization in terms of sample\ncomplexity.",
      "published": "February 24, 2025",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "pdf_link": "http://arxiv.org/pdf/2502.16849v1",
      "arxiv_url": "http://arxiv.org/abs/2502.16849v1"
    },
    {
      "title": "Improving variable selection properties by using external data",
      "authors": [
        "Paul Rognon-Vael",
        "David Rossell",
        "Piotr Zwiernik"
      ],
      "abstract": "Sparse high-dimensional signal recovery is only possible under certain\nconditions on the number of parameters, sample size, signal strength and\nunderlying sparsity. We show that these mathematical limits can be pushed when\none has external information that allow splitting parameters into blocks. This\noccurs in many applications, including data integration and transfer learning\ntasks. Specifically, we consider the Gaussian sequence model and linear\nregression, and show how block-based $\\ell_0$ penalties attain model selection\nconsistency under milder conditions than standard $\\ell_0$ penalties, and they\nalso attain faster model recovery rates. We first provide results for\noracle-based $\\ell_0$ penalties that have access to perfect sparsity and signal\nstrength information, and subsequently empirical Bayes-motivated block $\\ell_0$\npenalties that does not require oracle information.",
      "published": "February 21, 2025",
      "categories": [
        "math.ST",
        "stat.ME",
        "stat.TH",
        "62F07 (Primary) 62C12, 62R07 (Secondary)"
      ],
      "pdf_link": "http://arxiv.org/pdf/2502.15584v1",
      "arxiv_url": "http://arxiv.org/abs/2502.15584v1"
    },
    {
      "title": "Distribution Matching for Self-Supervised Transfer Learning",
      "authors": [
        "Yuling Jiao",
        "Wensen Ma",
        "Defeng Sun",
        "Hansheng Wang",
        "Yang Wang"
      ],
      "abstract": "In this paper, we propose a novel self-supervised transfer learning method\ncalled Distribution Matching (DM), which drives the representation distribution\ntoward a predefined reference distribution while preserving augmentation\ninvariance. The design of DM results in a learned representation space that is\nintuitively structured and offers easily interpretable hyperparameters.\nExperimental results across multiple real-world datasets and evaluation metrics\ndemonstrate that DM performs competitively on target classification tasks\ncompared to existing self-supervised transfer learning methods. Additionally,\nwe provide robust theoretical guarantees for DM, including a population theorem\nand an end-to-end sample theorem. The population theorem bridges the gap\nbetween the self-supervised learning task and target classification accuracy,\nwhile the sample theorem shows that, even with a limited number of samples from\nthe target domain, DM can deliver exceptional classification performance,\nprovided the unlabeled sample size is sufficiently large.",
      "published": "February 20, 2025",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG",
        "stat.ME"
      ],
      "pdf_link": "http://arxiv.org/pdf/2502.14424v1",
      "arxiv_url": "http://arxiv.org/abs/2502.14424v1"
    },
    {
      "title": "Unsupervised optimal deep transfer learning for classification under\n  general conditional shift",
      "authors": [
        "Junjun Lang",
        "Yukun Liu"
      ],
      "abstract": "Classifiers trained solely on labeled source data may yield misleading\nresults when applied to unlabeled target data drawn from a different\ndistribution. Transfer learning can rectify this by transferring knowledge from\nsource to target data, but its effectiveness frequently relies on stringent\nassumptions, such as label shift. In this paper, we introduce a novel General\nConditional Shift (GCS) assumption, which encompasses label shift as a special\nscenario. Under GCS, we demonstrate that both the target distribution and the\nshift function are identifiable. To estimate the conditional probabilities\n${\\bm\\eta}_P$ for source data, we propose leveraging deep neural networks\n(DNNs). Subsequent to transferring the DNN estimator, we estimate the target\nlabel distribution ${\\bm\\pi}_Q$ utilizing a pseudo-maximum likelihood approach.\nUltimately, by incorporating these estimates and circumventing the need to\nestimate the shift function, we construct our proposed Bayes classifier. We\nestablish concentration bounds for our estimators of both ${\\bm\\eta}_P$ and\n${\\bm\\pi}_Q$ in terms of the intrinsic dimension of ${\\bm\\eta}_P$ . Notably,\nour DNN-based classifier achieves the optimal minimax rate, up to a logarithmic\nfactor. A key advantage of our method is its capacity to effectively combat the\ncurse of dimensionality when ${\\bm\\eta}_P$ exhibits a low-dimensional\nstructure. Numerical simulations, along with an analysis of an Alzheimer's\ndisease dataset, underscore its exceptional performance.",
      "published": "February 18, 2025",
      "categories": [
        "math.ST",
        "stat.TH"
      ],
      "pdf_link": "http://arxiv.org/pdf/2502.12729v1",
      "arxiv_url": "http://arxiv.org/abs/2502.12729v1"
    },
    {
      "title": "Transfer Learning of CATE with Kernel Ridge Regression",
      "authors": [
        "Seok-Jin Kim",
        "Hongjie Liu",
        "Molei Liu",
        "Kaizheng Wang"
      ],
      "abstract": "The proliferation of data has sparked significant interest in leveraging\nfindings from one study to estimate treatment effects in a different target\npopulation without direct outcome observations. However, the transfer learning\nprocess is frequently hindered by substantial covariate shift and limited\noverlap between (i) the source and target populations, as well as (ii) the\ntreatment and control groups within the source. We propose a novel method for\noverlap-adaptive transfer learning of conditional average treatment effect\n(CATE) using kernel ridge regression (KRR). Our approach involves partitioning\nthe labeled source data into two subsets. The first one is used to train\ncandidate CATE models based on regression adjustment and pseudo-outcomes. An\noptimal model is then selected using the second subset and unlabeled target\ndata, employing another pseudo-outcome-based strategy. We provide a theoretical\njustification for our method through sharp non-asymptotic MSE bounds,\nhighlighting its adaptivity to both weak overlaps and the complexity of CATE\nfunction. Extensive numerical studies confirm that our method achieves superior\nfinite-sample efficiency and adaptability. We conclude by demonstrating the\neffectiveness of our approach using a 401(k) eligibility dataset.",
      "published": "February 17, 2025",
      "categories": [
        "stat.ME",
        "cs.LG",
        "stat.ML"
      ],
      "pdf_link": "http://arxiv.org/pdf/2502.11331v1",
      "arxiv_url": "http://arxiv.org/abs/2502.11331v1"
    },
    {
      "title": "Multifidelity Simulation-based Inference for Computationally Expensive\n  Simulators",
      "authors": [
        "Anastasia N. Krouglova",
        "Hayden R. Johnson",
        "Basile Confavreux",
        "Michael Deistler",
        "Pedro J. Gon\u00e7alves"
      ],
      "abstract": "Across many domains of science, stochastic models are an essential tool to\nunderstand the mechanisms underlying empirically observed data. Models can be\nof different levels of detail and accuracy, with models of high-fidelity (i.e.,\nhigh accuracy) to the phenomena under study being often preferable. However,\ninferring parameters of high-fidelity models via simulation-based inference is\nchallenging, especially when the simulator is computationally expensive. We\nintroduce MF-NPE, a multifidelity approach to neural posterior estimation that\nleverages inexpensive low-fidelity simulations to infer parameters of\nhigh-fidelity simulators within a limited simulation budget. MF-NPE performs\nneural posterior estimation with limited high-fidelity resources by virtue of\ntransfer learning, with the ability to prioritize individual observations using\nactive learning. On one statistical task with analytical ground-truth and two\nreal-world tasks, MF-NPE shows comparable performance to current approaches\nwhile requiring up to two orders of magnitude fewer high-fidelity simulations.\nOverall, MF-NPE opens new opportunities to perform efficient Bayesian inference\non computationally expensive simulators.",
      "published": "February 12, 2025",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "pdf_link": "http://arxiv.org/pdf/2502.08416v2",
      "arxiv_url": "http://arxiv.org/abs/2502.08416v2"
    },
    {
      "title": "Knowledge-Guided Wasserstein Distributionally Robust Optimization",
      "authors": [
        "Zitao Wang",
        "Ziyuan Wang",
        "Molei Liu",
        "Nian Si"
      ],
      "abstract": "Transfer learning is a popular strategy to leverage external knowledge and\nimprove statistical efficiency, particularly with a limited target sample. We\npropose a novel knowledge-guided Wasserstein Distributionally Robust\nOptimization (KG-WDRO) framework that adaptively incorporates multiple sources\nof external knowledge to overcome the conservativeness of vanilla WDRO, which\noften results in overly pessimistic shrinkage toward zero. Our method\nconstructs smaller Wasserstein ambiguity sets by controlling the transportation\nalong directions informed by the source knowledge. This strategy can alleviate\nperturbations on the predictive projection of the covariates and protect\nagainst information loss. Theoretically, we establish the equivalence between\nour WDRO formulation and the knowledge-guided shrinkage estimation based on\ncollinear similarity, ensuring tractability and geometrizing the feasible set.\nThis also reveals a novel and general interpretation for recent shrinkage-based\ntransfer learning approaches from the perspective of distributional robustness.\nIn addition, our framework can adjust for scaling differences in the regression\nmodels between the source and target and accommodates general types of\nregularization such as lasso and ridge. Extensive simulations demonstrate the\nsuperior performance and adaptivity of KG-WDRO in enhancing small-sample\ntransfer learning.",
      "published": "February 12, 2025",
      "categories": [
        "cs.LG",
        "stat.ME",
        "stat.ML"
      ],
      "pdf_link": "http://arxiv.org/pdf/2502.08146v1",
      "arxiv_url": "http://arxiv.org/abs/2502.08146v1"
    }
  ]
}