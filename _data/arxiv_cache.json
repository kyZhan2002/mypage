{
  "timestamp": 1740014049,
  "papers": [
    {
      "title": "Unsupervised optimal deep transfer learning for classification under\n  general conditional shift",
      "authors": [
        "Junjun Lang",
        "Yukun Liu"
      ],
      "abstract": "Classifiers trained solely on labeled source data may yield misleading\nresults when applied to unlabeled target data drawn from a different\ndistribution. Transfer learning can rectify this by transferring knowledge from\nsource to target data, but its effectiveness frequently relies on stringent\nassumptions, such as label shift. In this paper, we introduce a novel General\nConditional Shift (GCS) assumption, which encompasses label shift as a special\nscenario. Under GCS, we demonstrate that both the target distribution and the\nshift function are identifiable. To estimate the conditional probabilities\n${\\bm\\eta}_P$ for source data, we propose leveraging deep neural networks\n(DNNs). Subsequent to transferring the DNN estimator, we estimate the target\nlabel distribution ${\\bm\\pi}_Q$ utilizing a pseudo-maximum likelihood approach.\nUltimately, by incorporating these estimates and circumventing the need to\nestimate the shift function, we construct our proposed Bayes classifier. We\nestablish concentration bounds for our estimators of both ${\\bm\\eta}_P$ and\n${\\bm\\pi}_Q$ in terms of the intrinsic dimension of ${\\bm\\eta}_P$ . Notably,\nour DNN-based classifier achieves the optimal minimax rate, up to a logarithmic\nfactor. A key advantage of our method is its capacity to effectively combat the\ncurse of dimensionality when ${\\bm\\eta}_P$ exhibits a low-dimensional\nstructure. Numerical simulations, along with an analysis of an Alzheimer's\ndisease dataset, underscore its exceptional performance.",
      "published": "February 18, 2025",
      "categories": [
        "math.ST",
        "stat.TH"
      ],
      "pdf_link": "http://arxiv.org/pdf/2502.12729v1",
      "arxiv_url": "http://arxiv.org/abs/2502.12729v1"
    },
    {
      "title": "Transfer Learning of CATE with Kernel Ridge Regression",
      "authors": [
        "Seok-Jin Kim",
        "Hongjie Liu",
        "Molei Liu",
        "Kaizheng Wang"
      ],
      "abstract": "The proliferation of data has sparked significant interest in leveraging\nfindings from one study to estimate treatment effects in a different target\npopulation without direct outcome observations. However, the transfer learning\nprocess is frequently hindered by substantial covariate shift and limited\noverlap between (i) the source and target populations, as well as (ii) the\ntreatment and control groups within the source. We propose a novel method for\noverlap-adaptive transfer learning of conditional average treatment effect\n(CATE) using kernel ridge regression (KRR). Our approach involves partitioning\nthe labeled source data into two subsets. The first one is used to train\ncandidate CATE models based on regression adjustment and pseudo-outcomes. An\noptimal model is then selected using the second subset and unlabeled target\ndata, employing another pseudo-outcome-based strategy. We provide a theoretical\njustification for our method through sharp non-asymptotic MSE bounds,\nhighlighting its adaptivity to both weak overlaps and the complexity of CATE\nfunction. Extensive numerical studies confirm that our method achieves superior\nfinite-sample efficiency and adaptability. We conclude by demonstrating the\neffectiveness of our approach using a 401(k) eligibility dataset.",
      "published": "February 17, 2025",
      "categories": [
        "stat.ME",
        "cs.LG",
        "stat.ML"
      ],
      "pdf_link": "http://arxiv.org/pdf/2502.11331v1",
      "arxiv_url": "http://arxiv.org/abs/2502.11331v1"
    },
    {
      "title": "Multifidelity Simulation-based Inference for Computationally Expensive\n  Simulators",
      "authors": [
        "Anastasia N. Krouglova",
        "Hayden R. Johnson",
        "Basile Confavreux",
        "Michael Deistler",
        "Pedro J. Gon\u00e7alves"
      ],
      "abstract": "Across many domains of science, stochastic models are an essential tool to\nunderstand the mechanisms underlying empirically observed data. Models can be\nof different levels of detail and accuracy, with models of high-fidelity (i.e.,\nhigh accuracy) to the phenomena under study being often preferable. However,\ninferring parameters of high-fidelity models via simulation-based inference is\nchallenging, especially when the simulator is computationally expensive. We\nintroduce MF-NPE, a multifidelity approach to neural posterior estimation that\nleverages inexpensive low-fidelity simulations to infer parameters of\nhigh-fidelity simulators within a limited simulation budget. MF-NPE performs\nneural posterior estimation with limited high-fidelity resources by virtue of\ntransfer learning, with the ability to prioritize individual observations using\nactive learning. On one statistical task with analytical ground-truth and two\nreal-world tasks, MF-NPE shows comparable performance to current approaches\nwhile requiring up to two orders of magnitude fewer high-fidelity simulations.\nOverall, MF-NPE opens new opportunities to perform efficient Bayesian inference\non computationally expensive simulators.",
      "published": "February 12, 2025",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "pdf_link": "http://arxiv.org/pdf/2502.08416v2",
      "arxiv_url": "http://arxiv.org/abs/2502.08416v2"
    },
    {
      "title": "Knowledge-Guided Wasserstein Distributionally Robust Optimization",
      "authors": [
        "Zitao Wang",
        "Ziyuan Wang",
        "Molei Liu",
        "Nian Si"
      ],
      "abstract": "Transfer learning is a popular strategy to leverage external knowledge and\nimprove statistical efficiency, particularly with a limited target sample. We\npropose a novel knowledge-guided Wasserstein Distributionally Robust\nOptimization (KG-WDRO) framework that adaptively incorporates multiple sources\nof external knowledge to overcome the conservativeness of vanilla WDRO, which\noften results in overly pessimistic shrinkage toward zero. Our method\nconstructs smaller Wasserstein ambiguity sets by controlling the transportation\nalong directions informed by the source knowledge. This strategy can alleviate\nperturbations on the predictive projection of the covariates and protect\nagainst information loss. Theoretically, we establish the equivalence between\nour WDRO formulation and the knowledge-guided shrinkage estimation based on\ncollinear similarity, ensuring tractability and geometrizing the feasible set.\nThis also reveals a novel and general interpretation for recent shrinkage-based\ntransfer learning approaches from the perspective of distributional robustness.\nIn addition, our framework can adjust for scaling differences in the regression\nmodels between the source and target and accommodates general types of\nregularization such as lasso and ridge. Extensive simulations demonstrate the\nsuperior performance and adaptivity of KG-WDRO in enhancing small-sample\ntransfer learning.",
      "published": "February 12, 2025",
      "categories": [
        "cs.LG",
        "stat.ME",
        "stat.ML"
      ],
      "pdf_link": "http://arxiv.org/pdf/2502.08146v1",
      "arxiv_url": "http://arxiv.org/abs/2502.08146v1"
    },
    {
      "title": "Generative Distribution Prediction: A Unified Approach to Multimodal\n  Learning",
      "authors": [
        "Xinyu Tian",
        "Xiaotong Shen"
      ],
      "abstract": "Accurate prediction with multimodal data-encompassing tabular, textual, and\nvisual inputs or outputs-is fundamental to advancing analytics in diverse\napplication domains. Traditional approaches often struggle to integrate\nheterogeneous data types while maintaining high predictive accuracy. We\nintroduce Generative Distribution Prediction (GDP), a novel framework that\nleverages multimodal synthetic data generation-such as conditional diffusion\nmodels-to enhance predictive performance across structured and unstructured\nmodalities. GDP is model-agnostic, compatible with any high-fidelity generative\nmodel, and supports transfer learning for domain adaptation. We establish a\nrigorous theoretical foundation for GDP, providing statistical guarantees on\nits predictive accuracy when using diffusion models as the generative backbone.\nBy estimating the data-generating distribution and adapting to various loss\nfunctions for risk minimization, GDP enables accurate point predictions across\nmultimodal settings. We empirically validate GDP on four supervised learning\ntasks-tabular data prediction, question answering, image captioning, and\nadaptive quantile regression-demonstrating its versatility and effectiveness\nacross diverse domains.",
      "published": "February 10, 2025",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_link": "http://arxiv.org/pdf/2502.07090v1",
      "arxiv_url": "http://arxiv.org/abs/2502.07090v1"
    },
    {
      "title": "Model Diffusion for Certifiable Few-shot Transfer Learning",
      "authors": [
        "Fady Rezk",
        "Royson Lee",
        "Henry Gouk",
        "Timothy Hospedales",
        "Minyoung Kim"
      ],
      "abstract": "In modern large-scale deep learning, a prevalent and effective workflow for\nsolving low-data problems is adapting powerful pre-trained foundation models\n(FMs) to new tasks via parameter-efficient fine-tuning (PEFT). However, while\nempirically effective, the resulting solutions lack generalisation guarantees\nto certify their accuracy - which may be required for ethical or legal reasons\nprior to deployment in high-importance applications. In this paper we develop a\nnovel transfer learning approach that is designed to facilitate non-vacuous\nlearning theoretic generalisation guarantees for downstream tasks, even in the\nlow-shot regime. Specifically, we first use upstream tasks to train a\ndistribution over PEFT parameters. We then learn the downstream task by a\nsample-and-evaluate procedure -- sampling plausible PEFTs from the trained\ndiffusion model and selecting the one with the highest likelihood on the\ndownstream data. Crucially, this confines our model hypothesis to a finite set\nof PEFT samples. In contrast to learning in the typical continuous hypothesis\nspaces of neural network weights, this facilitates tighter risk certificates.\nWe instantiate our bound and show non-trivial generalization guarantees\ncompared to existing learning approaches which lead to vacuous bounds in the\nlow-shot regime.",
      "published": "February 10, 2025",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_link": "http://arxiv.org/pdf/2502.06970v1",
      "arxiv_url": "http://arxiv.org/abs/2502.06970v1"
    },
    {
      "title": "Provable Sample-Efficient Transfer Learning Conditional Diffusion Models\n  via Representation Learning",
      "authors": [
        "Ziheng Cheng",
        "Tianyu Xie",
        "Shiyue Zhang",
        "Cheng Zhang"
      ],
      "abstract": "While conditional diffusion models have achieved remarkable success in\nvarious applications, they require abundant data to train from scratch, which\nis often infeasible in practice. To address this issue, transfer learning has\nemerged as an essential paradigm in small data regimes. Despite its empirical\nsuccess, the theoretical underpinnings of transfer learning conditional\ndiffusion models remain unexplored. In this paper, we take the first step\ntowards understanding the sample efficiency of transfer learning conditional\ndiffusion models through the lens of representation learning. Inspired by\npractical training procedures, we assume that there exists a low-dimensional\nrepresentation of conditions shared across all tasks. Our analysis shows that\nwith a well-learned representation from source tasks, the samplecomplexity of\ntarget tasks can be reduced substantially. In addition, we investigate the\npractical implications of our theoretical results in several real-world\napplications of conditional diffusion models. Numerical experiments are also\nconducted to verify our results.",
      "published": "February 06, 2025",
      "categories": [
        "cs.LG",
        "math.ST",
        "stat.ML",
        "stat.TH"
      ],
      "pdf_link": "http://arxiv.org/pdf/2502.04491v1",
      "arxiv_url": "http://arxiv.org/abs/2502.04491v1"
    }
  ]
}