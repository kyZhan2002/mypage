{
  "timestamp": 1750728466,
  "papers": [
    {
      "title": "On Design of Representative Distributionally Robust Formulations for\n  Evaluation of Tail Risk Measures",
      "authors": [
        "Anand Deo"
      ],
      "abstract": "Conditional Value-at-Risk (CVaR) is a risk measure widely used to quantify\nthe impact of extreme losses. Owing to the lack of representative samples CVaR\nis sensitive to the tails of the underlying distribution. In order to combat\nthis sensitivity, Distributionally Robust Optimization (DRO), which evaluates\nthe worst-case CVaR measure over a set of plausible data distributions is often\ndeployed. Unfortunately, an improper choice of the DRO formulation can lead to\na severe underestimation of tail risk. This paper aims at leveraging extreme\nvalue theory to arrive at a DRO formulation which leads to representative\nworst-case CVaR evaluations in that the above pitfall is avoided while\nsimultaneously, the worst case evaluation is not a gross over-estimate of the\ntrue CVaR. We demonstrate theoretically that even when there is paucity of\nsamples in the tail of the distribution, our formulation is readily\nimplementable from data, only requiring calibration of a single scalar\nparameter. We showcase that our formulation can be easily extended to provide\nrobustness to tail risk in multivariate applications as well as in the\nevaluation of other commonly used risk measures. Numerical illustrations on\nsynthetic and real-world data showcase the practical utility of our approach.",
      "published": "June 19, 2025",
      "categories": [
        "q-fin.RM",
        "math.PR",
        "stat.ME",
        "stat.ML"
      ],
      "pdf_link": "http://arxiv.org/pdf/2506.16230v1",
      "arxiv_url": "http://arxiv.org/abs/2506.16230v1"
    },
    {
      "title": "Adjustment for Confounding using Pre-Trained Representations",
      "authors": [
        "Rickmer Schulte",
        "David R\u00fcgamer",
        "Thomas Nagler"
      ],
      "abstract": "There is growing interest in extending average treatment effect (ATE)\nestimation to incorporate non-tabular data, such as images and text, which may\nact as sources of confounding. Neglecting these effects risks biased results\nand flawed scientific conclusions. However, incorporating non-tabular data\nnecessitates sophisticated feature extractors, often in combination with ideas\nof transfer learning. In this work, we investigate how latent features from\npre-trained neural networks can be leveraged to adjust for sources of\nconfounding. We formalize conditions under which these latent features enable\nvalid adjustment and statistical inference in ATE estimation, demonstrating\nresults along the example of double machine learning. We discuss critical\nchallenges inherent to latent feature learning and downstream parameter\nestimation arising from the high dimensionality and non-identifiability of\nrepresentations. Common structural assumptions for obtaining fast convergence\nrates with additive or sparse linear models are shown to be unrealistic for\nlatent features. We argue, however, that neural networks are largely\ninsensitive to these issues. In particular, we show that neural networks can\nachieve fast convergence rates by adapting to intrinsic notions of sparsity and\ndimension of the learning problem.",
      "published": "June 17, 2025",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG",
        "stat.CO",
        "stat.ME"
      ],
      "pdf_link": "http://arxiv.org/pdf/2506.14329v1",
      "arxiv_url": "http://arxiv.org/abs/2506.14329v1"
    }
  ]
}