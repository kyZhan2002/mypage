{
  "timestamp": 1760664009,
  "papers": [
    {
      "title": "Transfer Learning with Distance Covariance for Random Forest: Error\n  Bounds and an EHR Application",
      "authors": [
        "Chenze Li",
        "Subhadeep Paul"
      ],
      "abstract": "Random forest is an important method for ML applications due to its broad\noutperformance over competing methods for structured tabular data. We propose a\nmethod for transfer learning in nonparametric regression using a centered\nrandom forest (CRF) with distance covariance-based feature weights, assuming\nthe unknown source and target regression functions are different for a few\nfeatures (sparsely different). Our method first obtains residuals from\npredicting the response in the target domain using a source domain-trained CRF.\nThen, we fit another CRF to the residuals, but with feature splitting\nprobabilities proportional to the sample distance covariance between the\nfeatures and the residuals in an independent sample. We derive an upper bound\non the mean square error rate of the procedure as a function of sample sizes\nand difference dimension, theoretically demonstrating transfer learning\nbenefits in random forests. In simulations, we show that the results obtained\nfor the CRFs also hold numerically for the standard random forest (SRF) method\nwith data-driven feature split selection. Beyond transfer learning, our results\nalso show the benefit of distance-covariance-based weights on the performance\nof RF in some situations. Our method shows significant gains in predicting the\nmortality of ICU patients in smaller-bed target hospitals using a large\nmulti-hospital dataset of electronic health records for 200,000 ICU patients.",
      "published": "October 13, 2025",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.ST",
        "stat.ME",
        "stat.TH"
      ],
      "pdf_link": "http://arxiv.org/pdf/2510.10870v1",
      "arxiv_url": "http://arxiv.org/abs/2510.10870v1"
    },
    {
      "title": "Quantifying Dataset Similarity to Guide Transfer Learning",
      "authors": [
        "Shudong Sun",
        "Hao Helen Zhang"
      ],
      "abstract": "Transfer learning has become a cornerstone of modern machine learning, as it\ncan empower models by leveraging knowledge from related domains to improve\nlearning effectiveness. However, transferring from poorly aligned data can harm\nrather than help performance, making it crucial to determine whether the\ntransfer will be beneficial before implementation. This work aims to address\nthis challenge by proposing an innovative metric to measure dataset similarity\nand provide quantitative guidance on transferability. In the literature,\nexisting methods largely focus on feature distributions while overlooking label\ninformation and predictive relationships, potentially missing critical\ntransferability insights. In contrast, our proposed metric, the Cross-Learning\nScore (CLS), measures dataset similarity through bidirectional generalization\nperformance between domains. We provide a theoretical justification for CLS by\nestablishing its connection to the cosine similarity between the decision\nboundaries for the target and source datasets. Computationally, CLS is\nefficient and fast to compute as it bypasses the problem of expensive\ndistribution estimation for high-dimensional problems. We further introduce a\ngeneral framework that categorizes source datasets into positive, ambiguous, or\nnegative transfer zones based on their CLS relative to the baseline error,\nenabling informed decisions. Additionally, we extend this approach to\nencoder-head architectures in deep learning to better reflect modern transfer\npipelines. Extensive experiments on diverse synthetic and real-world tasks\ndemonstrate that CLS can reliably predict whether transfer will improve or\ndegrade performance, offering a principled tool for guiding data selection in\ntransfer learning.",
      "published": "October 13, 2025",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "pdf_link": "http://arxiv.org/pdf/2510.10866v1",
      "arxiv_url": "http://arxiv.org/abs/2510.10866v1"
    },
    {
      "title": "Tight Robustness Certificates and Wasserstein Distributional Attacks for\n  Deep Neural Networks",
      "authors": [
        "Bach C. Le",
        "Tung V. Dao",
        "Binh T. Nguyen",
        "Hong T. M. Chu"
      ],
      "abstract": "Wasserstein distributionally robust optimization (WDRO) provides a framework\nfor adversarial robustness, yet existing methods based on global Lipschitz\ncontinuity or strong duality often yield loose upper bounds or require\nprohibitive computation. In this work, we address these limitations by\nintroducing a primal approach and adopting a notion of exact Lipschitz\ncertificate to tighten this upper bound of WDRO. In addition, we propose a\nnovel Wasserstein distributional attack (WDA) that directly constructs a\ncandidate for the worst-case distribution. Compared to existing point-wise\nattack and its variants, our WDA offers greater flexibility in the number and\nlocation of attack points. In particular, by leveraging the piecewise-affine\nstructure of ReLU networks on their activation cells, our approach results in\nan exact tractable characterization of the corresponding WDRO problem.\nExtensive evaluations demonstrate that our method achieves competitive robust\naccuracy against state-of-the-art baselines while offering tighter certificates\nthan existing methods. Our code is available at\nhttps://github.com/OLab-Repo/WDA",
      "published": "October 11, 2025",
      "categories": [
        "cs.LG",
        "math.OC",
        "stat.ML"
      ],
      "pdf_link": "http://arxiv.org/pdf/2510.10000v1",
      "arxiv_url": "http://arxiv.org/abs/2510.10000v1"
    },
    {
      "title": "Distributionally robust approximation property of neural networks",
      "authors": [
        "Mihriban Ceylan",
        "David J. Pr\u00f6mel"
      ],
      "abstract": "The universal approximation property uniformly with respect to weakly compact\nfamilies of measures is established for several classes of neural networks. To\nthat end, we prove that these neural networks are dense in Orlicz spaces,\nthereby extending classical universal approximation theorems even beyond the\ntraditional $L^p$-setting. The covered classes of neural networks include\nwidely used architectures like feedforward neural networks with non-polynomial\nactivation functions, deep narrow networks with ReLU activation functions and\nfunctional input neural networks.",
      "published": "October 10, 2025",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.FA",
        "math.PR"
      ],
      "pdf_link": "http://arxiv.org/pdf/2510.09177v1",
      "arxiv_url": "http://arxiv.org/abs/2510.09177v1"
    },
    {
      "title": "Structured Output Regularization: a framework for few-shot transfer\n  learning",
      "authors": [
        "Nicolas Ewen",
        "Jairo Diaz-Rodriguez",
        "Kelly Ramsay"
      ],
      "abstract": "Traditional transfer learning typically reuses large pre-trained networks by\nfreezing some of their weights and adding task-specific layers. While this\napproach is computationally efficient, it limits the model's ability to adapt\nto domain-specific features and can still lead to overfitting with very limited\ndata. To address these limitations, we propose Structured Output Regularization\n(SOR), a simple yet effective framework that freezes the internal network\nstructures (e.g., convolutional filters) while using a combination of group\nlasso and $L_1$ penalties. This framework tailors the model to specific data\nwith minimal additional parameters and is easily applicable to various network\ncomponents, such as convolutional filters or various blocks in neural networks\nenabling broad applicability for transfer learning tasks. We evaluate SOR on\nthree few shot medical imaging classification tasks and we achieve competitive\nresults using DenseNet121, and EfficientNetB4 bases compared to established\nbenchmarks.",
      "published": "October 09, 2025",
      "categories": [
        "cs.CV",
        "cs.LG",
        "stat.ML"
      ],
      "pdf_link": "http://arxiv.org/pdf/2510.08728v1",
      "arxiv_url": "http://arxiv.org/abs/2510.08728v1"
    },
    {
      "title": "Scalable deep fusion of spaceborne lidar and synthetic aperture radar\n  for global forest structural complexity mapping",
      "authors": [
        "Tiago de Conto",
        "John Armston",
        "Ralph Dubayah"
      ],
      "abstract": "Forest structural complexity metrics integrate multiple canopy attributes\ninto a single value that reflects habitat quality and ecosystem function.\nSpaceborne lidar from the Global Ecosystem Dynamics Investigation (GEDI) has\nenabled mapping of structural complexity in temperate and tropical forests, but\nits sparse sampling limits continuous high-resolution mapping. We present a\nscalable, deep learning framework fusing GEDI observations with multimodal\nSynthetic Aperture Radar (SAR) datasets to produce global, high-resolution (25\nm) wall-to-wall maps of forest structural complexity. Our adapted\nEfficientNetV2 architecture, trained on over 130 million GEDI footprints,\nachieves high performance (global R2 = 0.82) with fewer than 400,000\nparameters, making it an accessible tool that enables researchers to process\ndatasets at any scale without requiring specialized computing infrastructure.\nThe model produces accurate predictions with calibrated uncertainty estimates\nacross biomes and time periods, preserving fine-scale spatial patterns. It has\nbeen used to generate a global, multi-temporal dataset of forest structural\ncomplexity from 2015 to 2022. Through transfer learning, this framework can be\nextended to predict additional forest structural variables with minimal\ncomputational cost. This approach supports continuous, multi-temporal\nmonitoring of global forest structural dynamics and provides tools for\nbiodiversity conservation and ecosystem management efforts in a changing\nclimate.",
      "published": "October 07, 2025",
      "categories": [
        "cs.CV",
        "cs.LG",
        "stat.AP"
      ],
      "pdf_link": "http://arxiv.org/pdf/2510.06299v1",
      "arxiv_url": "http://arxiv.org/abs/2510.06299v1"
    },
    {
      "title": "Transfer Learning on Edge Connecting Probability Estimation under\n  Graphon Model",
      "authors": [
        "Yuyao Wang",
        "Yu-Hung Cheng",
        "Debarghya Mukherjee",
        "Huimin Cheng"
      ],
      "abstract": "Graphon models provide a flexible nonparametric framework for estimating\nlatent connectivity probabilities in networks, enabling a range of downstream\napplications such as link prediction and data augmentation. However, accurate\ngraphon estimation typically requires a large graph, whereas in practice, one\noften only observes a small-sized network. One approach to addressing this\nissue is to adopt a transfer learning framework, which aims to improve\nestimation in a small target graph by leveraging structural information from a\nlarger, related source graph. In this paper, we propose a novel method, namely\nGTRANS, a transfer learning framework that integrates neighborhood smoothing\nand Gromov-Wasserstein optimal transport to align and transfer structural\npatterns between graphs. To prevent negative transfer, GTRANS includes an\nadaptive debiasing mechanism that identifies and corrects for target-specific\ndeviations via residual smoothing. We provide theoretical guarantees on the\nstability of the estimated alignment matrix and demonstrate the effectiveness\nof GTRANS in improving the accuracy of target graph estimation through\nextensive synthetic and real data experiments. These improvements translate\ndirectly to enhanced performance in downstream applications, such as the graph\nclassification task and the link prediction task.",
      "published": "October 07, 2025",
      "categories": [
        "cs.LG",
        "math.ST",
        "stat.TH"
      ],
      "pdf_link": "http://arxiv.org/pdf/2510.05527v1",
      "arxiv_url": "http://arxiv.org/abs/2510.05527v1"
    },
    {
      "title": "Busemann Functions in the Wasserstein Space: Existence, Closed-Forms,\n  and Applications to Slicing",
      "authors": [
        "Cl\u00e9ment Bonet",
        "Elsa Cazelles",
        "Lucas Drumetz",
        "Nicolas Courty"
      ],
      "abstract": "The Busemann function has recently found much interest in a variety of\ngeometric machine learning problems, as it naturally defines projections onto\ngeodesic rays of Riemannian manifolds and generalizes the notion of\nhyperplanes. As several sources of data can be conveniently modeled as\nprobability distributions, it is natural to study this function in the\nWasserstein space, which carries a rich formal Riemannian structure induced by\nOptimal Transport metrics. In this work, we investigate the existence and\ncomputation of Busemann functions in Wasserstein space, which admits geodesic\nrays. We establish closed-form expressions in two important cases:\none-dimensional distributions and Gaussian measures. These results enable\nexplicit projection schemes for probability distributions on $\\mathbb{R}$,\nwhich in turn allow us to define novel Sliced-Wasserstein distances over\nGaussian mixtures and labeled datasets. We demonstrate the efficiency of those\noriginal schemes on synthetic datasets as well as transfer learning problems.",
      "published": "October 06, 2025",
      "categories": [
        "cs.LG",
        "math.MG",
        "stat.ML"
      ],
      "pdf_link": "http://arxiv.org/pdf/2510.04579v1",
      "arxiv_url": "http://arxiv.org/abs/2510.04579v1"
    },
    {
      "title": "Optimal Regularization Under Uncertainty: Distributional Robustness and\n  Convexity Constraints",
      "authors": [
        "Oscar Leong",
        "Eliza O'Reilly",
        "Yong Sheng Soh"
      ],
      "abstract": "Regularization is a central tool for addressing ill-posedness in inverse\nproblems and statistical estimation, with the choice of a suitable penalty\noften determining the reliability and interpretability of downstream solutions.\nWhile recent work has characterized optimal regularizers for well-specified\ndata distributions, practical deployments are often complicated by\ndistributional uncertainty and the need to enforce structural constraints such\nas convexity. In this paper, we introduce a framework for distributionally\nrobust optimal regularization, which identifies regularizers that remain\neffective under perturbations of the data distribution. Our approach leverages\nconvex duality to reformulate the underlying distributionally robust\noptimization problem, eliminating the inner maximization and yielding\nformulations that are amenable to numerical computation. We show how the\nresulting robust regularizers interpolate between memorization of the training\ndistribution and uniform priors, providing insights into their behavior as\nrobustness parameters vary. For example, we show how certain ambiguity sets,\nsuch as those based on the Wasserstein-1 distance, naturally induce regularity\nin the optimal regularizer by promoting regularizers with smaller Lipschitz\nconstants. We further investigate the setting where regularizers are required\nto be convex, formulating a convex program for their computation and\nillustrating their stability with respect to distributional shifts. Taken\ntogether, our results provide both theoretical and computational foundations\nfor designing regularizers that are reliable under model uncertainty and\nstructurally constrained for robust deployment.",
      "published": "October 03, 2025",
      "categories": [
        "math.OC",
        "math.MG",
        "math.ST",
        "stat.ML",
        "stat.TH"
      ],
      "pdf_link": "http://arxiv.org/pdf/2510.03464v1",
      "arxiv_url": "http://arxiv.org/abs/2510.03464v1"
    }
  ]
}