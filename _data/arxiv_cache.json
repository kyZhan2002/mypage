{
  "timestamp": 1748309044,
  "papers": [
    {
      "title": "Linear Mixture Distributionally Robust Markov Decision Processes",
      "authors": [
        "Zhishuai Liu",
        "Pan Xu"
      ],
      "abstract": "Many real-world decision-making problems face the off-dynamics challenge: the\nagent learns a policy in a source domain and deploys it in a target domain with\ndifferent state transitions. The distributionally robust Markov decision\nprocess (DRMDP) addresses this challenge by finding a robust policy that\nperforms well under the worst-case environment within a pre-specified\nuncertainty set of transition dynamics. Its effectiveness heavily hinges on the\nproper design of these uncertainty sets, based on prior knowledge of the\ndynamics. In this work, we propose a novel linear mixture DRMDP framework,\nwhere the nominal dynamics is assumed to be a linear mixture model. In contrast\nwith existing uncertainty sets directly defined as a ball centered around the\nnominal kernel, linear mixture DRMDPs define the uncertainty sets based on a\nball around the mixture weighting parameter. We show that this new framework\nprovides a more refined representation of uncertainties compared to\nconventional models based on $(s,a)$-rectangularity and $d$-rectangularity,\nwhen prior knowledge about the mixture model is present. We propose a meta\nalgorithm for robust policy learning in linear mixture DRMDPs with general\n$f$-divergence defined uncertainty sets, and analyze its sample complexities\nunder three divergence metrics instantiations: total variation,\nKullback-Leibler, and $\\chi^2$ divergences. These results establish the\nstatistical learnability of linear mixture DRMDPs, laying the theoretical\nfoundation for future research on this new setting.",
      "published": "May 23, 2025",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO",
        "stat.ML"
      ],
      "pdf_link": "http://arxiv.org/pdf/2505.18044v1",
      "arxiv_url": "http://arxiv.org/abs/2505.18044v1"
    },
    {
      "title": "A Distributionally-Robust Framework for Nuisance in Causal Effect\n  Estimation",
      "authors": [
        "Akira Tanimoto"
      ],
      "abstract": "Causal inference requires evaluating models on balanced distributions between\ntreatment and control groups, while training data often exhibits imbalance due\nto historical decision-making policies. Most conventional statistical methods\naddress this distribution shift through inverse probability weighting (IPW),\nwhich requires estimating propensity scores as an intermediate step. These\nmethods face two key challenges: inaccurate propensity estimation and\ninstability from extreme weights. We decompose the generalization error to\nisolate these issues--propensity ambiguity and statistical instability--and\naddress them through an adversarial loss function. Our approach combines\ndistributionally robust optimization for handling propensity uncertainty with\nweight regularization based on weighted Rademacher complexity. Experiments on\nsynthetic and real-world datasets demonstrate consistent improvements over\nexisting methods.",
      "published": "May 23, 2025",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_link": "http://arxiv.org/pdf/2505.17717v1",
      "arxiv_url": "http://arxiv.org/abs/2505.17717v1"
    },
    {
      "title": "Wasserstein Transfer Learning",
      "authors": [
        "Kaicheng Zhang",
        "Sinian Zhang",
        "Doudou Zhou",
        "Yidong Zhou"
      ],
      "abstract": "Transfer learning is a powerful paradigm for leveraging knowledge from source\ndomains to enhance learning in a target domain. However, traditional transfer\nlearning approaches often focus on scalar or multivariate data within Euclidean\nspaces, limiting their applicability to complex data structures such as\nprobability distributions. To address this, we introduce a novel framework for\ntransfer learning in regression models, where outputs are probability\ndistributions residing in the Wasserstein space. When the informative subset of\ntransferable source domains is known, we propose an estimator with provable\nasymptotic convergence rates, quantifying the impact of domain similarity on\ntransfer efficiency. For cases where the informative subset is unknown, we\ndevelop a data-driven transfer learning procedure designed to mitigate negative\ntransfer. The proposed methods are supported by rigorous theoretical analysis\nand are validated through extensive simulations and real-world applications.",
      "published": "May 23, 2025",
      "categories": [
        "cs.LG",
        "math.ST",
        "stat.TH"
      ],
      "pdf_link": "http://arxiv.org/pdf/2505.17404v1",
      "arxiv_url": "http://arxiv.org/abs/2505.17404v1"
    },
    {
      "title": "Transfer Faster, Price Smarter: Minimax Dynamic Pricing under\n  Cross-Market Preference Shift",
      "authors": [
        "Yi Zhang",
        "Elynn Chen",
        "Yujun Yan"
      ],
      "abstract": "We study contextual dynamic pricing when a target market can leverage K\nauxiliary markets -- offline logs or concurrent streams -- whose mean utilities\ndiffer by a structured preference shift. We propose Cross-Market Transfer\nDynamic Pricing (CM-TDP), the first algorithm that provably handles such\nmodel-shift transfer and delivers minimax-optimal regret for both linear and\nnon-parametric utility models.\n  For linear utilities of dimension d, where the difference between source- and\ntarget-task coefficients is $s_{0}$-sparse, CM-TDP attains regret\n$\\tilde{O}((d*K^{-1}+s_{0})\\log T)$. For nonlinear demand residing in a\nreproducing kernel Hilbert space with effective dimension $\\alpha$, complexity\n$\\beta$ and task-similarity parameter $H$, the regret becomes\n$\\tilde{O}\\!(K^{-2\\alpha\\beta/(2\\alpha\\beta+1)}T^{1/(2\\alpha\\beta+1)} +\nH^{2/(2\\alpha+1)}T^{1/(2\\alpha+1)})$, matching information-theoretic lower\nbounds up to logarithmic factors. The RKHS bound is the first of its kind for\ntransfer pricing and is of independent interest.\n  Extensive simulations show up to 50% lower cumulative regret and 5 times\nfaster learning relative to single-market pricing baselines. By bridging\ntransfer learning, robust aggregation, and revenue optimization, CM-TDP moves\ntoward pricing systems that transfer faster, price smarter.",
      "published": "May 22, 2025",
      "categories": [
        "stat.ME",
        "cs.LG",
        "stat.AP"
      ],
      "pdf_link": "http://arxiv.org/pdf/2505.17203v1",
      "arxiv_url": "http://arxiv.org/abs/2505.17203v1"
    },
    {
      "title": "HR-VILAGE-3K3M: A Human Respiratory Viral Immunization Longitudinal Gene\n  Expression Dataset for Systems Immunity",
      "authors": [
        "Xuejun Sun",
        "Yiran Song",
        "Xiaochen Zhou",
        "Ruilie Cai",
        "Yu Zhang",
        "Xinyi Li",
        "Rui Peng",
        "Jialiu Xie",
        "Yuanyuan Yan",
        "Muyao Tang",
        "Prem Lakshmanane",
        "Baiming Zou",
        "James S. Hagood",
        "Raymond J. Pickles",
        "Didong Li",
        "Fei Zou",
        "Xiaojing Zheng"
      ],
      "abstract": "Respiratory viral infections pose a global health burden, yet the cellular\nimmune responses driving protection or pathology remain unclear. Natural\ninfection cohorts often lack pre-exposure baseline data and structured temporal\nsampling. In contrast, inoculation and vaccination trials generate insightful\nlongitudinal transcriptomic data. However, the scattering of these datasets\nacross platforms, along with inconsistent metadata and preprocessing procedure,\nhinders AI-driven discovery. To address these challenges, we developed the\nHuman Respiratory Viral Immunization LongitudinAl Gene Expression\n(HR-VILAGE-3K3M) repository: an AI-ready, rigorously curated dataset that\nintegrates 14,136 RNA-seq profiles from 3,178 subjects across 66 studies\nencompassing over 2.56 million cells. Spanning vaccination, inoculation, and\nmixed exposures, the dataset includes microarray, bulk RNA-seq, and single-cell\nRNA-seq from whole blood, PBMCs, and nasal swabs, sourced from GEO, ImmPort,\nand ArrayExpress. We harmonized subject-level metadata, standardized outcome\nmeasures, applied unified preprocessing pipelines with rigorous quality\ncontrol, and aligned all data to official gene symbols. To demonstrate the\nutility of HR-VILAGE-3K3M, we performed predictive modeling of vaccine\nresponders and evaluated batch-effect correction methods. Beyond these initial\ndemonstrations, it supports diverse systems immunology applications and\nbenchmarking of feature selection and transfer learning algorithms. Its scale\nand heterogeneity also make it ideal for pretraining foundation models of the\nhuman immune response and for advancing multimodal learning frameworks. As the\nlargest longitudinal transcriptomic resource for human respiratory viral\nimmunization, it provides an accessible platform for reproducible AI-driven\nresearch, accelerating systems immunology and vaccine development against\nemerging viral threats.",
      "published": "May 19, 2025",
      "categories": [
        "q-bio.GN",
        "cs.LG",
        "stat.AP"
      ],
      "pdf_link": "http://arxiv.org/pdf/2505.14725v1",
      "arxiv_url": "http://arxiv.org/abs/2505.14725v1"
    },
    {
      "title": "A Finite-Sample Analysis of Distributionally Robust Average-Reward\n  Reinforcement Learning",
      "authors": [
        "Zachary Roch",
        "Chi Zhang",
        "George Atia",
        "Yue Wang"
      ],
      "abstract": "Robust reinforcement learning (RL) under the average-reward criterion is\ncrucial for long-term decision making under potential environment mismatches,\nyet its finite-sample complexity study remains largely unexplored. Existing\nworks offer algorithms with asymptotic guarantees, but the absence of\nfinite-sample analysis hinders its principled understanding and practical\ndeployment, especially in data-limited settings. We close this gap by proposing\nRobust Halpern Iteration (RHI), the first algorithm with provable finite-sample\ncomplexity guarantee. Under standard uncertainty sets -- including\ncontamination sets and $\\ell_p$-norm balls -- RHI attains an $\\epsilon$-optimal\npolicy with near-optimal sample complexity of $\\tilde{\\mathcal\nO}\\left(\\frac{SA\\mathcal H^{2}}{\\epsilon^{2}}\\right)$, where $S$ and $A$ denote\nthe numbers of states and actions, and $\\mathcal H$ is the robust optimal bias\nspan. This result gives the first polynomial sample complexity guarantee for\nrobust average-reward RL. Moreover, our RHI's independence from prior knowledge\ndistinguishes it from many previous average-reward RL studies. Our work thus\nconstitutes a significant advancement in enhancing the practical applicability\nof robust average-reward methods to complex, real-world problems.",
      "published": "May 18, 2025",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_link": "http://arxiv.org/pdf/2505.12462v1",
      "arxiv_url": "http://arxiv.org/abs/2505.12462v1"
    },
    {
      "title": "Near-Optimal Sample Complexities of Divergence-based S-rectangular\n  Distributionally Robust Reinforcement Learning",
      "authors": [
        "Zhenghao Li",
        "Shengbo Wang",
        "Nian Si"
      ],
      "abstract": "Distributionally robust reinforcement learning (DR-RL) has recently gained\nsignificant attention as a principled approach that addresses discrepancies\nbetween training and testing environments. To balance robustness, conservatism,\nand computational traceability, the literature has introduced DR-RL models with\nSA-rectangular and S-rectangular adversaries. While most existing statistical\nanalyses focus on SA-rectangular models, owing to their algorithmic simplicity\nand the optimality of deterministic policies, S-rectangular models more\naccurately capture distributional discrepancies in many real-world applications\nand often yield more effective robust randomized policies. In this paper, we\nstudy the empirical value iteration algorithm for divergence-based\nS-rectangular DR-RL and establish near-optimal sample complexity bounds of\n$\\widetilde{O}(|\\mathcal{S}||\\mathcal{A}|(1-\\gamma)^{-4}\\varepsilon^{-2})$,\nwhere $\\varepsilon$ is the target accuracy, $|\\mathcal{S}|$ and $|\\mathcal{A}|$\ndenote the cardinalities of the state and action spaces, and $\\gamma$ is the\ndiscount factor. To the best of our knowledge, these are the first sample\ncomplexity results for divergence-based S-rectangular models that achieve\noptimal dependence on $|\\mathcal{S}|$, $|\\mathcal{A}|$, and $\\varepsilon$\nsimultaneously. We further validate this theoretical dependence through\nnumerical experiments on a robust inventory control problem and a theoretical\nworst-case example, demonstrating the fast learning performance of our proposed\nalgorithm.",
      "published": "May 18, 2025",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_link": "http://arxiv.org/pdf/2505.12202v1",
      "arxiv_url": "http://arxiv.org/abs/2505.12202v1"
    },
    {
      "title": "Residual Feature Integration is Sufficient to Prevent Negative Transfer",
      "authors": [
        "Yichen Xu",
        "Ryumei Nakada",
        "Linjun Zhang",
        "Lexin Li"
      ],
      "abstract": "Transfer learning typically leverages representations learned from a source\ndomain to improve performance on a target task. A common approach is to extract\nfeatures from a pre-trained model and directly apply them for target\nprediction. However, this strategy is prone to negative transfer where the\nsource representation fails to align with the target distribution. In this\narticle, we propose Residual Feature Integration (REFINE), a simple yet\neffective method designed to mitigate negative transfer. Our approach combines\na fixed source-side representation with a trainable target-side encoder and\nfits a shallow neural network on the resulting joint representation, which\nadapts to the target domain while preserving transferable knowledge from the\nsource domain. Theoretically, we prove that REFINE is sufficient to prevent\nnegative transfer under mild conditions, and derive the generalization bound\ndemonstrating its theoretical benefit. Empirically, we show that REFINE\nconsistently enhances performance across diverse application and data\nmodalities including vision, text, and tabular data, and outperforms numerous\nalternative solutions. Our method is lightweight, architecture-agnostic, and\nrobust, making it a valuable addition to the existing transfer learning\ntoolbox.",
      "published": "May 17, 2025",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.ST",
        "stat.ML",
        "stat.TH"
      ],
      "pdf_link": "http://arxiv.org/pdf/2505.11771v1",
      "arxiv_url": "http://arxiv.org/abs/2505.11771v1"
    },
    {
      "title": "Humble your Overconfident Networks: Unlearning Overfitting via\n  Sequential Monte Carlo Tempered Deep Ensembles",
      "authors": [
        "Andrew Millard",
        "Zheng Zhao",
        "Joshua Murphy",
        "Simon Maskell"
      ],
      "abstract": "Sequential Monte Carlo (SMC) methods offer a principled approach to Bayesian\nuncertainty quantification but are traditionally limited by the need for\nfull-batch gradient evaluations. We introduce a scalable variant by\nincorporating Stochastic Gradient Hamiltonian Monte Carlo (SGHMC) proposals\ninto SMC, enabling efficient mini-batch based sampling. Our resulting SMCSGHMC\nalgorithm outperforms standard stochastic gradient descent (SGD) and deep\nensembles across image classification, out-of-distribution (OOD) detection, and\ntransfer learning tasks. We further show that SMCSGHMC mitigates overfitting\nand improves calibration, providing a flexible, scalable pathway for converting\npretrained neural networks into well-calibrated Bayesian models.",
      "published": "May 16, 2025",
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.CO"
      ],
      "pdf_link": "http://arxiv.org/pdf/2505.11671v1",
      "arxiv_url": "http://arxiv.org/abs/2505.11671v1"
    }
  ]
}