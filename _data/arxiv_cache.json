{
  "timestamp": 1741051000,
  "papers": [
    {
      "title": "A Principled Approach to Bayesian Transfer Learning",
      "authors": [
        "Adam Bretherton",
        "Joshua J. Bon",
        "David J. Warne",
        "Kerrie Mengersen",
        "Christopher Drovandi"
      ],
      "abstract": "Updating $\\textit{a priori}$ information given some observed data is the core\ntenet of Bayesian inference. Bayesian transfer learning extends this idea by\nincorporating information from a related dataset to improve the inference on\nthe observed data which may have been collected under slightly different\nsettings. The use of related information can be useful when the observed data\nis scarce, for example. Current Bayesian transfer learning methods that are\nbased on the so-called $\\textit{power prior}$ can adaptively transfer\ninformation from related data. Unfortunately, it is not always clear under\nwhich scenario Bayesian transfer learning performs best or even if it will\nimprove Bayesian inference. Additionally, current power prior methods rely on\nconjugacy to evaluate the posterior of interest. We propose using leave-one-out\ncross validation on the target dataset as a means of evaluating Bayesian\ntransfer learning methods. Further, we introduce a new framework,\n$\\textit{transfer sequential Monte Carlo}$, for power prior approaches that\nefficiently chooses the transfer parameter while avoiding the need for\nconjugate priors. We assess the performance of our proposed methods in two\ncomprehensive simulation studies.",
      "published": "February 27, 2025",
      "categories": [
        "stat.ME",
        "stat.CO",
        "62F15, 62F07 (Primary) 62-08 (Secondary)"
      ],
      "pdf_link": "http://arxiv.org/pdf/2502.19796v1",
      "arxiv_url": "http://arxiv.org/abs/2502.19796v1"
    },
    {
      "title": "Conformal Prediction Under Generalized Covariate Shift with Posterior\n  Drift",
      "authors": [
        "Baozhen Wang",
        "Xingye Qiao"
      ],
      "abstract": "In many real applications of statistical learning, collecting sufficiently\nmany training data is often expensive, time-consuming, or even unrealistic. In\nthis case, a transfer learning approach, which aims to leverage knowledge from\na related source domain to improve the learning performance in the target\ndomain, is more beneficial. There have been many transfer learning methods\ndeveloped under various distributional assumptions. In this article, we study a\nparticular type of classification problem, called conformal prediction, under a\nnew distributional assumption for transfer learning. Classifiers under the\nconformal prediction framework predict a set of plausible labels instead of one\nsingle label for each data instance, affording a more cautious and safer\ndecision. We consider a generalization of the \\textit{covariate shift with\nposterior drift} setting for transfer learning. Under this setting, we propose\na weighted conformal classifier that leverages both the source and target\nsamples, with a coverage guarantee in the target domain. Theoretical studies\ndemonstrate favorable asymptotic properties. Numerical studies further\nillustrate the usefulness of the proposed method.",
      "published": "February 25, 2025",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.ST",
        "stat.TH"
      ],
      "pdf_link": "http://arxiv.org/pdf/2502.17744v1",
      "arxiv_url": "http://arxiv.org/abs/2502.17744v1"
    },
    {
      "title": "Distributionally Robust Active Learning for Gaussian Process Regression",
      "authors": [
        "Shion Takeno",
        "Yoshito Okura",
        "Yu Inatsu",
        "Aoyama Tatsuya",
        "Tomonari Tanaka",
        "Akahane Satoshi",
        "Hiroyuki Hanada",
        "Noriaki Hashimoto",
        "Taro Murayama",
        "Hanju Lee",
        "Shinya Kojima",
        "Ichiro Takeuchi"
      ],
      "abstract": "Gaussian process regression (GPR) or kernel ridge regression is a widely used\nand powerful tool for nonlinear prediction. Therefore, active learning (AL) for\nGPR, which actively collects data labels to achieve an accurate prediction with\nfewer data labels, is an important problem. However, existing AL methods do not\ntheoretically guarantee prediction accuracy for target distribution.\nFurthermore, as discussed in the distributionally robust learning literature,\nspecifying the target distribution is often difficult. Thus, this paper\nproposes two AL methods that effectively reduce the worst-case expected error\nfor GPR, which is the worst-case expectation in target distribution candidates.\nWe show an upper bound of the worst-case expected squared error, which suggests\nthat the error will be arbitrarily small by a finite number of data labels\nunder mild conditions. Finally, we demonstrate the effectiveness of the\nproposed methods through synthetic and real-world datasets.",
      "published": "February 24, 2025",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_link": "http://arxiv.org/pdf/2502.16870v1",
      "arxiv_url": "http://arxiv.org/abs/2502.16870v1"
    },
    {
      "title": "Provable Benefits of Unsupervised Pre-training and Transfer Learning via\n  Single-Index Models",
      "authors": [
        "Taj Jones-McCormick",
        "Aukosh Jagannath",
        "Subhabrata Sen"
      ],
      "abstract": "Unsupervised pre-training and transfer learning are commonly used techniques\nto initialize training algorithms for neural networks, particularly in settings\nwith limited labeled data. In this paper, we study the effects of unsupervised\npre-training and transfer learning on the sample complexity of high-dimensional\nsupervised learning. Specifically, we consider the problem of training a\nsingle-layer neural network via online stochastic gradient descent. We\nestablish that pre-training and transfer learning (under concept shift) reduce\nsample complexity by polynomial factors (in the dimension) under very general\nassumptions. We also uncover some surprising settings where pre-training grants\nexponential improvement over random initialization in terms of sample\ncomplexity.",
      "published": "February 24, 2025",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "pdf_link": "http://arxiv.org/pdf/2502.16849v1",
      "arxiv_url": "http://arxiv.org/abs/2502.16849v1"
    },
    {
      "title": "Transfer Learning through Enhanced Sufficient Representation: Enriching\n  Source Domain Knowledge with Target Data",
      "authors": [
        "Yeheng Ge",
        "Xueyu Zhou",
        "Jian Huang"
      ],
      "abstract": "Transfer learning is an important approach for addressing the challenges\nposed by limited data availability in various applications. It accomplishes\nthis by transferring knowledge from well-established source domains to a less\nfamiliar target domain. However, traditional transfer learning methods often\nface difficulties due to rigid model assumptions and the need for a high degree\nof similarity between source and target domain models. In this paper, we\nintroduce a novel method for transfer learning called Transfer learning through\nEnhanced Sufficient Representation (TESR). Our approach begins by estimating a\nsufficient and invariant representation from the source domains. This\nrepresentation is then enhanced with an independent component derived from the\ntarget data, ensuring that it is sufficient for the target domain and adaptable\nto its specific characteristics. A notable advantage of TESR is that it does\nnot rely on assuming similar model structures across different tasks. For\nexample, the source domain models can be regression models, while the target\ndomain task can be classification. This flexibility makes TESR applicable to a\nwide range of supervised learning problems. We explore the theoretical\nproperties of TESR and validate its performance through simulation studies and\nreal-world data applications, demonstrating its effectiveness in finite sample\nsettings.",
      "published": "February 22, 2025",
      "categories": [
        "stat.ML",
        "cs.LG",
        "62G05, 68T07"
      ],
      "pdf_link": "http://arxiv.org/pdf/2502.20414v1",
      "arxiv_url": "http://arxiv.org/abs/2502.20414v1"
    },
    {
      "title": "Improving variable selection properties by using external data",
      "authors": [
        "Paul Rognon-Vael",
        "David Rossell",
        "Piotr Zwiernik"
      ],
      "abstract": "Sparse high-dimensional signal recovery is only possible under certain\nconditions on the number of parameters, sample size, signal strength and\nunderlying sparsity. We show that these mathematical limits can be pushed when\none has external information that allow splitting parameters into blocks. This\noccurs in many applications, including data integration and transfer learning\ntasks. Specifically, we consider the Gaussian sequence model and linear\nregression, and show how block-based $\\ell_0$ penalties attain model selection\nconsistency under milder conditions than standard $\\ell_0$ penalties, and they\nalso attain faster model recovery rates. We first provide results for\noracle-based $\\ell_0$ penalties that have access to perfect sparsity and signal\nstrength information, and subsequently empirical Bayes-motivated block $\\ell_0$\npenalties that does not require oracle information.",
      "published": "February 21, 2025",
      "categories": [
        "math.ST",
        "stat.ME",
        "stat.TH",
        "62F07 (Primary) 62C12, 62R07 (Secondary)"
      ],
      "pdf_link": "http://arxiv.org/pdf/2502.15584v1",
      "arxiv_url": "http://arxiv.org/abs/2502.15584v1"
    },
    {
      "title": "Distribution Matching for Self-Supervised Transfer Learning",
      "authors": [
        "Yuling Jiao",
        "Wensen Ma",
        "Defeng Sun",
        "Hansheng Wang",
        "Yang Wang"
      ],
      "abstract": "In this paper, we propose a novel self-supervised transfer learning method\ncalled Distribution Matching (DM), which drives the representation distribution\ntoward a predefined reference distribution while preserving augmentation\ninvariance. The design of DM results in a learned representation space that is\nintuitively structured and offers easily interpretable hyperparameters.\nExperimental results across multiple real-world datasets and evaluation metrics\ndemonstrate that DM performs competitively on target classification tasks\ncompared to existing self-supervised transfer learning methods. Additionally,\nwe provide robust theoretical guarantees for DM, including a population theorem\nand an end-to-end sample theorem. The population theorem bridges the gap\nbetween the self-supervised learning task and target classification accuracy,\nwhile the sample theorem shows that, even with a limited number of samples from\nthe target domain, DM can deliver exceptional classification performance,\nprovided the unlabeled sample size is sufficiently large.",
      "published": "February 20, 2025",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG",
        "stat.ME"
      ],
      "pdf_link": "http://arxiv.org/pdf/2502.14424v1",
      "arxiv_url": "http://arxiv.org/abs/2502.14424v1"
    },
    {
      "title": "Unsupervised optimal deep transfer learning for classification under\n  general conditional shift",
      "authors": [
        "Junjun Lang",
        "Yukun Liu"
      ],
      "abstract": "Classifiers trained solely on labeled source data may yield misleading\nresults when applied to unlabeled target data drawn from a different\ndistribution. Transfer learning can rectify this by transferring knowledge from\nsource to target data, but its effectiveness frequently relies on stringent\nassumptions, such as label shift. In this paper, we introduce a novel General\nConditional Shift (GCS) assumption, which encompasses label shift as a special\nscenario. Under GCS, we demonstrate that both the target distribution and the\nshift function are identifiable. To estimate the conditional probabilities\n${\\bm\\eta}_P$ for source data, we propose leveraging deep neural networks\n(DNNs). Subsequent to transferring the DNN estimator, we estimate the target\nlabel distribution ${\\bm\\pi}_Q$ utilizing a pseudo-maximum likelihood approach.\nUltimately, by incorporating these estimates and circumventing the need to\nestimate the shift function, we construct our proposed Bayes classifier. We\nestablish concentration bounds for our estimators of both ${\\bm\\eta}_P$ and\n${\\bm\\pi}_Q$ in terms of the intrinsic dimension of ${\\bm\\eta}_P$ . Notably,\nour DNN-based classifier achieves the optimal minimax rate, up to a logarithmic\nfactor. A key advantage of our method is its capacity to effectively combat the\ncurse of dimensionality when ${\\bm\\eta}_P$ exhibits a low-dimensional\nstructure. Numerical simulations, along with an analysis of an Alzheimer's\ndisease dataset, underscore its exceptional performance.",
      "published": "February 18, 2025",
      "categories": [
        "math.ST",
        "stat.TH"
      ],
      "pdf_link": "http://arxiv.org/pdf/2502.12729v1",
      "arxiv_url": "http://arxiv.org/abs/2502.12729v1"
    }
  ]
}