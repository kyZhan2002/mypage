{
  "timestamp": 1747358705,
  "papers": [
    {
      "title": "Wasserstein Distributionally Robust Nonparametric Regression",
      "authors": [
        "Changyu Liu",
        "Yuling Jiao",
        "Junhui Wang",
        "Jian Huang"
      ],
      "abstract": "Distributionally robust optimization has become a powerful tool for\nprediction and decision-making under model uncertainty. By focusing on the\nlocal worst-case risk, it enhances robustness by identifying the most\nunfavorable distribution within a predefined ambiguity set. While extensive\nresearch has been conducted in parametric settings, studies on nonparametric\nframeworks remain limited. This paper studies the generalization properties of\nWasserstein distributionally robust nonparametric estimators, with particular\nattention to the impact of model misspecification, where non-negligible\ndiscrepancies between the estimation function space and target function can\nimpair generalization performance. We establish non-asymptotic error bounds for\nthe excess local worst-case risk by analyzing the regularization effects\ninduced by distributional perturbations and employing feedforward neural\nnetworks with Lipschitz constraints. These bounds illustrate how uncertainty\nlevels and neural network structures influence generalization performance and\nare applicable to both Lipschitz and quadratic loss functions. Furthermore, we\ninvestigate the Lagrangian relaxation of the local worst-case risk and derive\ncorresponding non-asymptotic error bounds for these estimators. The robustness\nof the proposed estimator is evaluated through simulation studies and\nillustrated with an application to the MNIST dataset.",
      "published": "May 12, 2025",
      "categories": [
        "stat.ML",
        "cs.LG",
        "62G05, 62G08, 68T07"
      ],
      "pdf_link": "http://arxiv.org/pdf/2505.07967v1",
      "arxiv_url": "http://arxiv.org/abs/2505.07967v1"
    },
    {
      "title": "Transfer Learning Across Fixed-Income Product Classes",
      "authors": [
        "Nicolas Camenzind",
        "Damir Filipovic"
      ],
      "abstract": "We propose a framework for transfer learning of discount curves across\ndifferent fixed-income product classes. Motivated by challenges in estimating\ndiscount curves from sparse or noisy data, we extend kernel ridge regression\n(KR) to a vector-valued setting, formulating a convex optimization problem in a\nvector-valued reproducing kernel Hilbert space (RKHS). Each component of the\nsolution corresponds to the discount curve implied by a specific product class.\nWe introduce an additional regularization term motivated by economic\nprinciples, promoting smoothness of spread curves between product classes, and\nshow that it leads to a valid separable kernel structure. A main theoretical\ncontribution is a decomposition of the vector-valued RKHS norm induced by\nseparable kernels. We further provide a Gaussian process interpretation of\nvector-valued KR, enabling quantification of estimation uncertainty.\nIllustrative examples demonstrate that transfer learning significantly improves\nextrapolation performance and tightens confidence intervals compared to\nsingle-curve estimation.",
      "published": "May 12, 2025",
      "categories": [
        "stat.ML",
        "cs.LG",
        "q-fin.CP",
        "q-fin.MF"
      ],
      "pdf_link": "http://arxiv.org/pdf/2505.07676v1",
      "arxiv_url": "http://arxiv.org/abs/2505.07676v1"
    },
    {
      "title": "Enhancing Inference for Small Cohorts via Transfer Learning and Weighted\n  Integration of Multiple Datasets",
      "authors": [
        "Subharup Guha",
        "Mengqi Xu",
        "Yi Li"
      ],
      "abstract": "Lung sepsis remains a significant concern in the Northeastern U.S., yet the\nnational eICU Collaborative Database includes only a small number of patients\nfrom this region, highlighting underrepresentation. Understanding clinical\nvariables such as FiO2, creatinine, platelets, and lactate, which reflect\noxygenation, kidney function, coagulation, and metabolism, is crucial because\nthese markers influence sepsis outcomes and may vary by sex. Transfer learning\nhelps address small sample sizes by borrowing information from larger datasets,\nalthough differences in covariates and outcome-generating mechanisms between\nthe target and external cohorts can complicate the process. We propose a novel\nweighting method, TRANSfer LeArning wiTh wEights (TRANSLATE), to integrate data\nfrom various sources by incorporating domain-specific characteristics through\nlearned weights that align external data with the target cohort. These weights\nadjust for cohort differences, are proportional to each cohort's effective\nsample size, and downweight dissimilar cohorts. TRANSLATE offers theoretical\nguarantees for improved precision and applies to a wide range of estimands,\nincluding means, variances, and distribution functions. Simulations and a\nreal-data application to sepsis outcomes in the Northeast cohort, using a much\nlarger sample from other U.S. regions, show that the method enhances inference\nwhile accounting for regional heterogeneity.",
      "published": "May 11, 2025",
      "categories": [
        "stat.ME"
      ],
      "pdf_link": "http://arxiv.org/pdf/2505.07153v1",
      "arxiv_url": "http://arxiv.org/abs/2505.07153v1"
    },
    {
      "title": "A systematic review of challenges and proposed solutions in modeling\n  multimodal data",
      "authors": [
        "Maryam Farhadizadeh",
        "Maria Weymann",
        "Michael Bla\u00df",
        "Johann Kraus",
        "Christopher Gundler",
        "Sebastian Walter",
        "Noah Hempen",
        "Harald Binder",
        "Nadine Binder"
      ],
      "abstract": "Multimodal data modeling has emerged as a powerful approach in clinical\nresearch, enabling the integration of diverse data types such as imaging,\ngenomics, wearable sensors, and electronic health records. Despite its\npotential to improve diagnostic accuracy and support personalized care,\nmodeling such heterogeneous data presents significant technical challenges.\nThis systematic review synthesizes findings from 69 studies to identify common\nobstacles, including missing modalities, limited sample sizes, dimensionality\nimbalance, interpretability issues, and finding the optimal fusion techniques.\nWe highlight recent methodological advances, such as transfer learning,\ngenerative models, attention mechanisms, and neural architecture search that\noffer promising solutions. By mapping current trends and innovations, this\nreview provides a comprehensive overview of the field and offers practical\ninsights to guide future research and development in multimodal modeling for\nmedical applications.",
      "published": "May 11, 2025",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_link": "http://arxiv.org/pdf/2505.06945v2",
      "arxiv_url": "http://arxiv.org/abs/2505.06945v2"
    },
    {
      "title": "Model Steering: Learning with a Reference Model Improves Generalization\n  Bounds and Scaling Laws",
      "authors": [
        "Xiyuan Wei",
        "Ming Lin",
        "Fanjiang Ye",
        "Fengguang Song",
        "Liangliang Cao",
        "My T. Thai",
        "Tianbao Yang"
      ],
      "abstract": "This paper formalizes an emerging learning paradigm that uses a trained model\nas a reference to guide and enhance the training of a target model through\nstrategic data selection or weighting, named $\\textbf{model steering}$. While\nad-hoc methods have been used in various contexts, including the training of\nlarge foundation models, its underlying principles remain insufficiently\nunderstood, leading to sub-optimal performance. In this work, we propose a\ntheory-driven framework for model steering called $\\textbf{DRRho risk\nminimization}$, which is rooted in Distributionally Robust Optimization (DRO).\nThrough a generalization analysis, we provide theoretical insights into why\nthis approach improves generalization and data efficiency compared to training\nwithout a reference model. To the best of our knowledge, this is the first time\nsuch theoretical insights are provided for the new learning paradigm, which\nsignificantly enhance our understanding and practice of model steering.\nBuilding on these insights and the connection between contrastive learning and\nDRO, we introduce a novel method for Contrastive Language-Image Pretraining\n(CLIP) with a reference model, termed DRRho-CLIP. Extensive experiments\nvalidate the theoretical insights, reveal a superior scaling law compared to\nCLIP without a reference model, and demonstrate its strength over existing\nheuristic approaches.",
      "published": "May 10, 2025",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_link": "http://arxiv.org/pdf/2505.06699v2",
      "arxiv_url": "http://arxiv.org/abs/2505.06699v2"
    },
    {
      "title": "Multi-modal cascade feature transfer for polymer property prediction",
      "authors": [
        "Kiichi Obuchi",
        "Yuta Yahagi",
        "Kiyohiko Toyama",
        "Shukichi Tanaka",
        "Kota Matsui"
      ],
      "abstract": "In this paper, we propose a novel transfer learning approach called\nmulti-modal cascade model with feature transfer for polymer property\nprediction.Polymers are characterized by a composite of data in several\ndifferent formats, including molecular descriptors and additive information as\nwell as chemical structures. However, in conventional approaches, prediction\nmodels were often constructed using each type of data separately. Our model\nenables more accurate prediction of physical properties for polymers by\ncombining features extracted from the chemical structure by graph convolutional\nneural networks (GCN) with features such as molecular descriptors and additive\ninformation. The predictive performance of the proposed method is empirically\nevaluated using several polymer datasets. We report that the proposed method\nshows high predictive performance compared to the baseline conventional\napproach using a single feature.",
      "published": "May 06, 2025",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "pdf_link": "http://arxiv.org/pdf/2505.03704v2",
      "arxiv_url": "http://arxiv.org/abs/2505.03704v2"
    },
    {
      "title": "Decision Making under Model Misspecification: DRO with Robust Bayesian\n  Ambiguity Sets",
      "authors": [
        "Charita Dellaporta",
        "Patrick O'Hara",
        "Theodoros Damoulas"
      ],
      "abstract": "Distributionally Robust Optimisation (DRO) protects risk-averse\ndecision-makers by considering the worst-case risk within an ambiguity set of\ndistributions based on the empirical distribution or a model. To further guard\nagainst finite, noisy data, model-based approaches admit Bayesian formulations\nthat propagate uncertainty from the posterior to the decision-making problem.\nHowever, when the model is misspecified, the decision maker must stretch the\nambiguity set to contain the data-generating process (DGP), leading to overly\nconservative decisions. We address this challenge by introducing DRO with\nRobust, to model misspecification, Bayesian Ambiguity Sets (DRO-RoBAS). These\nare Maximum Mean Discrepancy ambiguity sets centred at a robust posterior\npredictive distribution that incorporates beliefs about the DGP. We show that\nthe resulting optimisation problem obtains a dual formulation in the\nReproducing Kernel Hilbert Space and we give probabilistic guarantees on the\ntolerance level of the ambiguity set. Our method outperforms other Bayesian and\nempirical DRO approaches in out-of-sample performance on the Newsvendor and\nPortfolio problems with various cases of model misspecification.",
      "published": "May 06, 2025",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "pdf_link": "http://arxiv.org/pdf/2505.03585v1",
      "arxiv_url": "http://arxiv.org/abs/2505.03585v1"
    },
    {
      "title": "StablePCA: Learning Shared Representations across Multiple Sources via\n  Minimax Optimization",
      "authors": [
        "Zhenyu Wang",
        "Molei Liu",
        "Jing Lei",
        "Francis Bach",
        "Zijian Guo"
      ],
      "abstract": "When synthesizing multisource high-dimensional data, a key objective is to\nextract low-dimensional feature representations that effectively approximate\nthe original features across different sources. Such general feature extraction\nfacilitates the discovery of transferable knowledge, mitigates systematic\nbiases such as batch effects, and promotes fairness. In this paper, we propose\nStable Principal Component Analysis (StablePCA), a novel method for group\ndistributionally robust learning of latent representations from\nhigh-dimensional multi-source data. A primary challenge in generalizing PCA to\nthe multi-source regime lies in the nonconvexity of the fixed rank constraint,\nrendering the minimax optimization nonconvex. To address this challenge, we\nemploy the Fantope relaxation, reformulating the problem as a convex minimax\noptimization, with the objective defined as the maximum loss across sources. To\nsolve the relaxed formulation, we devise an optimistic-gradient Mirror Prox\nalgorithm with explicit closed-form updates. Theoretically, we establish the\nglobal convergence of the Mirror Prox algorithm, with the convergence rate\nprovided from the optimization perspective. Furthermore, we offer practical\ncriteria to assess how closely the solution approximates the original nonconvex\nformulation. Through extensive numerical experiments, we demonstrate\nStablePCA's high accuracy and efficiency in extracting robust low-dimensional\nrepresentations across various finite-sample scenarios.",
      "published": "May 02, 2025",
      "categories": [
        "cs.LG",
        "math.OC",
        "stat.CO",
        "stat.ME"
      ],
      "pdf_link": "http://arxiv.org/pdf/2505.00940v1",
      "arxiv_url": "http://arxiv.org/abs/2505.00940v1"
    },
    {
      "title": "AI-Assisted Decision-Making for Clinical Assessment of Auto-Segmented\n  Contour Quality",
      "authors": [
        "Biling Wang",
        "Austen Maniscalco",
        "Ti Bai",
        "Siqiu Wang",
        "Michael Dohopolski",
        "Mu-Han Lin",
        "Chenyang Shen",
        "Dan Nguyen",
        "Junzhou Huang",
        "Steve Jiang",
        "Xinlei Wang"
      ],
      "abstract": "Purpose: This study presents a Deep Learning (DL)-based quality assessment\n(QA) approach for evaluating auto-generated contours (auto-contours) in\nradiotherapy, with emphasis on Online Adaptive Radiotherapy (OART). Leveraging\nBayesian Ordinal Classification (BOC) and calibrated uncertainty thresholds,\nthe method enables confident QA predictions without relying on ground truth\ncontours or extensive manual labeling. Methods: We developed a BOC model to\nclassify auto-contour quality and quantify prediction uncertainty. A\ncalibration step was used to optimize uncertainty thresholds that meet clinical\naccuracy needs. The method was validated under three data scenarios: no manual\nlabels, limited labels, and extensive labels. For rectum contours in prostate\ncancer, we applied geometric surrogate labels when manual labels were absent,\ntransfer learning when limited, and direct supervision when ample labels were\navailable. Results: The BOC model delivered robust performance across all\nscenarios. Fine-tuning with just 30 manual labels and calibrating with 34\nsubjects yielded over 90% accuracy on test data. Using the calibrated\nthreshold, over 93% of the auto-contours' qualities were accurately predicted\nin over 98% of cases, reducing unnecessary manual reviews and highlighting\ncases needing correction. Conclusion: The proposed QA model enhances contouring\nefficiency in OART by reducing manual workload and enabling fast, informed\nclinical decisions. Through uncertainty quantification, it ensures safer, more\nreliable radiotherapy workflows.",
      "published": "May 01, 2025",
      "categories": [
        "cs.CV",
        "cs.AI",
        "stat.AP"
      ],
      "pdf_link": "http://arxiv.org/pdf/2505.00308v2",
      "arxiv_url": "http://arxiv.org/abs/2505.00308v2"
    },
    {
      "title": "Convergence rate for Nearest Neighbour matching: geometry of the domain\n  and higher-order regularity",
      "authors": [
        "Simon Viel",
        "Lionel Truquet",
        "Ikko Yamane"
      ],
      "abstract": "Estimating some mathematical expectations from partially observed data and in\nparticular missing outcomes is a central problem encountered in numerous fields\nsuch as transfer learning, counterfactual analysis or causal inference.\nMatching estimators, estimators based on k-nearest neighbours, are widely used\nin this context. It is known that the variance of such estimators can converge\nto zero at a parametric rate, but their bias can have a slower rate when the\ndimension of the covariates is larger than 2. This makes analysis of this bias\nparticularly important. In this paper, we provide higher order properties of\nthe bias. In contrast to the existing literature related to this problem, we do\nnot assume that the support of the target distribution of the covariates is\nstrictly included in that of the source, and we analyse two geometric\nconditions on the support that avoid such boundary bias problems. We show that\nthese conditions are much more general than the usual convex support\nassumption, leading to an improvement of existing results. Furthermore, we show\nthat the matching estimator studied by Abadie and Imbens (2006) for the average\ntreatment effect can be asymptotically efficient when the dimension of the\ncovariates is less than 4, a result only known in dimension 1.",
      "published": "April 30, 2025",
      "categories": [
        "math.ST",
        "stat.TH"
      ],
      "pdf_link": "http://arxiv.org/pdf/2504.21633v1",
      "arxiv_url": "http://arxiv.org/abs/2504.21633v1"
    }
  ]
}