{
  "timestamp": 1763342726,
  "papers": [
    {
      "title": "Reluctant Transfer Learning in Penalized Regressions for Individualized Treatment Rules under Effect Heterogeneity",
      "authors": [
        "Eun Jeong Oh",
        "Min Qian"
      ],
      "abstract": "Estimating individualized treatment rules (ITRs) is fundamental to precision medicine, where the goal is to tailor treatment decisions to individual patient characteristics. While numerous methods have been developed for ITR estimation, there is limited research on model updating that accounts for shifted treatment-covariate relationships in the ITR setting. In real-world practice, models trained on source data must be updated for new (target) datasets that exhibit shifts in treatment effects. To address this challenge, we propose a Reluctant Transfer Learning (RTL) framework that enables efficient model adaptation by selectively transferring essential model components (e.g., regression coefficients) from source to target data, without requiring access to individual-level source data. Leveraging the principle of reluctant modeling, the RTL approach incorporates model adjustments only when they improve performance on the target dataset, thereby controlling complexity and enhancing generalizability. Our method supports multi-armed treatment settings, performs variable selection for interpretability, and provides theoretical guarantees for the value convergence. Through simulation studies and an application to a real data example from the Best Apnea Interventions for Research (BestAIR) trial, we demonstrate that RTL outperforms existing alternatives. The proposed framework offers an efficient, practically feasible approach to adaptive treatment decision-making under evolving treatment effect conditions.",
      "published": "November 11, 2025",
      "categories": [
        "stat.ME"
      ],
      "pdf_link": "https://arxiv.org/pdf/2511.08559v1",
      "arxiv_url": "https://arxiv.org/abs/2511.08559v1"
    },
    {
      "title": "Source-Optimal Training is Transfer-Suboptimal",
      "authors": [
        "C. Evans Hedges"
      ],
      "abstract": "We prove a fundamental misalignment in transfer learning: the source regularization that minimizes source risk almost never coincides with the regularization maximizing transfer benefit. Through sharp phase boundaries for L2-SP ridge regression, we characterize the transfer-optimal source penalty $\u03c4_0^*$ and show it diverges predictably from task-optimal values, requiring stronger regularization in high-SNR regimes and weaker regularization in low-SNR regimes. Additionally, in isotropic settings the decision to transfer is remarkably independent of target sample size and noise, depending only on task alignment and source characteristics. CIFAR-10 and MNIST experiments confirm this counterintuitive pattern persists in non-linear networks.",
      "published": "November 11, 2025",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.ST"
      ],
      "pdf_link": "https://arxiv.org/pdf/2511.08401v1",
      "arxiv_url": "https://arxiv.org/abs/2511.08401v1"
    },
    {
      "title": "Distributionally Robust Online Markov Game with Linear Function Approximation",
      "authors": [
        "Zewu Zheng",
        "Yuanyuan Lin"
      ],
      "abstract": "The sim-to-real gap, where agents trained in a simulator face significant performance degradation during testing, is a fundamental challenge in reinforcement learning. Extansive works adopt the framework of distributionally robust RL, to learn a policy that acts robustly under worst case environment shift. Within this framework, our objective is to devise algorithms that are sample efficient with interactive data collection and large state spaces. By assuming d-rectangularity of environment dynamic shift, we identify a fundamental hardness result for learning in online Markov game, and address it by adopting minimum value assumption. Then, a novel least square value iteration type algorithm, DR-CCE-LSI, with exploration bonus devised specifically for multiple agents, is proposed to find an \\episilon-approximate robust Coarse Correlated Equilibrium(CCE). To obtain sample efficient learning, we find that: when the feature mapping function satisfies certain properties, our algorithm, DR-CCE-LSI, is able to achieve \u03b5-approximate CCE with a regret bound of O{dHmin{H,1/min{\u03c3_i}}\\sqrt{K}}, where K is the number of interacting episodes, H is the horizon length, d is the feature dimension, and \\simga_i represents the uncertainty level of player i. Our work introduces the first sample-efficient algorithm for this setting, matches the best result so far in single agent setting, and achieves minimax optimalsample complexity in terms of the feature dimension d. Meanwhile, we also conduct simulation study to validate the efficacy of our algorithm in learning a robust equilibrium.",
      "published": "November 11, 2025",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "pdf_link": "https://arxiv.org/pdf/2511.07831v1",
      "arxiv_url": "https://arxiv.org/abs/2511.07831v1"
    }
  ]
}