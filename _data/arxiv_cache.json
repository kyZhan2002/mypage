{
  "timestamp": 1744852852,
  "papers": [
    {
      "title": "Rank-based transfer learning for high-dimensional survival data with\n  application to sepsis data",
      "authors": [
        "Nan Qiao",
        "Haowei Jiang",
        "Cunjie Lin"
      ],
      "abstract": "Sepsis remains a critical challenge due to its high mortality and complex\nprognosis. To address data limitations in studying MSSA sepsis, we extend\nexisting transfer learning frameworks to accommodate transformation models for\nhigh-dimensional survival data. Specifically, we construct a measurement index\nbased on C-index for intelligently identifying the helpful source datasets, and\nthe target model performance is improved by leveraging information from the\nidentified source datasets via performing the transfer step and debiasing step.\nWe further provide an algorithm to construct confidence intervals for each\ncoefficient component. Another significant development is that statistical\nproperties are rigorously established, including $\\ell_1/\\ell_2$-estimation\nerror bounds of the transfer learning algorithm, detection consistency property\nof the transferable source detection algorithm and asymptotic theories for the\nconfidence interval construction. Extensive simulations and analysis of\nMIMIC-IV sepsis data demonstrate the estimation and prediction accuracy, and\npractical advantages of our approach, providing significant improvements in\nsurvival estimates for MSSA sepsis patients.",
      "published": "April 15, 2025",
      "categories": [
        "stat.AP"
      ],
      "pdf_link": "http://arxiv.org/pdf/2504.11270v1",
      "arxiv_url": "http://arxiv.org/abs/2504.11270v1"
    },
    {
      "title": "Conditional Data Synthesis Augmentation",
      "authors": [
        "Xinyu Tian",
        "Xiaotong Shen"
      ],
      "abstract": "Reliable machine learning and statistical analysis rely on diverse,\nwell-distributed training data. However, real-world datasets are often limited\nin size and exhibit underrepresentation across key subpopulations, leading to\nbiased predictions and reduced performance, particularly in supervised tasks\nsuch as classification. To address these challenges, we propose Conditional\nData Synthesis Augmentation (CoDSA), a novel framework that leverages\ngenerative models, such as diffusion models, to synthesize high-fidelity data\nfor improving model performance across multimodal domains including tabular,\ntextual, and image data. CoDSA generates synthetic samples that faithfully\ncapture the conditional distributions of the original data, with a focus on\nunder-sampled or high-interest regions. Through transfer learning, CoDSA\nfine-tunes pre-trained generative models to enhance the realism of synthetic\ndata and increase sample density in sparse areas. This process preserves\ninter-modal relationships, mitigates data imbalance, improves domain\nadaptation, and boosts generalization. We also introduce a theoretical\nframework that quantifies the statistical accuracy improvements enabled by\nCoDSA as a function of synthetic sample volume and targeted region allocation,\nproviding formal guarantees of its effectiveness. Extensive experiments\ndemonstrate that CoDSA consistently outperforms non-adaptive augmentation\nstrategies and state-of-the-art baselines in both supervised and unsupervised\nsettings.",
      "published": "April 10, 2025",
      "categories": [
        "stat.ME",
        "cs.LG"
      ],
      "pdf_link": "http://arxiv.org/pdf/2504.07426v1",
      "arxiv_url": "http://arxiv.org/abs/2504.07426v1"
    },
    {
      "title": "Sparse Optimization for Transfer Learning: A L0-Regularized Framework\n  for Multi-Source Domain Adaptation",
      "authors": [
        "Chenqi Gong",
        "Hu Yang"
      ],
      "abstract": "This paper explores transfer learning in heterogeneous multi-source\nenvironments with distributional divergence between target and auxiliary\ndomains. To address challenges in statistical bias and computational\nefficiency, we propose a Sparse Optimization for Transfer Learning (SOTL)\nframework based on L0-regularization. The method extends the Joint Estimation\nTransferred from Strata (JETS) paradigm with two key innovations: (1)\nL0-constrained exact sparsity for parameter space compression and complexity\nreduction, and (2) refining optimization focus to emphasize target parameters\nover redundant ones. Simulations show that SOTL significantly improves both\nestimation accuracy and computational speed, especially under adversarial\nauxiliary domain conditions. Empirical validation on the Community and Crime\nbenchmarks demonstrates the statistical robustness of the SOTL method in\ncross-domain transfer.",
      "published": "April 07, 2025",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "pdf_link": "http://arxiv.org/pdf/2504.04812v1",
      "arxiv_url": "http://arxiv.org/abs/2504.04812v1"
    },
    {
      "title": "Block Toeplitz Sparse Precision Matrix Estimation for Large-Scale\n  Interval-Valued Time Series Forecasting",
      "authors": [
        "Wan Tian",
        "Zhongfeng Qin"
      ],
      "abstract": "Modeling and forecasting interval-valued time series (ITS) have attracted\nconsiderable attention due to their growing presence in various contexts. To\nthe best of our knowledge, there have been no efforts to model large-scale ITS.\nIn this paper, we propose a feature extraction procedure for large-scale ITS,\nwhich involves key steps such as auto-segmentation and clustering, and feature\ntransfer learning. This procedure can be seamlessly integrated with any\nsuitable prediction models for forecasting purposes. Specifically, we transform\nthe automatic segmentation and clustering of ITS into the estimation of\nToeplitz sparse precision matrices and assignment set. The\nmajorization-minimization algorithm is employed to convert this highly\nnon-convex optimization problem into two subproblems. We derive efficient\ndynamic programming and alternating direction method to solve these two\nsubproblems alternately and establish their convergence properties. By\nemploying the Joint Recurrence Plot (JRP) to image subsequence and assigning a\nclass label to each cluster, an image dataset is constructed. Then, an\nappropriate neural network is chosen to train on this image dataset and used to\nextract features for the next step of forecasting. Real data applications\ndemonstrate that the proposed method can effectively obtain invariant\nrepresentations of the raw data and enhance forecasting performance.",
      "published": "April 04, 2025",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "pdf_link": "http://arxiv.org/pdf/2504.03322v1",
      "arxiv_url": "http://arxiv.org/abs/2504.03322v1"
    },
    {
      "title": "A model-free feature extraction procedure for interval-valued time\n  series prediction",
      "authors": [
        "Wan Tian",
        "Zhongfeng Qin",
        "Tao Hu"
      ],
      "abstract": "In this paper, we present a novel feature extraction procedure to predict\ninterval-valued time series by combing transfer learning and imaging\napproaches. Initially, we represent interval-valued time series using a\nbivariate point-valued time series, which serves as a representative form. We\nfirst transform each time series into images by employing various imaging\napproaches such as recurrence plot, gramian angular summation/difference field,\nand Markov transition field, and construct an image dataset by treating each\nimaging method's output as a separate class. Based on this dataset, we train\nseveral candidates for a feature extraction network (FEN), specifically ResNet\nwith varying layers. Then we choose the penultimate layer of the FEN to extract\nthe most relevant features from the transformed images. We integrate the\nextracted features into conventional predictive models to formulate the\ncorresponding prediction models. To formulate prediction, we integrate the\nextracted features into a regular prediction model. The proposed methods are\nevaluated based on the S\\&P 500 index and three data-generating processes\n(DGPs), and the experimental results demonstrate a notable improvement in\nprediction performance compared to existing methods.",
      "published": "April 04, 2025",
      "categories": [
        "stat.AP"
      ],
      "pdf_link": "http://arxiv.org/pdf/2504.03310v1",
      "arxiv_url": "http://arxiv.org/abs/2504.03310v1"
    }
  ]
}