{
  "timestamp": 1758763149,
  "papers": [
    {
      "title": "DRO-REBEL: Distributionally Robust Relative-Reward Regression for Fast\n  and Efficient LLM Alignment",
      "authors": [
        "Sharan Sahu",
        "Martin T. Wells"
      ],
      "abstract": "Reinforcement learning with human feedback (RLHF) has become crucial for\naligning Large Language Models (LLMs) with human intent. However, existing\noffline RLHF approaches suffer from overoptimization, where models overfit to\nreward misspecification and drift from preferred behaviors observed during\ntraining. We introduce DRO-REBEL, a unified family of robust REBEL updates with\ntype-$p$ Wasserstein, KL, and $\\chi^2$ ambiguity sets. Using Fenchel duality,\neach update reduces to a simple relative-reward regression, preserving\nscalability and avoiding PPO-style clipping or auxiliary value networks. Under\nstandard linear-reward and log-linear policy classes with a data-coverage\ncondition, we establish $O(n^{-1/4})$ estimation bounds with tighter constants\nthan prior DRO-DPO approaches, and recover the minimax-optimal $O(n^{-1/2})$\nrate via a localized Rademacher complexity analysis. The same analysis closes\nthe gap for Wasserstein-DPO and KL-DPO, showing both also attain optimal\nparametric rates. We derive practical SGD algorithms for all three divergences:\ngradient regularization (Wasserstein), importance weighting (KL), and a fast\n1-D dual solve ($\\chi^2$). Experiments on Emotion Alignment, the large-scale\nArmoRM multi-objective benchmark, and HH-Alignment demonstrate strong\nworst-case robustness across unseen preference mixtures, model sizes, and data\nscales, with $\\chi^2$-REBEL showing consistently strong empirical performance.\nA controlled radius--coverage study validates a no-free-lunch trade-off: radii\nshrinking faster than empirical divergence concentration rates achieve\nminimax-optimal parametric rates but forfeit coverage, while\ncoverage-guaranteeing radii incur $O(n^{-1/4})$ rates.",
      "published": "September 23, 2025",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_link": "http://arxiv.org/pdf/2509.19104v1",
      "arxiv_url": "http://arxiv.org/abs/2509.19104v1"
    },
    {
      "title": "Asymptotically Optimal Problem-Dependent Bandit Policies for Transfer\n  Learning",
      "authors": [
        "Adrien Prevost",
        "Timothee Mathieu",
        "Odalric-Ambrym Maillard"
      ],
      "abstract": "We study the non-contextual multi-armed bandit problem in a transfer learning\nsetting: before any pulls, the learner is given N'_k i.i.d. samples from each\nsource distribution nu'_k, and the true target distributions nu_k lie within a\nknown distance bound d_k(nu_k, nu'_k) <= L_k. In this framework, we first\nderive a problem-dependent asymptotic lower bound on cumulative regret that\nextends the classical Lai-Robbins result to incorporate the transfer parameters\n(d_k, L_k, N'_k). We then propose KL-UCB-Transfer, a simple index policy that\nmatches this new bound in the Gaussian case. Finally, we validate our approach\nvia simulations, showing that KL-UCB-Transfer significantly outperforms the\nno-prior baseline when source and target distributions are sufficiently close.",
      "published": "September 23, 2025",
      "categories": [
        "cs.LG",
        "math.ST",
        "stat.TH"
      ],
      "pdf_link": "http://arxiv.org/pdf/2509.19098v1",
      "arxiv_url": "http://arxiv.org/abs/2509.19098v1"
    },
    {
      "title": "A Generative Conditional Distribution Equality Testing Framework and Its\n  Minimax Analysis",
      "authors": [
        "Siming Zheng",
        "Meifang Lan",
        "Tong Wang",
        "Yuanyuan Lin"
      ],
      "abstract": "In this paper, we propose a general framework for testing the equality of the\nconditional distributions in a two-sample problem. This problem is most\nrelevant to transfer learning under covariate shift. Our framework is built on\nneural network-based generative methods and sample splitting techniques by\ntransforming the conditional distribution testing problem into an unconditional\none. We introduce two special tests: the generative permutation-based\nconditional distribution equality test and the generative classification\naccuracy-based conditional distribution equality test. Theoretically, we\nestablish a minimax lower bound for statistical inference in testing the\nequality of two conditional distributions under certain smoothness conditions.\nWe demonstrate that the generative permutation-based conditional distribution\nequality test and its modified version can attain this lower bound precisely or\nup to some iterated logarithmic factor. Moreover, we prove the testing\nconsistency of the generative classification accuracy-based conditional\ndistribution equality test. We also establish the convergence rate for the\nlearned conditional generator by deriving new results related to the\nrecently-developed offset Rademacher complexity and approximation properties\nusing neural networks. Empirically, we conduct numerical studies including\nsynthetic datasets and two real-world datasets, demonstrating the effectiveness\nof our approach.",
      "published": "September 22, 2025",
      "categories": [
        "cs.LG",
        "math.ST",
        "stat.ME",
        "stat.TH"
      ],
      "pdf_link": "http://arxiv.org/pdf/2509.17729v2",
      "arxiv_url": "http://arxiv.org/abs/2509.17729v2"
    },
    {
      "title": "What is a good matching of probability measures? A counterfactual lens\n  on transport maps",
      "authors": [
        "Lucas De Lara",
        "Luca Ganassali"
      ],
      "abstract": "Coupling probability measures lies at the core of many problems in statistics\nand machine learning, from domain adaptation to transfer learning and causal\ninference. Yet, even when restricted to deterministic transports, such\ncouplings are not identifiable: two atomless marginals admit infinitely many\ntransport maps. The common recourse to optimal transport, motivated by cost\nminimization and cyclical monotonicity, obscures the fact that several distinct\nnotions of multivariate monotone matchings coexist. In this work, we first\ncarry a comparative analysis of three constructions of transport maps:\ncyclically monotone, quantile-preserving and triangular monotone maps. We\nestablish necessary and sufficient conditions for their equivalence, thereby\nclarifying their respective structural properties. In parallel, we formulate\ncounterfactual reasoning within the framework of structural causal models as a\nproblem of selecting transport maps between fixed marginals, which makes\nexplicit the role of untestable assumptions in counterfactual reasoning. Then,\nwe are able to connect these two perspectives by identifying conditions on\ncausal graphs and structural equations under which counterfactual maps coincide\nwith classical statistical transports. In this way, we delineate the\ncircumstances in which causal assumptions support the use of a specific\nstructure of transport map. Taken together, our results aim to enrich the\ntheoretical understanding of families of transport maps and to clarify their\npossible causal interpretations. We hope this work contributes to establishing\nnew bridges between statistical transport and causal inference.",
      "published": "September 19, 2025",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.ST",
        "stat.ME",
        "stat.TH"
      ],
      "pdf_link": "http://arxiv.org/pdf/2509.16027v1",
      "arxiv_url": "http://arxiv.org/abs/2509.16027v1"
    },
    {
      "title": "Transfer learning under latent space model",
      "authors": [
        "Kuangnan Fang",
        "Ruixuan Qin",
        "Xinyan Fan"
      ],
      "abstract": "Latent space model plays a crucial role in network analysis, and accurate\nestimation of latent variables is essential for downstream tasks such as link\nprediction. However, the large number of parameters to be estimated presents a\nchallenge, especially when the latent space dimension is not exceptionally\nsmall. In this paper, we propose a transfer learning method that leverages\ninformation from networks with latent variables similar to those in the target\nnetwork, thereby improving the estimation accuracy for the target. Given\ntransferable source networks, we introduce a two-stage transfer learning\nalgorithm that accommodates differences in node numbers between source and\ntarget networks. In each stage, we derive sufficient identification conditions\nand design tailored projected gradient descent algorithms for estimation.\nTheoretical properties of the resulting estimators are established. When the\ntransferable networks are unknown, a detection algorithm is introduced to\nidentify suitable source networks. Simulation studies and analyses of two real\ndatasets demonstrate the effectiveness of the proposed methods.",
      "published": "September 19, 2025",
      "categories": [
        "stat.ME",
        "stat.ML"
      ],
      "pdf_link": "http://arxiv.org/pdf/2509.15797v1",
      "arxiv_url": "http://arxiv.org/abs/2509.15797v1"
    },
    {
      "title": "SETrLUSI: Stochastic Ensemble Multi-Source Transfer Learning Using\n  Statistical Invariant",
      "authors": [
        "Chunna Li",
        "Yiwei Song",
        "Yuanhai Shao"
      ],
      "abstract": "In transfer learning, a source domain often carries diverse knowledge, and\ndifferent domains usually emphasize different types of knowledge. Different\nfrom handling only a single type of knowledge from all domains in traditional\ntransfer learning methods, we introduce an ensemble learning framework with a\nweak mode of convergence in the form of Statistical Invariant (SI) for\nmulti-source transfer learning, formulated as Stochastic Ensemble Multi-Source\nTransfer Learning Using Statistical Invariant (SETrLUSI). The proposed SI\nextracts and integrates various types of knowledge from both source and target\ndomains, which not only effectively utilizes diverse knowledge but also\naccelerates the convergence process. Further, SETrLUSI incorporates stochastic\nSI selection, proportional source domain sampling, and target domain\nbootstrapping, which improves training efficiency while enhancing model\nstability. Experiments show that SETrLUSI has good convergence and outperforms\nrelated methods with a lower time cost.",
      "published": "September 19, 2025",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "pdf_link": "http://arxiv.org/pdf/2509.15593v1",
      "arxiv_url": "http://arxiv.org/abs/2509.15593v1"
    },
    {
      "title": "Representation-Aware Distributionally Robust Optimization: A Knowledge\n  Transfer Framework",
      "authors": [
        "Zitao Wang",
        "Nian Si",
        "Molei Liu"
      ],
      "abstract": "We propose REpresentation-Aware Distributionally Robust Estimation (READ), a\nnovel framework for Wasserstein distributionally robust learning that accounts\nfor predictive representations when guarding against distributional shifts.\nUnlike classical approaches that treat all feature perturbations equally, READ\nembeds a multidimensional alignment parameter into the transport cost, allowing\nthe model to differentially discourage perturbations along directions\nassociated with informative representations. This yields robustness to feature\nvariation while preserving invariant structure. Our first contribution is a\ntheoretical foundation: we show that seminorm regularizations for linear\nregression and binary classification arise as Wasserstein distributionally\nrobust objectives, thereby providing tractable reformulations of READ and\nunifying a broad class of regularized estimators under the DRO lens. Second, we\nadopt a principled procedure for selecting the Wasserstein radius using the\ntechniques of robust Wasserstein profile inference. This further enables the\nconstruction of valid, representation-aware confidence regions for model\nparameters with distinct geometric features. Finally, we analyze the geometry\nof READ estimators as the alignment parameters vary and propose an optimization\nalgorithm to estimate the projection of the global optimum onto this solution\nsurface. This procedure selects among equally robust estimators while optimally\nconstructing a representation structure. We conclude by demonstrating the\neffectiveness of our framework through extensive simulations and a real-world\nstudy, providing a powerful robust estimation grounded in learning\nrepresentation.",
      "published": "September 11, 2025",
      "categories": [
        "stat.ME",
        "cs.LG"
      ],
      "pdf_link": "http://arxiv.org/pdf/2509.09371v1",
      "arxiv_url": "http://arxiv.org/abs/2509.09371v1"
    }
  ]
}