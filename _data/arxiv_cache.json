{
  "timestamp": 1744247979,
  "papers": [
    {
      "title": "Sparse Optimization for Transfer Learning: A L0-Regularized Framework\n  for Multi-Source Domain Adaptation",
      "authors": [
        "Chenqi Gong",
        "Hu Yang"
      ],
      "abstract": "This paper explores transfer learning in heterogeneous multi-source\nenvironments with distributional divergence between target and auxiliary\ndomains. To address challenges in statistical bias and computational\nefficiency, we propose a Sparse Optimization for Transfer Learning (SOTL)\nframework based on L0-regularization. The method extends the Joint Estimation\nTransferred from Strata (JETS) paradigm with two key innovations: (1)\nL0-constrained exact sparsity for parameter space compression and complexity\nreduction, and (2) refining optimization focus to emphasize target parameters\nover redundant ones. Simulations show that SOTL significantly improves both\nestimation accuracy and computational speed, especially under adversarial\nauxiliary domain conditions. Empirical validation on the Community and Crime\nbenchmarks demonstrates the statistical robustness of the SOTL method in\ncross-domain transfer.",
      "published": "April 07, 2025",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "pdf_link": "http://arxiv.org/pdf/2504.04812v1",
      "arxiv_url": "http://arxiv.org/abs/2504.04812v1"
    },
    {
      "title": "Block Toeplitz Sparse Precision Matrix Estimation for Large-Scale\n  Interval-Valued Time Series Forecasting",
      "authors": [
        "Wan Tian",
        "Zhongfeng Qin"
      ],
      "abstract": "Modeling and forecasting interval-valued time series (ITS) have attracted\nconsiderable attention due to their growing presence in various contexts. To\nthe best of our knowledge, there have been no efforts to model large-scale ITS.\nIn this paper, we propose a feature extraction procedure for large-scale ITS,\nwhich involves key steps such as auto-segmentation and clustering, and feature\ntransfer learning. This procedure can be seamlessly integrated with any\nsuitable prediction models for forecasting purposes. Specifically, we transform\nthe automatic segmentation and clustering of ITS into the estimation of\nToeplitz sparse precision matrices and assignment set. The\nmajorization-minimization algorithm is employed to convert this highly\nnon-convex optimization problem into two subproblems. We derive efficient\ndynamic programming and alternating direction method to solve these two\nsubproblems alternately and establish their convergence properties. By\nemploying the Joint Recurrence Plot (JRP) to image subsequence and assigning a\nclass label to each cluster, an image dataset is constructed. Then, an\nappropriate neural network is chosen to train on this image dataset and used to\nextract features for the next step of forecasting. Real data applications\ndemonstrate that the proposed method can effectively obtain invariant\nrepresentations of the raw data and enhance forecasting performance.",
      "published": "April 04, 2025",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "pdf_link": "http://arxiv.org/pdf/2504.03322v1",
      "arxiv_url": "http://arxiv.org/abs/2504.03322v1"
    },
    {
      "title": "A model-free feature extraction procedure for interval-valued time\n  series prediction",
      "authors": [
        "Wan Tian",
        "Zhongfeng Qin",
        "Tao Hu"
      ],
      "abstract": "In this paper, we present a novel feature extraction procedure to predict\ninterval-valued time series by combing transfer learning and imaging\napproaches. Initially, we represent interval-valued time series using a\nbivariate point-valued time series, which serves as a representative form. We\nfirst transform each time series into images by employing various imaging\napproaches such as recurrence plot, gramian angular summation/difference field,\nand Markov transition field, and construct an image dataset by treating each\nimaging method's output as a separate class. Based on this dataset, we train\nseveral candidates for a feature extraction network (FEN), specifically ResNet\nwith varying layers. Then we choose the penultimate layer of the FEN to extract\nthe most relevant features from the transformed images. We integrate the\nextracted features into conventional predictive models to formulate the\ncorresponding prediction models. To formulate prediction, we integrate the\nextracted features into a regular prediction model. The proposed methods are\nevaluated based on the S\\&P 500 index and three data-generating processes\n(DGPs), and the experimental results demonstrate a notable improvement in\nprediction performance compared to existing methods.",
      "published": "April 04, 2025",
      "categories": [
        "stat.AP"
      ],
      "pdf_link": "http://arxiv.org/pdf/2504.03310v1",
      "arxiv_url": "http://arxiv.org/abs/2504.03310v1"
    },
    {
      "title": "Privacy-Preserving Transfer Learning for Community Detection using\n  Locally Distributed Multiple Networks",
      "authors": [
        "Xiao Guo",
        "Xuming He",
        "Xiangyu Chang",
        "Shujie Ma"
      ],
      "abstract": "This paper develops a new spectral clustering-based method called TransNet\nfor transfer learning in community detection of network data. Our goal is to\nimprove the clustering performance of the target network using auxiliary source\nnetworks, which are heterogeneous, privacy-preserved, and locally stored across\nvarious sources. The edges of each locally stored network are perturbed using\nthe randomized response mechanism to achieve differential privacy. Notably, we\nallow the source networks to have distinct privacy-preserving and heterogeneity\nlevels as often desired in practice. To better utilize the information from the\nsource networks, we propose a novel adaptive weighting method to aggregate the\neigenspaces of the source networks multiplied by adaptive weights chosen to\nincorporate the effects of privacy and heterogeneity. We propose a\nregularization method that combines the weighted average eigenspace of the\nsource networks with the eigenspace of the target network to achieve an optimal\nbalance between them. Theoretically, we show that the adaptive weighting method\nenjoys the error-bound-oracle property in the sense that the error bound of the\nestimated eigenspace only depends on informative source networks. We also\ndemonstrate that TransNet performs better than the estimator using only the\ntarget network and the estimator using only the weighted source networks.",
      "published": "April 01, 2025",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "pdf_link": "http://arxiv.org/pdf/2504.00890v1",
      "arxiv_url": "http://arxiv.org/abs/2504.00890v1"
    },
    {
      "title": "Nested Stochastic Gradient Descent for (Generalized) Sinkhorn\n  Distance-Regularized Distributionally Robust Optimization",
      "authors": [
        "Yufeng Yang",
        "Yi Zhou",
        "Zhaosong Lu"
      ],
      "abstract": "Distributionally robust optimization (DRO) is a powerful technique to train\nrobust models against data distribution shift. This paper aims to solve\nregularized nonconvex DRO problems, where the uncertainty set is modeled by a\nso-called generalized Sinkhorn distance and the loss function is nonconvex and\npossibly unbounded. Such a distance allows to model uncertainty of\ndistributions with different probability supports and divergence functions. For\nthis class of regularized DRO problems, we derive a novel dual formulation\ntaking the form of nested stochastic programming, where the dual variable\ndepends on the data sample. To solve the dual problem, we provide theoretical\nevidence to design a nested stochastic gradient descent (SGD) algorithm, which\nleverages stochastic approximation to estimate the nested stochastic gradients.\nWe study the convergence rate of nested SGD and establish polynomial iteration\nand sample complexities that are independent of the data size and parameter\ndimension, indicating its potential for solving large-scale DRO problems. We\nconduct numerical experiments to demonstrate the efficiency and robustness of\nthe proposed algorithm.",
      "published": "March 29, 2025",
      "categories": [
        "math.OC",
        "cs.LG",
        "stat.ML"
      ],
      "pdf_link": "http://arxiv.org/pdf/2503.22923v1",
      "arxiv_url": "http://arxiv.org/abs/2503.22923v1"
    },
    {
      "title": "Robust Mean Estimation for Optimization: The Impact of Heavy Tails",
      "authors": [
        "Bart P. G. van Parys",
        "Bert Zwart"
      ],
      "abstract": "We consider the problem of constructing a least conservative estimator of the\nexpected value $\\mu$ of a non-negative heavy-tailed random variable. We require\nthat the probability of overestimating the expected value $\\mu$ is kept\nappropriately small; a natural requirement if its subsequent use in a decision\nprocess is anticipated. In this setting, we show it is optimal to estimate\n$\\mu$ by solving a distributionally robust optimization (DRO) problem using the\nKullback-Leibler (KL) divergence. We further show that the statistical\nproperties of KL-DRO compare favorably with other estimators based on\ntruncation, variance regularization, or Wasserstein DRO.",
      "published": "March 27, 2025",
      "categories": [
        "math.OC",
        "math.PR",
        "math.ST",
        "stat.TH",
        "60F10, 62G35, 90C17"
      ],
      "pdf_link": "http://arxiv.org/pdf/2503.21421v1",
      "arxiv_url": "http://arxiv.org/abs/2503.21421v1"
    }
  ]
}