{
  "timestamp": 1759886284,
  "papers": [
    {
      "title": "Busemann Functions in the Wasserstein Space: Existence, Closed-Forms,\n  and Applications to Slicing",
      "authors": [
        "Cl\u00e9ment Bonet",
        "Elsa Cazelles",
        "Lucas Drumetz",
        "Nicolas Courty"
      ],
      "abstract": "The Busemann function has recently found much interest in a variety of\ngeometric machine learning problems, as it naturally defines projections onto\ngeodesic rays of Riemannian manifolds and generalizes the notion of\nhyperplanes. As several sources of data can be conveniently modeled as\nprobability distributions, it is natural to study this function in the\nWasserstein space, which carries a rich formal Riemannian structure induced by\nOptimal Transport metrics. In this work, we investigate the existence and\ncomputation of Busemann functions in Wasserstein space, which admits geodesic\nrays. We establish closed-form expressions in two important cases:\none-dimensional distributions and Gaussian measures. These results enable\nexplicit projection schemes for probability distributions on $\\mathbb{R}$,\nwhich in turn allow us to define novel Sliced-Wasserstein distances over\nGaussian mixtures and labeled datasets. We demonstrate the efficiency of those\noriginal schemes on synthetic datasets as well as transfer learning problems.",
      "published": "October 06, 2025",
      "categories": [
        "cs.LG",
        "math.MG",
        "stat.ML"
      ],
      "pdf_link": "http://arxiv.org/pdf/2510.04579v1",
      "arxiv_url": "http://arxiv.org/abs/2510.04579v1"
    },
    {
      "title": "Optimal Regularization Under Uncertainty: Distributional Robustness and\n  Convexity Constraints",
      "authors": [
        "Oscar Leong",
        "Eliza O'Reilly",
        "Yong Sheng Soh"
      ],
      "abstract": "Regularization is a central tool for addressing ill-posedness in inverse\nproblems and statistical estimation, with the choice of a suitable penalty\noften determining the reliability and interpretability of downstream solutions.\nWhile recent work has characterized optimal regularizers for well-specified\ndata distributions, practical deployments are often complicated by\ndistributional uncertainty and the need to enforce structural constraints such\nas convexity. In this paper, we introduce a framework for distributionally\nrobust optimal regularization, which identifies regularizers that remain\neffective under perturbations of the data distribution. Our approach leverages\nconvex duality to reformulate the underlying distributionally robust\noptimization problem, eliminating the inner maximization and yielding\nformulations that are amenable to numerical computation. We show how the\nresulting robust regularizers interpolate between memorization of the training\ndistribution and uniform priors, providing insights into their behavior as\nrobustness parameters vary. For example, we show how certain ambiguity sets,\nsuch as those based on the Wasserstein-1 distance, naturally induce regularity\nin the optimal regularizer by promoting regularizers with smaller Lipschitz\nconstants. We further investigate the setting where regularizers are required\nto be convex, formulating a convex program for their computation and\nillustrating their stability with respect to distributional shifts. Taken\ntogether, our results provide both theoretical and computational foundations\nfor designing regularizers that are reliable under model uncertainty and\nstructurally constrained for robust deployment.",
      "published": "October 03, 2025",
      "categories": [
        "math.OC",
        "math.MG",
        "math.ST",
        "stat.ML",
        "stat.TH"
      ],
      "pdf_link": "http://arxiv.org/pdf/2510.03464v1",
      "arxiv_url": "http://arxiv.org/abs/2510.03464v1"
    },
    {
      "title": "Bayesian Transfer Learning for High-Dimensional Linear Regression via\n  Adaptive Shrinkage",
      "authors": [
        "Parsa Jamshidian",
        "Donatello Telesca"
      ],
      "abstract": "We introduce BLAST, Bayesian Linear regression with Adaptive Shrinkage for\nTransfer, a Bayesian multi-source transfer learning framework for\nhigh-dimensional linear regression. The proposed analytical framework leverages\nglobal-local shrinkage priors together with Bayesian source selection to\nbalance information sharing and regularization. We show how Bayesian source\nselection allows for the extraction of the most useful data sources, while\ndiscounting biasing information that may lead to negative transfer. In this\nframework, both source selection and sparse regression are jointly accounted\nfor in prediction and inference via Bayesian model averaging. The structure of\nour model admits efficient posterior simulation via a Gibbs sampling algorithm\nallowing full posterior inference for the target regression coefficients,\nmaking BLAST both computationally practical and inferentially straightforward.\nOur method achieves more accurate posterior inference for the target than\nregularization approaches based on target data alone, while offering\ncompetitive predictive performance and superior uncertainty quantification\ncompared to current state-of-the-art transfer learning methods. We validate its\neffectiveness through extensive simulation studies and illustrate its\nanalytical properties when applied to a case study on the estimation of tumor\nmutational burden from gene expression, using data from The Cancer Genome Atlas\n(TCGA).",
      "published": "October 03, 2025",
      "categories": [
        "stat.ME",
        "stat.AP",
        "stat.CO"
      ],
      "pdf_link": "http://arxiv.org/pdf/2510.03449v1",
      "arxiv_url": "http://arxiv.org/abs/2510.03449v1"
    },
    {
      "title": "Exactly or Approximately Wasserstein Distributionally Robust Estimation\n  According to Wasserstein Radii Being Small or Large",
      "authors": [
        "Xiao Ding",
        "Enbin Song",
        "Dunbiao Niu",
        "Zhujun Cao",
        "Qingjiang Shi"
      ],
      "abstract": "This paper primarily considers the robust estimation problem under\nWasserstein distance constraints on the parameter and noise distributions in\nthe linear measurement model with additive noise, which can be formulated as an\ninfinite-dimensional nonconvex minimax problem. We prove that the existence of\na saddle point for this problem is equivalent to that for a finite-dimensional\nminimax problem, and give a counterexample demonstrating that the saddle point\nmay not exist. Motivated by this observation, we present a verifiable necessary\nand sufficient condition whose parameters can be derived from a convex problem\nand its dual. Additionally, we also introduce a simplified sufficient\ncondition, which intuitively indicates that when the Wasserstein radii are\nsmall enough, the saddle point always exists. In the absence of the saddle\npoint, we solve an finite-dimensional nonconvex minimax problem, obtained by\nrestricting the estimator to be linear. Its optimal value establishes an upper\nbound on the robust estimation problem, while its optimal solution yields a\nrobust linear estimator. Numerical experiments are also provided to validate\nour theoretical results.",
      "published": "October 02, 2025",
      "categories": [
        "eess.SP",
        "math.OC",
        "math.ST",
        "stat.TH"
      ],
      "pdf_link": "http://arxiv.org/pdf/2510.01763v2",
      "arxiv_url": "http://arxiv.org/abs/2510.01763v2"
    },
    {
      "title": "Deep Learning Approaches with Explainable AI for Differentiating\n  Alzheimer Disease and Mild Cognitive Impairment",
      "authors": [
        "Fahad Mostafa",
        "Kannon Hossain",
        "Hafiz Khan"
      ],
      "abstract": "Early and accurate diagnosis of Alzheimer Disease is critical for effective\nclinical intervention, particularly in distinguishing it from Mild Cognitive\nImpairment, a prodromal stage marked by subtle structural changes. In this\nstudy, we propose a hybrid deep learning ensemble framework for Alzheimer\nDisease classification using structural magnetic resonance imaging. Gray and\nwhite matter slices are used as inputs to three pretrained convolutional neural\nnetworks such as ResNet50, NASNet, and MobileNet, each fine tuned through an\nend to end process. To further enhance performance, we incorporate a stacked\nensemble learning strategy with a meta learner and weighted averaging to\noptimally combine the base models. Evaluated on the Alzheimer Disease\nNeuroimaging Initiative dataset, the proposed method achieves state of the art\naccuracy of 99.21% for Alzheimer Disease vs. Mild Cognitive Impairment and\n91.0% for Mild Cognitive Impairment vs. Normal Controls, outperforming\nconventional transfer learning and baseline ensemble methods. To improve\ninterpretability in image based diagnostics, we integrate Explainable AI\ntechniques by Gradient weighted Class Activation, which generates heatmaps and\nattribution maps that highlight critical regions in gray and white matter\nslices, revealing structural biomarkers that influence model decisions. These\nresults highlight the frameworks potential for robust and scalable clinical\ndecision support in neurodegenerative disease diagnostics.",
      "published": "September 27, 2025",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "stat.AP",
        "stat.ML"
      ],
      "pdf_link": "http://arxiv.org/pdf/2510.00048v2",
      "arxiv_url": "http://arxiv.org/abs/2510.00048v2"
    },
    {
      "title": "Conditional Risk Minimization with Side Information: A Tractable,\n  Universal Optimal Transport Framework",
      "authors": [
        "Xinqiao Xie",
        "Jonathan Yu-Meng Li"
      ],
      "abstract": "Conditional risk minimization arises in high-stakes decisions where risk must\nbe assessed in light of side information, such as stressed economic conditions,\nspecific customer profiles, or other contextual covariates. Constructing\nreliable conditional distributions from limited data is notoriously difficult,\nmotivating a series of optimal-transport-based proposals that address this\nuncertainty in a distributionally robust manner. Yet these approaches remain\nfragmented, each constrained by its own limitations: some rely on point\nestimates or restrictive structural assumptions, others apply only to narrow\nclasses of risk measures, and their structural connections are unclear. We\nintroduce a universal framework for distributionally robust conditional risk\nminimization, built on a novel union-ball formulation in optimal transport.\nThis framework offers three key advantages: interpretability, by subsuming\nexisting methods as special cases and revealing their deep structural links;\ntractability, by yielding convex reformulations for virtually all major risk\nfunctionals studied in the literature; and scalability, by supporting\ncutting-plane algorithms for large-scale conditional risk problems.\nApplications to portfolio optimization with rank-dependent expected utility\nhighlight the practical effectiveness of the framework, with conditional models\nconverging to optimal solutions where unconditional ones clearly do not.",
      "published": "September 27, 2025",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.OC",
        "q-fin.PM",
        "q-fin.RM"
      ],
      "pdf_link": "http://arxiv.org/pdf/2509.23128v1",
      "arxiv_url": "http://arxiv.org/abs/2509.23128v1"
    },
    {
      "title": "Transfer Learning under Group-Label Shift: A Semiparametric Exponential\n  Tilting Approach",
      "authors": [
        "Manli Cheng",
        "Subha Maity",
        "Qinglong Tian",
        "Pengfei Li"
      ],
      "abstract": "We propose a new framework for binary classification in transfer learning\nsettings where both covariate and label distributions may shift between source\nand target domains. Unlike traditional covariate shift or label shift\nassumptions, we introduce a group-label shift assumption that accommodates\nsubpopulation imbalance and mitigates spurious correlations, thereby improving\nrobustness to real-world distributional changes. To model the joint\ndistribution difference, we adopt a flexible exponential tilting formulation\nand establish mild, verifiable identification conditions via an instrumental\nvariable strategy. We develop a computationally efficient two-step\nlikelihood-based estimation procedure that combines logistic regression for the\nsource outcome model with conditional likelihood estimation using both source\nand target covariates. We derive consistency and asymptotic normality for the\nresulting estimators, and extend the theory to receiver operating\ncharacteristic curves, the area under the curve, and other target functionals,\naddressing the nonstandard challenges posed by plug-in classifiers. Simulation\nstudies demonstrate that our method outperforms existing alternatives under\nsubpopulation shift scenarios. A semi-synthetic application using the\nwaterbirds dataset further confirms the proposed method's ability to transfer\ninformation effectively and improve target-domain classification accuracy.",
      "published": "September 26, 2025",
      "categories": [
        "stat.ME",
        "math.ST",
        "stat.TH"
      ],
      "pdf_link": "http://arxiv.org/pdf/2509.22268v1",
      "arxiv_url": "http://arxiv.org/abs/2509.22268v1"
    }
  ]
}