{
  "timestamp": 1755481374,
  "papers": [
    {
      "title": "Empirical Bayes for Data Integration",
      "authors": [
        "Paul Rognon-Vael",
        "David Rossell"
      ],
      "abstract": "We discuss the use of empirical Bayes for data integration, in the sense of\ntransfer learning. Our main interest is in settings where one wishes to learn\nstructure (e.g. feature selection) and one only has access to incomplete data\nfrom previous studies, such as summaries, estimates or lists of relevant\nfeatures. We discuss differences between full Bayes and empirical Bayes, and\ndevelop a computational framework for the latter. We discuss how empirical\nBayes attains consistent variable selection under weaker conditions (sparsity\nand betamin assumptions) than full Bayes and other standard criteria do, and\nhow it attains faster convergence rates. Our high-dimensional regression\nexamples show that fully Bayesian inference enjoys excellent properties, and\nthat data integration with empirical Bayes can offer moderate yet meaningful\nimprovements in practice.",
      "published": "August 10, 2025",
      "categories": [
        "stat.ME"
      ],
      "pdf_link": "http://arxiv.org/pdf/2508.08336v1",
      "arxiv_url": "http://arxiv.org/abs/2508.08336v1"
    },
    {
      "title": "Model Recycling Framework for Multi-Source Data-Free Supervised Transfer\n  Learning",
      "authors": [
        "Sijia Wang",
        "Ricardo Henao"
      ],
      "abstract": "Increasing concerns for data privacy and other difficulties associated with\nretrieving source data for model training have created the need for source-free\ntransfer learning, in which one only has access to pre-trained models instead\nof data from the original source domains. This setting introduces many\nchallenges, as many existing transfer learning methods typically rely on access\nto source data, which limits their direct applicability to scenarios where\nsource data is unavailable. Further, practical concerns make it more difficult,\nfor instance efficiently selecting models for transfer without information on\nsource data, and transferring without full access to the source models. So\nmotivated, we propose a model recycling framework for parameter-efficient\ntraining of models that identifies subsets of related source models to reuse in\nboth white-box and black-box settings. Consequently, our framework makes it\npossible for Model as a Service (MaaS) providers to build libraries of\nefficient pre-trained models, thus creating an opportunity for multi-source\ndata-free supervised transfer learning.",
      "published": "August 04, 2025",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_link": "http://arxiv.org/pdf/2508.02039v1",
      "arxiv_url": "http://arxiv.org/abs/2508.02039v1"
    }
  ]
}