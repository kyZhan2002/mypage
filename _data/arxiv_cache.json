{
  "timestamp": 1741223808,
  "papers": [
    {
      "title": "The Distributionally Robust Optimization Model of Sparse Principal\n  Component Analysis",
      "authors": [
        "Lei Wang",
        "Xin Liu",
        "Xiaojun Chen"
      ],
      "abstract": "We consider sparse principal component analysis (PCA) under a stochastic\nsetting where the underlying probability distribution of the random parameter\nis uncertain. This problem is formulated as a distributionally robust\noptimization (DRO) model based on a constructive approach to capturing\nuncertainty in the covariance matrix, which constitutes a nonsmooth constrained\nmin-max optimization problem. We further prove that the inner maximization\nproblem admits a closed-form solution, reformulating the original DRO model\ninto an equivalent minimization problem on the Stiefel manifold. This\ntransformation leads to a Riemannian optimization problem with intricate\nnonsmooth terms, a challenging formulation beyond the reach of existing\nalgorithms. To address this issue, we devise an efficient smoothing manifold\nproximal gradient algorithm. We prove the Riemannian gradient consistency and\nglobal convergence of our algorithm to a stationary point of the nonsmooth\nminimization problem. Moreover, we establish the iteration complexity of our\nalgorithm. Finally, numerical experiments are conducted to validate the\neffectiveness and scalability of our algorithm, as well as to highlight the\nnecessity and rationality of adopting the DRO model for sparse PCA.",
      "published": "March 04, 2025",
      "categories": [
        "math.OC",
        "cs.LG",
        "stat.ML"
      ],
      "pdf_link": "http://arxiv.org/pdf/2503.02494v1",
      "arxiv_url": "http://arxiv.org/abs/2503.02494v1"
    },
    {
      "title": "Optimal Transfer Learning for Missing Not-at-Random Matrix Completion",
      "authors": [
        "Akhil Jalan",
        "Yassir Jedra",
        "Arya Mazumdar",
        "Soumendu Sundar Mukherjee",
        "Purnamrita Sarkar"
      ],
      "abstract": "We study transfer learning for matrix completion in a Missing Not-at-Random\n(MNAR) setting that is motivated by biological problems. The target matrix $Q$\nhas entire rows and columns missing, making estimation impossible without side\ninformation. To address this, we use a noisy and incomplete source matrix $P$,\nwhich relates to $Q$ via a feature shift in latent space. We consider both the\nactive and passive sampling of rows and columns. We establish minimax lower\nbounds for entrywise estimation error in each setting. Our computationally\nefficient estimation framework achieves this lower bound for the active\nsetting, which leverages the source data to query the most informative rows and\ncolumns of $Q$. This avoids the need for incoherence assumptions required for\nrate optimality in the passive sampling setting. We demonstrate the\neffectiveness of our approach through comparisons with existing algorithms on\nreal-world biological datasets.",
      "published": "February 28, 2025",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_link": "http://arxiv.org/pdf/2503.00174v1",
      "arxiv_url": "http://arxiv.org/abs/2503.00174v1"
    },
    {
      "title": "A Principled Approach to Bayesian Transfer Learning",
      "authors": [
        "Adam Bretherton",
        "Joshua J. Bon",
        "David J. Warne",
        "Kerrie Mengersen",
        "Christopher Drovandi"
      ],
      "abstract": "Updating $\\textit{a priori}$ information given some observed data is the core\ntenet of Bayesian inference. Bayesian transfer learning extends this idea by\nincorporating information from a related dataset to improve the inference on\nthe observed data which may have been collected under slightly different\nsettings. The use of related information can be useful when the observed data\nis scarce, for example. Current Bayesian transfer learning methods that are\nbased on the so-called $\\textit{power prior}$ can adaptively transfer\ninformation from related data. Unfortunately, it is not always clear under\nwhich scenario Bayesian transfer learning performs best or even if it will\nimprove Bayesian inference. Additionally, current power prior methods rely on\nconjugacy to evaluate the posterior of interest. We propose using leave-one-out\ncross validation on the target dataset as a means of evaluating Bayesian\ntransfer learning methods. Further, we introduce a new framework,\n$\\textit{transfer sequential Monte Carlo}$, for power prior approaches that\nefficiently chooses the transfer parameter while avoiding the need for\nconjugate priors. We assess the performance of our proposed methods in two\ncomprehensive simulation studies.",
      "published": "February 27, 2025",
      "categories": [
        "stat.ME",
        "stat.CO",
        "62F15, 62F07 (Primary) 62-08 (Secondary)"
      ],
      "pdf_link": "http://arxiv.org/pdf/2502.19796v1",
      "arxiv_url": "http://arxiv.org/abs/2502.19796v1"
    },
    {
      "title": "Conformal Prediction Under Generalized Covariate Shift with Posterior\n  Drift",
      "authors": [
        "Baozhen Wang",
        "Xingye Qiao"
      ],
      "abstract": "In many real applications of statistical learning, collecting sufficiently\nmany training data is often expensive, time-consuming, or even unrealistic. In\nthis case, a transfer learning approach, which aims to leverage knowledge from\na related source domain to improve the learning performance in the target\ndomain, is more beneficial. There have been many transfer learning methods\ndeveloped under various distributional assumptions. In this article, we study a\nparticular type of classification problem, called conformal prediction, under a\nnew distributional assumption for transfer learning. Classifiers under the\nconformal prediction framework predict a set of plausible labels instead of one\nsingle label for each data instance, affording a more cautious and safer\ndecision. We consider a generalization of the \\textit{covariate shift with\nposterior drift} setting for transfer learning. Under this setting, we propose\na weighted conformal classifier that leverages both the source and target\nsamples, with a coverage guarantee in the target domain. Theoretical studies\ndemonstrate favorable asymptotic properties. Numerical studies further\nillustrate the usefulness of the proposed method.",
      "published": "February 25, 2025",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.ST",
        "stat.TH"
      ],
      "pdf_link": "http://arxiv.org/pdf/2502.17744v1",
      "arxiv_url": "http://arxiv.org/abs/2502.17744v1"
    },
    {
      "title": "Distributionally Robust Active Learning for Gaussian Process Regression",
      "authors": [
        "Shion Takeno",
        "Yoshito Okura",
        "Yu Inatsu",
        "Aoyama Tatsuya",
        "Tomonari Tanaka",
        "Akahane Satoshi",
        "Hiroyuki Hanada",
        "Noriaki Hashimoto",
        "Taro Murayama",
        "Hanju Lee",
        "Shinya Kojima",
        "Ichiro Takeuchi"
      ],
      "abstract": "Gaussian process regression (GPR) or kernel ridge regression is a widely used\nand powerful tool for nonlinear prediction. Therefore, active learning (AL) for\nGPR, which actively collects data labels to achieve an accurate prediction with\nfewer data labels, is an important problem. However, existing AL methods do not\ntheoretically guarantee prediction accuracy for target distribution.\nFurthermore, as discussed in the distributionally robust learning literature,\nspecifying the target distribution is often difficult. Thus, this paper\nproposes two AL methods that effectively reduce the worst-case expected error\nfor GPR, which is the worst-case expectation in target distribution candidates.\nWe show an upper bound of the worst-case expected squared error, which suggests\nthat the error will be arbitrarily small by a finite number of data labels\nunder mild conditions. Finally, we demonstrate the effectiveness of the\nproposed methods through synthetic and real-world datasets.",
      "published": "February 24, 2025",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_link": "http://arxiv.org/pdf/2502.16870v1",
      "arxiv_url": "http://arxiv.org/abs/2502.16870v1"
    },
    {
      "title": "Provable Benefits of Unsupervised Pre-training and Transfer Learning via\n  Single-Index Models",
      "authors": [
        "Taj Jones-McCormick",
        "Aukosh Jagannath",
        "Subhabrata Sen"
      ],
      "abstract": "Unsupervised pre-training and transfer learning are commonly used techniques\nto initialize training algorithms for neural networks, particularly in settings\nwith limited labeled data. In this paper, we study the effects of unsupervised\npre-training and transfer learning on the sample complexity of high-dimensional\nsupervised learning. Specifically, we consider the problem of training a\nsingle-layer neural network via online stochastic gradient descent. We\nestablish that pre-training and transfer learning (under concept shift) reduce\nsample complexity by polynomial factors (in the dimension) under very general\nassumptions. We also uncover some surprising settings where pre-training grants\nexponential improvement over random initialization in terms of sample\ncomplexity.",
      "published": "February 24, 2025",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "pdf_link": "http://arxiv.org/pdf/2502.16849v1",
      "arxiv_url": "http://arxiv.org/abs/2502.16849v1"
    },
    {
      "title": "Transfer Learning through Enhanced Sufficient Representation: Enriching\n  Source Domain Knowledge with Target Data",
      "authors": [
        "Yeheng Ge",
        "Xueyu Zhou",
        "Jian Huang"
      ],
      "abstract": "Transfer learning is an important approach for addressing the challenges\nposed by limited data availability in various applications. It accomplishes\nthis by transferring knowledge from well-established source domains to a less\nfamiliar target domain. However, traditional transfer learning methods often\nface difficulties due to rigid model assumptions and the need for a high degree\nof similarity between source and target domain models. In this paper, we\nintroduce a novel method for transfer learning called Transfer learning through\nEnhanced Sufficient Representation (TESR). Our approach begins by estimating a\nsufficient and invariant representation from the source domains. This\nrepresentation is then enhanced with an independent component derived from the\ntarget data, ensuring that it is sufficient for the target domain and adaptable\nto its specific characteristics. A notable advantage of TESR is that it does\nnot rely on assuming similar model structures across different tasks. For\nexample, the source domain models can be regression models, while the target\ndomain task can be classification. This flexibility makes TESR applicable to a\nwide range of supervised learning problems. We explore the theoretical\nproperties of TESR and validate its performance through simulation studies and\nreal-world data applications, demonstrating its effectiveness in finite sample\nsettings.",
      "published": "February 22, 2025",
      "categories": [
        "stat.ML",
        "cs.LG",
        "62G05, 68T07"
      ],
      "pdf_link": "http://arxiv.org/pdf/2502.20414v1",
      "arxiv_url": "http://arxiv.org/abs/2502.20414v1"
    },
    {
      "title": "Improving variable selection properties by using external data",
      "authors": [
        "Paul Rognon-Vael",
        "David Rossell",
        "Piotr Zwiernik"
      ],
      "abstract": "Sparse high-dimensional signal recovery is only possible under certain\nconditions on the number of parameters, sample size, signal strength and\nunderlying sparsity. We show that these mathematical limits can be pushed when\none has external information that allow splitting parameters into blocks. This\noccurs in many applications, including data integration and transfer learning\ntasks. Specifically, we consider the Gaussian sequence model and linear\nregression, and show how block-based $\\ell_0$ penalties attain model selection\nconsistency under milder conditions than standard $\\ell_0$ penalties, and they\nalso attain faster model recovery rates. We first provide results for\noracle-based $\\ell_0$ penalties that have access to perfect sparsity and signal\nstrength information, and subsequently empirical Bayes-motivated block $\\ell_0$\npenalties that does not require oracle information.",
      "published": "February 21, 2025",
      "categories": [
        "math.ST",
        "stat.ME",
        "stat.TH",
        "62F07 (Primary) 62C12, 62R07 (Secondary)"
      ],
      "pdf_link": "http://arxiv.org/pdf/2502.15584v1",
      "arxiv_url": "http://arxiv.org/abs/2502.15584v1"
    },
    {
      "title": "Distribution Matching for Self-Supervised Transfer Learning",
      "authors": [
        "Yuling Jiao",
        "Wensen Ma",
        "Defeng Sun",
        "Hansheng Wang",
        "Yang Wang"
      ],
      "abstract": "In this paper, we propose a novel self-supervised transfer learning method\ncalled Distribution Matching (DM), which drives the representation distribution\ntoward a predefined reference distribution while preserving augmentation\ninvariance. The design of DM results in a learned representation space that is\nintuitively structured and offers easily interpretable hyperparameters.\nExperimental results across multiple real-world datasets and evaluation metrics\ndemonstrate that DM performs competitively on target classification tasks\ncompared to existing self-supervised transfer learning methods. Additionally,\nwe provide robust theoretical guarantees for DM, including a population theorem\nand an end-to-end sample theorem. The population theorem bridges the gap\nbetween the self-supervised learning task and target classification accuracy,\nwhile the sample theorem shows that, even with a limited number of samples from\nthe target domain, DM can deliver exceptional classification performance,\nprovided the unlabeled sample size is sufficiently large.",
      "published": "February 20, 2025",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG",
        "stat.ME"
      ],
      "pdf_link": "http://arxiv.org/pdf/2502.14424v1",
      "arxiv_url": "http://arxiv.org/abs/2502.14424v1"
    }
  ]
}