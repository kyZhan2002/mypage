{
  "timestamp": 1746840057,
  "papers": [
    {
      "title": "Multi-modal cascade feature transfer for polymer property prediction",
      "authors": [
        "Kiichi Obuchi",
        "Yuta Yahagi",
        "Kiyohiko Toyama",
        "Shukichi Tanaka",
        "Kota Matsui"
      ],
      "abstract": "In this paper, we propose a novel transfer learning approach called\nmulti-modal cascade model with feature transfer for polymer property\nprediction.Polymers are characterized by a composite of data in several\ndifferent formats, including molecular descriptors and additive information as\nwell as chemical structures. However, in conventional approaches, prediction\nmodels were often constructed using each type of data separately. Our model\nenables more accurate prediction of physical properties for polymers by\ncombining features extracted from the chemical structure by graph convolutional\nneural networks (GCN) with features such as molecular descriptors and additive\ninformation. The predictive performance of the proposed method is empirically\nevaluated using several polymer datasets. We report that the proposed method\nshows high predictive performance compared to the baseline conventional\napproach using a single feature.",
      "published": "May 06, 2025",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "pdf_link": "http://arxiv.org/pdf/2505.03704v2",
      "arxiv_url": "http://arxiv.org/abs/2505.03704v2"
    },
    {
      "title": "Decision Making under Model Misspecification: DRO with Robust Bayesian\n  Ambiguity Sets",
      "authors": [
        "Charita Dellaporta",
        "Patrick O'Hara",
        "Theodoros Damoulas"
      ],
      "abstract": "Distributionally Robust Optimisation (DRO) protects risk-averse\ndecision-makers by considering the worst-case risk within an ambiguity set of\ndistributions based on the empirical distribution or a model. To further guard\nagainst finite, noisy data, model-based approaches admit Bayesian formulations\nthat propagate uncertainty from the posterior to the decision-making problem.\nHowever, when the model is misspecified, the decision maker must stretch the\nambiguity set to contain the data-generating process (DGP), leading to overly\nconservative decisions. We address this challenge by introducing DRO with\nRobust, to model misspecification, Bayesian Ambiguity Sets (DRO-RoBAS). These\nare Maximum Mean Discrepancy ambiguity sets centred at a robust posterior\npredictive distribution that incorporates beliefs about the DGP. We show that\nthe resulting optimisation problem obtains a dual formulation in the\nReproducing Kernel Hilbert Space and we give probabilistic guarantees on the\ntolerance level of the ambiguity set. Our method outperforms other Bayesian and\nempirical DRO approaches in out-of-sample performance on the Newsvendor and\nPortfolio problems with various cases of model misspecification.",
      "published": "May 06, 2025",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "pdf_link": "http://arxiv.org/pdf/2505.03585v1",
      "arxiv_url": "http://arxiv.org/abs/2505.03585v1"
    },
    {
      "title": "StablePCA: Learning Shared Representations across Multiple Sources via\n  Minimax Optimization",
      "authors": [
        "Zhenyu Wang",
        "Molei Liu",
        "Jing Lei",
        "Francis Bach",
        "Zijian Guo"
      ],
      "abstract": "When synthesizing multisource high-dimensional data, a key objective is to\nextract low-dimensional feature representations that effectively approximate\nthe original features across different sources. Such general feature extraction\nfacilitates the discovery of transferable knowledge, mitigates systematic\nbiases such as batch effects, and promotes fairness. In this paper, we propose\nStable Principal Component Analysis (StablePCA), a novel method for group\ndistributionally robust learning of latent representations from\nhigh-dimensional multi-source data. A primary challenge in generalizing PCA to\nthe multi-source regime lies in the nonconvexity of the fixed rank constraint,\nrendering the minimax optimization nonconvex. To address this challenge, we\nemploy the Fantope relaxation, reformulating the problem as a convex minimax\noptimization, with the objective defined as the maximum loss across sources. To\nsolve the relaxed formulation, we devise an optimistic-gradient Mirror Prox\nalgorithm with explicit closed-form updates. Theoretically, we establish the\nglobal convergence of the Mirror Prox algorithm, with the convergence rate\nprovided from the optimization perspective. Furthermore, we offer practical\ncriteria to assess how closely the solution approximates the original nonconvex\nformulation. Through extensive numerical experiments, we demonstrate\nStablePCA's high accuracy and efficiency in extracting robust low-dimensional\nrepresentations across various finite-sample scenarios.",
      "published": "May 02, 2025",
      "categories": [
        "cs.LG",
        "math.OC",
        "stat.CO",
        "stat.ME"
      ],
      "pdf_link": "http://arxiv.org/pdf/2505.00940v1",
      "arxiv_url": "http://arxiv.org/abs/2505.00940v1"
    },
    {
      "title": "AI-Assisted Decision-Making for Clinical Assessment of Auto-Segmented\n  Contour Quality",
      "authors": [
        "Biling Wang",
        "Austen Maniscalco",
        "Ti Bai",
        "Siqiu Wang",
        "Michael Dohopolski",
        "Mu-Han Lin",
        "Chenyang Shen",
        "Dan Nguyen",
        "Junzhou Huang",
        "Steve Jiang",
        "Xinlei Wang"
      ],
      "abstract": "Purpose: This study presents a Deep Learning (DL)-based quality assessment\n(QA) approach for evaluating auto-generated contours (auto-contours) in\nradiotherapy, with emphasis on Online Adaptive Radiotherapy (OART). Leveraging\nBayesian Ordinal Classification (BOC) and calibrated uncertainty thresholds,\nthe method enables confident QA predictions without relying on ground truth\ncontours or extensive manual labeling. Methods: We developed a BOC model to\nclassify auto-contour quality and quantify prediction uncertainty. A\ncalibration step was used to optimize uncertainty thresholds that meet clinical\naccuracy needs. The method was validated under three data scenarios: no manual\nlabels, limited labels, and extensive labels. For rectum contours in prostate\ncancer, we applied geometric surrogate labels when manual labels were absent,\ntransfer learning when limited, and direct supervision when ample labels were\navailable. Results: The BOC model delivered robust performance across all\nscenarios. Fine-tuning with just 30 manual labels and calibrating with 34\nsubjects yielded over 90% accuracy on test data. Using the calibrated\nthreshold, over 93% of the auto-contours' qualities were accurately predicted\nin over 98% of cases, reducing unnecessary manual reviews and highlighting\ncases needing correction. Conclusion: The proposed QA model enhances contouring\nefficiency in OART by reducing manual workload and enabling fast, informed\nclinical decisions. Through uncertainty quantification, it ensures safer, more\nreliable radiotherapy workflows.",
      "published": "May 01, 2025",
      "categories": [
        "cs.CV",
        "cs.AI",
        "stat.AP"
      ],
      "pdf_link": "http://arxiv.org/pdf/2505.00308v1",
      "arxiv_url": "http://arxiv.org/abs/2505.00308v1"
    },
    {
      "title": "Convergence rate for Nearest Neighbour matching: geometry of the domain\n  and higher-order regularity",
      "authors": [
        "Simon Viel",
        "Lionel Truquet",
        "Ikko Yamane"
      ],
      "abstract": "Estimating some mathematical expectations from partially observed data and in\nparticular missing outcomes is a central problem encountered in numerous fields\nsuch as transfer learning, counterfactual analysis or causal inference.\nMatching estimators, estimators based on k-nearest neighbours, are widely used\nin this context. It is known that the variance of such estimators can converge\nto zero at a parametric rate, but their bias can have a slower rate when the\ndimension of the covariates is larger than 2. This makes analysis of this bias\nparticularly important. In this paper, we provide higher order properties of\nthe bias. In contrast to the existing literature related to this problem, we do\nnot assume that the support of the target distribution of the covariates is\nstrictly included in that of the source, and we analyse two geometric\nconditions on the support that avoid such boundary bias problems. We show that\nthese conditions are much more general than the usual convex support\nassumption, leading to an improvement of existing results. Furthermore, we show\nthat the matching estimator studied by Abadie and Imbens (2006) for the average\ntreatment effect can be asymptotically efficient when the dimension of the\ncovariates is less than 4, a result only known in dimension 1.",
      "published": "April 30, 2025",
      "categories": [
        "math.ST",
        "stat.TH"
      ],
      "pdf_link": "http://arxiv.org/pdf/2504.21633v1",
      "arxiv_url": "http://arxiv.org/abs/2504.21633v1"
    },
    {
      "title": "Transfer Learning Under High-Dimensional Network Convolutional\n  Regression Model",
      "authors": [
        "Liyuan Wang",
        "Jiachen Chen",
        "Kathryn L. Lunetta",
        "Danyang Huang",
        "Huimin Cheng",
        "Debarghya Mukherjee"
      ],
      "abstract": "Transfer learning enhances model performance by utilizing knowledge from\nrelated domains, particularly when labeled data is scarce. While existing\nresearch addresses transfer learning under various distribution shifts in\nindependent settings, handling dependencies in networked data remains\nchallenging. To address this challenge, we propose a high-dimensional transfer\nlearning framework based on network convolutional regression (NCR), inspired by\nthe success of graph convolutional networks (GCNs). The NCR model incorporates\nrandom network structure by allowing each node's response to depend on its\nfeatures and the aggregated features of its neighbors, capturing local\ndependencies effectively. Our methodology includes a two-step transfer learning\nalgorithm that addresses domain shift between source and target networks, along\nwith a source detection mechanism to identify informative domains.\nTheoretically, we analyze the lasso estimator in the context of a random graph\nbased on the Erdos-Renyi model assumption, demonstrating that transfer learning\nimproves convergence rates when informative sources are present. Empirical\nevaluations, including simulations and a real-world application using Sina\nWeibo data, demonstrate substantial improvements in prediction accuracy,\nparticularly when labeled data in the target domain is limited.",
      "published": "April 28, 2025",
      "categories": [
        "cs.LG",
        "stat.ME"
      ],
      "pdf_link": "http://arxiv.org/pdf/2504.19979v2",
      "arxiv_url": "http://arxiv.org/abs/2504.19979v2"
    },
    {
      "title": "Comments on the minimal training set for CNN: a case study of the\n  frustrated $J_1$-$J_2$ Ising model on the square lattice",
      "authors": [
        "Shang-Wei Li",
        "Yuan-Heng Tseng",
        "Ming-Che Hsieh",
        "Fu-Jiun Jiang"
      ],
      "abstract": "The minimal training set to train a working CNN is explored in detail. The\nconsidered model is the frustrated $J_1$-$J_2$ Ising model on the square\nlattice. Here $J_1 < 0$ and $J_2 > 0$ are the nearest and next-to-nearest\nneighboring couplings, respectively. We train the CNN using the configurations\nof $g \\stackrel{\\text{def}}{=} J_2/|J_1| = 0.7$ and employ the resulting CNN to\nstudy the phase transition of $g = 0.8$. We find that this transfer learning is\nsuccessful. In particular, only configurations of two temperatures, one is\nbelow and one is above the critical temperature $T_c$ of $g=0.7$, are needed to\nreach accurately determination of the $T_c$ of $g=0.8$. However, it may be\nsubtle to use this strategy for the training. Specifically, for the considered\nmodel, due to the inefficiency of the single spin flip algorithm used in\nsampling the configurations at the low-temperature region, the two temperatures\nassociated with the training set should not be too far away from the $T_c$ of\n$g=0.7$, otherwise, the performance of the obtained CNN is not of high quality,\nhence cannot determine the $T_c$ of $g=0.8$ accurately. For the considered\nmodel, we also uncover the condition for training a successful CNN when only\nconfigurations of two temperatures are considered as the training set.",
      "published": "April 28, 2025",
      "categories": [
        "hep-lat",
        "cond-mat.stat-mech"
      ],
      "pdf_link": "http://arxiv.org/pdf/2504.19795v1",
      "arxiv_url": "http://arxiv.org/abs/2504.19795v1"
    },
    {
      "title": "Post-Transfer Learning Statistical Inference in High-Dimensional\n  Regression",
      "authors": [
        "Nguyen Vu Khai Tam",
        "Cao Huyen My",
        "Vo Nguyen Le Duy"
      ],
      "abstract": "Transfer learning (TL) for high-dimensional regression (HDR) is an important\nproblem in machine learning, particularly when dealing with limited sample size\nin the target task. However, there currently lacks a method to quantify the\nstatistical significance of the relationship between features and the response\nin TL-HDR settings. In this paper, we introduce a novel statistical inference\nframework for assessing the reliability of feature selection in TL-HDR, called\nPTL-SI (Post-TL Statistical Inference). The core contribution of PTL-SI is its\nability to provide valid $p$-values to features selected in TL-HDR, thereby\nrigorously controlling the false positive rate (FPR) at desired significance\nlevel $\\alpha$ (e.g., 0.05). Furthermore, we enhance statistical power by\nincorporating a strategic divide-and-conquer approach into our framework. We\ndemonstrate the validity and effectiveness of the proposed PTL-SI through\nextensive experiments on both synthetic and real-world high-dimensional\ndatasets, confirming its theoretical properties and utility in testing the\nreliability of feature selection in TL scenarios.",
      "published": "April 25, 2025",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "pdf_link": "http://arxiv.org/pdf/2504.18212v1",
      "arxiv_url": "http://arxiv.org/abs/2504.18212v1"
    },
    {
      "title": "Transfer Learning for High-dimensional Reduced Rank Time Series Models",
      "authors": [
        "Mingliang Ma Abolfazl Safikhani"
      ],
      "abstract": "The objective of transfer learning is to enhance estimation and inference in\na target data by leveraging knowledge gained from additional sources. Recent\nstudies have explored transfer learning for independent observations in\ncomplex, high-dimensional models assuming sparsity, yet research on time series\nmodels remains limited. Our focus is on transfer learning for sequences of\nobservations with temporal dependencies and a more intricate model parameter\nstructure. Specifically, we investigate the vector autoregressive model (VAR),\na widely recognized model for time series data, where the transition matrix can\nbe deconstructed into a combination of a sparse matrix and a low-rank one. We\npropose a new transfer learning algorithm tailored for estimating\nhigh-dimensional VAR models characterized by low-rank and sparse structures.\nAdditionally, we present a novel approach for selecting informative\nobservations from auxiliary datasets. Theoretical guarantees are established,\nencompassing model parameter consistency, informative set selection, and the\nasymptotic distribution of estimators under mild conditions. The latter\nfacilitates the construction of entry-wise confidence intervals for model\nparameters. Finally, we demonstrate the empirical efficacy of our methodologies\nthrough both simulated and real-world datasets.",
      "published": "April 22, 2025",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "pdf_link": "http://arxiv.org/pdf/2504.15691v1",
      "arxiv_url": "http://arxiv.org/abs/2504.15691v1"
    }
  ]
}