{
  "timestamp": 1759713567,
  "papers": [
    {
      "title": "Exactly or Approximately Wasserstein Distributionally Robust Estimation\n  According to Wasserstein Radii Being Small or Large",
      "authors": [
        "Xiao Ding",
        "Enbin Song",
        "Dunbiao Niu",
        "Zhujun Cao",
        "Qingjiang Shi"
      ],
      "abstract": "This paper primarily considers the robust estimation problem under\nWasserstein distance constraints on the parameter and noise distributions in\nthe linear measurement model with additive noise, which can be formulated as an\ninfinite-dimensional nonconvex minimax problem. We prove that the existence of\na saddle point for this problem is equivalent to that for a finite-dimensional\nminimax problem, and give a counterexample demonstrating that the saddle point\nmay not exist. Motivated by this observation, we present a verifiable necessary\nand sufficient condition whose parameters can be derived from a convex problem\nand its dual. Additionally, we also introduce a simplified sufficient\ncondition, which intuitively indicates that when the Wasserstein radii are\nsmall enough, the saddle point always exists. In the absence of the saddle\npoint, we solve an finite-dimensional nonconvex minimax problem, obtained by\nrestricting the estimator to be linear. Its optimal value establishes an upper\nbound on the robust estimation problem, while its optimal solution yields a\nrobust linear estimator. Numerical experiments are also provided to validate\nour theoretical results.",
      "published": "October 02, 2025",
      "categories": [
        "eess.SP",
        "math.OC",
        "math.ST",
        "stat.TH"
      ],
      "pdf_link": "http://arxiv.org/pdf/2510.01763v2",
      "arxiv_url": "http://arxiv.org/abs/2510.01763v2"
    },
    {
      "title": "Deep Learning Approaches with Explainable AI for Differentiating\n  Alzheimer Disease and Mild Cognitive Impairment",
      "authors": [
        "Fahad Mostafa",
        "Kannon Hossain",
        "Hafiz Khan"
      ],
      "abstract": "Early and accurate diagnosis of Alzheimer Disease is critical for effective\nclinical intervention, particularly in distinguishing it from Mild Cognitive\nImpairment, a prodromal stage marked by subtle structural changes. In this\nstudy, we propose a hybrid deep learning ensemble framework for Alzheimer\nDisease classification using structural magnetic resonance imaging. Gray and\nwhite matter slices are used as inputs to three pretrained convolutional neural\nnetworks such as ResNet50, NASNet, and MobileNet, each fine tuned through an\nend to end process. To further enhance performance, we incorporate a stacked\nensemble learning strategy with a meta learner and weighted averaging to\noptimally combine the base models. Evaluated on the Alzheimer Disease\nNeuroimaging Initiative dataset, the proposed method achieves state of the art\naccuracy of 99.21% for Alzheimer Disease vs. Mild Cognitive Impairment and\n91.0% for Mild Cognitive Impairment vs. Normal Controls, outperforming\nconventional transfer learning and baseline ensemble methods. To improve\ninterpretability in image based diagnostics, we integrate Explainable AI\ntechniques by Gradient weighted Class Activation, which generates heatmaps and\nattribution maps that highlight critical regions in gray and white matter\nslices, revealing structural biomarkers that influence model decisions. These\nresults highlight the frameworks potential for robust and scalable clinical\ndecision support in neurodegenerative disease diagnostics.",
      "published": "September 27, 2025",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "stat.AP",
        "stat.ML"
      ],
      "pdf_link": "http://arxiv.org/pdf/2510.00048v1",
      "arxiv_url": "http://arxiv.org/abs/2510.00048v1"
    },
    {
      "title": "Conditional Risk Minimization with Side Information: A Tractable,\n  Universal Optimal Transport Framework",
      "authors": [
        "Xinqiao Xie",
        "Jonathan Yu-Meng Li"
      ],
      "abstract": "Conditional risk minimization arises in high-stakes decisions where risk must\nbe assessed in light of side information, such as stressed economic conditions,\nspecific customer profiles, or other contextual covariates. Constructing\nreliable conditional distributions from limited data is notoriously difficult,\nmotivating a series of optimal-transport-based proposals that address this\nuncertainty in a distributionally robust manner. Yet these approaches remain\nfragmented, each constrained by its own limitations: some rely on point\nestimates or restrictive structural assumptions, others apply only to narrow\nclasses of risk measures, and their structural connections are unclear. We\nintroduce a universal framework for distributionally robust conditional risk\nminimization, built on a novel union-ball formulation in optimal transport.\nThis framework offers three key advantages: interpretability, by subsuming\nexisting methods as special cases and revealing their deep structural links;\ntractability, by yielding convex reformulations for virtually all major risk\nfunctionals studied in the literature; and scalability, by supporting\ncutting-plane algorithms for large-scale conditional risk problems.\nApplications to portfolio optimization with rank-dependent expected utility\nhighlight the practical effectiveness of the framework, with conditional models\nconverging to optimal solutions where unconditional ones clearly do not.",
      "published": "September 27, 2025",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.OC",
        "q-fin.PM",
        "q-fin.RM"
      ],
      "pdf_link": "http://arxiv.org/pdf/2509.23128v1",
      "arxiv_url": "http://arxiv.org/abs/2509.23128v1"
    },
    {
      "title": "Transfer Learning under Group-Label Shift: A Semiparametric Exponential\n  Tilting Approach",
      "authors": [
        "Manli Cheng",
        "Subha Maity",
        "Qinglong Tian",
        "Pengfei Li"
      ],
      "abstract": "We propose a new framework for binary classification in transfer learning\nsettings where both covariate and label distributions may shift between source\nand target domains. Unlike traditional covariate shift or label shift\nassumptions, we introduce a group-label shift assumption that accommodates\nsubpopulation imbalance and mitigates spurious correlations, thereby improving\nrobustness to real-world distributional changes. To model the joint\ndistribution difference, we adopt a flexible exponential tilting formulation\nand establish mild, verifiable identification conditions via an instrumental\nvariable strategy. We develop a computationally efficient two-step\nlikelihood-based estimation procedure that combines logistic regression for the\nsource outcome model with conditional likelihood estimation using both source\nand target covariates. We derive consistency and asymptotic normality for the\nresulting estimators, and extend the theory to receiver operating\ncharacteristic curves, the area under the curve, and other target functionals,\naddressing the nonstandard challenges posed by plug-in classifiers. Simulation\nstudies demonstrate that our method outperforms existing alternatives under\nsubpopulation shift scenarios. A semi-synthetic application using the\nwaterbirds dataset further confirms the proposed method's ability to transfer\ninformation effectively and improve target-domain classification accuracy.",
      "published": "September 26, 2025",
      "categories": [
        "stat.ME",
        "math.ST",
        "stat.TH"
      ],
      "pdf_link": "http://arxiv.org/pdf/2509.22268v1",
      "arxiv_url": "http://arxiv.org/abs/2509.22268v1"
    },
    {
      "title": "Transfer Learning in Regression with Influential Points",
      "authors": [
        "Bingbing Wang",
        "Jiaqi Wang",
        "Yu Tang"
      ],
      "abstract": "Regression prediction plays a crucial role in practical applications and\nstrongly relies on data annotation. However, due to prohibitive annotation\ncosts or domain-specific constraints, labeled data in the target domain is\noften scarce, making transfer learning a critical solution by leveraging\nknowledge from resource-rich source domains. In the practical target scenario,\nalthough transfer learning has been widely applied, influential points can\nsignificantly distort parameter estimation for the target domain model. This\nissue is further compounded when influential points are also present in source\ndomains, leading to aggravated performance degradation and posing critical\nrobustness challenges for existing transfer learning frameworks. In this study,\nwe innovatively introduce a transfer learning collaborative optimization\n(Trans-CO) framework for influential point detection and regression model\nfitting. Extensive simulation experiments demonstrate that the proposed\nTrans-CO algorithm outperforms competing methods in terms of model fitting\nperformance and influential point identification accuracy. Furthermore, it\nachieves superior predictive accuracy on real-world datasets, providing a novel\nsolution for transfer learning in regression with influential points",
      "published": "September 24, 2025",
      "categories": [
        "stat.ME"
      ],
      "pdf_link": "http://arxiv.org/pdf/2509.20272v1",
      "arxiv_url": "http://arxiv.org/abs/2509.20272v1"
    },
    {
      "title": "A Realistic Evaluation of Cross-Frequency Transfer Learning and\n  Foundation Forecasting Models",
      "authors": [
        "Kin G. Olivares",
        "Malcolm Wolff",
        "Tatiana Konstantinova",
        "Shankar Ramasubramanian",
        "Andrew Gordon Wilson",
        "Andres Potapczynski",
        "Willa Potosnak",
        "Mengfei Cao",
        "Boris Oreshkin",
        "Dmitry Efimov"
      ],
      "abstract": "Cross-frequency transfer learning (CFTL) has emerged as a popular framework\nfor curating large-scale time series datasets to pre-train foundation\nforecasting models (FFMs). Although CFTL has shown promise, current\nbenchmarking practices fall short of accurately assessing its performance. This\nshortcoming stems from many factors: an over-reliance on small-scale evaluation\ndatasets; inadequate treatment of sample size when computing summary\nstatistics; reporting of suboptimal statistical models; and failing to account\nfor non-negligible risks of overlap between pre-training and test datasets. To\naddress these limitations, we introduce a unified reimplementation of\nwidely-adopted neural forecasting networks, adapting them for the CFTL setup;\nwe pre-train only on proprietary and synthetic data, being careful to prevent\ntest leakage; and we evaluate on 15 large, diverse public forecast competition\ndatasets. Our empirical analysis reveals that statistical models' accuracy is\nfrequently underreported. Notably, we confirm that statistical models and their\nensembles consistently outperform existing FFMs by more than 8.2% in sCRPS, and\nby more than 20% MASE, across datasets. However, we also find that synthetic\ndataset pre-training does improve the accuracy of a FFM by 7% percent.",
      "published": "September 23, 2025",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.AP"
      ],
      "pdf_link": "http://arxiv.org/pdf/2509.19465v1",
      "arxiv_url": "http://arxiv.org/abs/2509.19465v1"
    },
    {
      "title": "DRO-REBEL: Distributionally Robust Relative-Reward Regression for Fast\n  and Efficient LLM Alignment",
      "authors": [
        "Sharan Sahu",
        "Martin T. Wells"
      ],
      "abstract": "Reinforcement learning with human feedback (RLHF) has become crucial for\naligning Large Language Models (LLMs) with human intent. However, existing\noffline RLHF approaches suffer from overoptimization, where models overfit to\nreward misspecification and drift from preferred behaviors observed during\ntraining. We introduce DRO-REBEL, a unified family of robust REBEL updates with\ntype-$p$ Wasserstein, KL, and $\\chi^2$ ambiguity sets. Using Fenchel duality,\neach update reduces to a simple relative-reward regression, preserving\nscalability and avoiding PPO-style clipping or auxiliary value networks. Under\nstandard linear-reward and log-linear policy classes with a data-coverage\ncondition, we establish $O(n^{-1/4})$ estimation bounds with tighter constants\nthan prior DRO-DPO approaches, and recover the minimax-optimal $O(n^{-1/2})$\nrate via a localized Rademacher complexity analysis. The same analysis closes\nthe gap for Wasserstein-DPO and KL-DPO, showing both also attain optimal\nparametric rates. We derive practical SGD algorithms for all three divergences:\ngradient regularization (Wasserstein), importance weighting (KL), and a fast\n1-D dual solve ($\\chi^2$). Experiments on Emotion Alignment, the large-scale\nArmoRM multi-objective benchmark, and HH-Alignment demonstrate strong\nworst-case robustness across unseen preference mixtures, model sizes, and data\nscales, with $\\chi^2$-REBEL showing consistently strong empirical performance.\nA controlled radius--coverage study validates a no-free-lunch trade-off: radii\nshrinking faster than empirical divergence concentration rates achieve\nminimax-optimal parametric rates but forfeit coverage, while\ncoverage-guaranteeing radii incur $O(n^{-1/4})$ rates.",
      "published": "September 23, 2025",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_link": "http://arxiv.org/pdf/2509.19104v1",
      "arxiv_url": "http://arxiv.org/abs/2509.19104v1"
    },
    {
      "title": "Asymptotically Optimal Problem-Dependent Bandit Policies for Transfer\n  Learning",
      "authors": [
        "Adrien Prevost",
        "Timothee Mathieu",
        "Odalric-Ambrym Maillard"
      ],
      "abstract": "We study the non-contextual multi-armed bandit problem in a transfer learning\nsetting: before any pulls, the learner is given N'_k i.i.d. samples from each\nsource distribution nu'_k, and the true target distributions nu_k lie within a\nknown distance bound d_k(nu_k, nu'_k) <= L_k. In this framework, we first\nderive a problem-dependent asymptotic lower bound on cumulative regret that\nextends the classical Lai-Robbins result to incorporate the transfer parameters\n(d_k, L_k, N'_k). We then propose KL-UCB-Transfer, a simple index policy that\nmatches this new bound in the Gaussian case. Finally, we validate our approach\nvia simulations, showing that KL-UCB-Transfer significantly outperforms the\nno-prior baseline when source and target distributions are sufficiently close.",
      "published": "September 23, 2025",
      "categories": [
        "cs.LG",
        "math.ST",
        "stat.TH"
      ],
      "pdf_link": "http://arxiv.org/pdf/2509.19098v1",
      "arxiv_url": "http://arxiv.org/abs/2509.19098v1"
    },
    {
      "title": "A Generative Conditional Distribution Equality Testing Framework and Its\n  Minimax Analysis",
      "authors": [
        "Siming Zheng",
        "Meifang Lan",
        "Tong Wang",
        "Yuanyuan Lin"
      ],
      "abstract": "In this paper, we propose a general framework for testing the equality of the\nconditional distributions in a two-sample problem. This problem is most\nrelevant to transfer learning under covariate shift. Our framework is built on\nneural network-based generative methods and sample splitting techniques by\ntransforming the conditional distribution testing problem into an unconditional\none. We introduce two special tests: the generative permutation-based\nconditional distribution equality test and the generative classification\naccuracy-based conditional distribution equality test. Theoretically, we\nestablish a minimax lower bound for statistical inference in testing the\nequality of two conditional distributions under certain smoothness conditions.\nWe demonstrate that the generative permutation-based conditional distribution\nequality test and its modified version can attain this lower bound precisely or\nup to some iterated logarithmic factor. Moreover, we prove the testing\nconsistency of the generative classification accuracy-based conditional\ndistribution equality test. We also establish the convergence rate for the\nlearned conditional generator by deriving new results related to the\nrecently-developed offset Rademacher complexity and approximation properties\nusing neural networks. Empirically, we conduct numerical studies including\nsynthetic datasets and two real-world datasets, demonstrating the effectiveness\nof our approach.",
      "published": "September 22, 2025",
      "categories": [
        "cs.LG",
        "math.ST",
        "stat.ME",
        "stat.TH"
      ],
      "pdf_link": "http://arxiv.org/pdf/2509.17729v2",
      "arxiv_url": "http://arxiv.org/abs/2509.17729v2"
    }
  ]
}