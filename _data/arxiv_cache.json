{
  "timestamp": 1760404732,
  "papers": [
    {
      "title": "Distributionally robust approximation property of neural networks",
      "authors": [
        "Mihriban Ceylan",
        "David J. Pr\u00f6mel"
      ],
      "abstract": "The universal approximation property uniformly with respect to weakly compact\nfamilies of measures is established for several classes of neural networks. To\nthat end, we prove that these neural networks are dense in Orlicz spaces,\nthereby extending classical universal approximation theorems even beyond the\ntraditional $L^p$-setting. The covered classes of neural networks include\nwidely used architectures like feedforward neural networks with non-polynomial\nactivation functions, deep narrow networks with ReLU activation functions and\nfunctional input neural networks.",
      "published": "October 10, 2025",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.FA",
        "math.PR"
      ],
      "pdf_link": "http://arxiv.org/pdf/2510.09177v1",
      "arxiv_url": "http://arxiv.org/abs/2510.09177v1"
    },
    {
      "title": "Structured Output Regularization: a framework for few-shot transfer\n  learning",
      "authors": [
        "Nicolas Ewen",
        "Jairo Diaz-Rodriguez",
        "Kelly Ramsay"
      ],
      "abstract": "Traditional transfer learning typically reuses large pre-trained networks by\nfreezing some of their weights and adding task-specific layers. While this\napproach is computationally efficient, it limits the model's ability to adapt\nto domain-specific features and can still lead to overfitting with very limited\ndata. To address these limitations, we propose Structured Output Regularization\n(SOR), a simple yet effective framework that freezes the internal network\nstructures (e.g., convolutional filters) while using a combination of group\nlasso and $L_1$ penalties. This framework tailors the model to specific data\nwith minimal additional parameters and is easily applicable to various network\ncomponents, such as convolutional filters or various blocks in neural networks\nenabling broad applicability for transfer learning tasks. We evaluate SOR on\nthree few shot medical imaging classification tasks and we achieve competitive\nresults using DenseNet121, and EfficientNetB4 bases compared to established\nbenchmarks.",
      "published": "October 09, 2025",
      "categories": [
        "cs.CV",
        "cs.LG",
        "stat.ML"
      ],
      "pdf_link": "http://arxiv.org/pdf/2510.08728v1",
      "arxiv_url": "http://arxiv.org/abs/2510.08728v1"
    },
    {
      "title": "Scalable deep fusion of spaceborne lidar and synthetic aperture radar\n  for global forest structural complexity mapping",
      "authors": [
        "Tiago de Conto",
        "John Armston",
        "Ralph Dubayah"
      ],
      "abstract": "Forest structural complexity metrics integrate multiple canopy attributes\ninto a single value that reflects habitat quality and ecosystem function.\nSpaceborne lidar from the Global Ecosystem Dynamics Investigation (GEDI) has\nenabled mapping of structural complexity in temperate and tropical forests, but\nits sparse sampling limits continuous high-resolution mapping. We present a\nscalable, deep learning framework fusing GEDI observations with multimodal\nSynthetic Aperture Radar (SAR) datasets to produce global, high-resolution (25\nm) wall-to-wall maps of forest structural complexity. Our adapted\nEfficientNetV2 architecture, trained on over 130 million GEDI footprints,\nachieves high performance (global R2 = 0.82) with fewer than 400,000\nparameters, making it an accessible tool that enables researchers to process\ndatasets at any scale without requiring specialized computing infrastructure.\nThe model produces accurate predictions with calibrated uncertainty estimates\nacross biomes and time periods, preserving fine-scale spatial patterns. It has\nbeen used to generate a global, multi-temporal dataset of forest structural\ncomplexity from 2015 to 2022. Through transfer learning, this framework can be\nextended to predict additional forest structural variables with minimal\ncomputational cost. This approach supports continuous, multi-temporal\nmonitoring of global forest structural dynamics and provides tools for\nbiodiversity conservation and ecosystem management efforts in a changing\nclimate.",
      "published": "October 07, 2025",
      "categories": [
        "cs.CV",
        "cs.LG",
        "stat.AP"
      ],
      "pdf_link": "http://arxiv.org/pdf/2510.06299v1",
      "arxiv_url": "http://arxiv.org/abs/2510.06299v1"
    },
    {
      "title": "Transfer Learning on Edge Connecting Probability Estimation under\n  Graphon Model",
      "authors": [
        "Yuyao Wang",
        "Yu-Hung Cheng",
        "Debarghya Mukherjee",
        "Huimin Cheng"
      ],
      "abstract": "Graphon models provide a flexible nonparametric framework for estimating\nlatent connectivity probabilities in networks, enabling a range of downstream\napplications such as link prediction and data augmentation. However, accurate\ngraphon estimation typically requires a large graph, whereas in practice, one\noften only observes a small-sized network. One approach to addressing this\nissue is to adopt a transfer learning framework, which aims to improve\nestimation in a small target graph by leveraging structural information from a\nlarger, related source graph. In this paper, we propose a novel method, namely\nGTRANS, a transfer learning framework that integrates neighborhood smoothing\nand Gromov-Wasserstein optimal transport to align and transfer structural\npatterns between graphs. To prevent negative transfer, GTRANS includes an\nadaptive debiasing mechanism that identifies and corrects for target-specific\ndeviations via residual smoothing. We provide theoretical guarantees on the\nstability of the estimated alignment matrix and demonstrate the effectiveness\nof GTRANS in improving the accuracy of target graph estimation through\nextensive synthetic and real data experiments. These improvements translate\ndirectly to enhanced performance in downstream applications, such as the graph\nclassification task and the link prediction task.",
      "published": "October 07, 2025",
      "categories": [
        "cs.LG",
        "math.ST",
        "stat.TH"
      ],
      "pdf_link": "http://arxiv.org/pdf/2510.05527v1",
      "arxiv_url": "http://arxiv.org/abs/2510.05527v1"
    },
    {
      "title": "Busemann Functions in the Wasserstein Space: Existence, Closed-Forms,\n  and Applications to Slicing",
      "authors": [
        "Cl\u00e9ment Bonet",
        "Elsa Cazelles",
        "Lucas Drumetz",
        "Nicolas Courty"
      ],
      "abstract": "The Busemann function has recently found much interest in a variety of\ngeometric machine learning problems, as it naturally defines projections onto\ngeodesic rays of Riemannian manifolds and generalizes the notion of\nhyperplanes. As several sources of data can be conveniently modeled as\nprobability distributions, it is natural to study this function in the\nWasserstein space, which carries a rich formal Riemannian structure induced by\nOptimal Transport metrics. In this work, we investigate the existence and\ncomputation of Busemann functions in Wasserstein space, which admits geodesic\nrays. We establish closed-form expressions in two important cases:\none-dimensional distributions and Gaussian measures. These results enable\nexplicit projection schemes for probability distributions on $\\mathbb{R}$,\nwhich in turn allow us to define novel Sliced-Wasserstein distances over\nGaussian mixtures and labeled datasets. We demonstrate the efficiency of those\noriginal schemes on synthetic datasets as well as transfer learning problems.",
      "published": "October 06, 2025",
      "categories": [
        "cs.LG",
        "math.MG",
        "stat.ML"
      ],
      "pdf_link": "http://arxiv.org/pdf/2510.04579v1",
      "arxiv_url": "http://arxiv.org/abs/2510.04579v1"
    },
    {
      "title": "Optimal Regularization Under Uncertainty: Distributional Robustness and\n  Convexity Constraints",
      "authors": [
        "Oscar Leong",
        "Eliza O'Reilly",
        "Yong Sheng Soh"
      ],
      "abstract": "Regularization is a central tool for addressing ill-posedness in inverse\nproblems and statistical estimation, with the choice of a suitable penalty\noften determining the reliability and interpretability of downstream solutions.\nWhile recent work has characterized optimal regularizers for well-specified\ndata distributions, practical deployments are often complicated by\ndistributional uncertainty and the need to enforce structural constraints such\nas convexity. In this paper, we introduce a framework for distributionally\nrobust optimal regularization, which identifies regularizers that remain\neffective under perturbations of the data distribution. Our approach leverages\nconvex duality to reformulate the underlying distributionally robust\noptimization problem, eliminating the inner maximization and yielding\nformulations that are amenable to numerical computation. We show how the\nresulting robust regularizers interpolate between memorization of the training\ndistribution and uniform priors, providing insights into their behavior as\nrobustness parameters vary. For example, we show how certain ambiguity sets,\nsuch as those based on the Wasserstein-1 distance, naturally induce regularity\nin the optimal regularizer by promoting regularizers with smaller Lipschitz\nconstants. We further investigate the setting where regularizers are required\nto be convex, formulating a convex program for their computation and\nillustrating their stability with respect to distributional shifts. Taken\ntogether, our results provide both theoretical and computational foundations\nfor designing regularizers that are reliable under model uncertainty and\nstructurally constrained for robust deployment.",
      "published": "October 03, 2025",
      "categories": [
        "math.OC",
        "math.MG",
        "math.ST",
        "stat.ML",
        "stat.TH"
      ],
      "pdf_link": "http://arxiv.org/pdf/2510.03464v1",
      "arxiv_url": "http://arxiv.org/abs/2510.03464v1"
    },
    {
      "title": "Bayesian Transfer Learning for High-Dimensional Linear Regression via\n  Adaptive Shrinkage",
      "authors": [
        "Parsa Jamshidian",
        "Donatello Telesca"
      ],
      "abstract": "We introduce BLAST, Bayesian Linear regression with Adaptive Shrinkage for\nTransfer, a Bayesian multi-source transfer learning framework for\nhigh-dimensional linear regression. The proposed analytical framework leverages\nglobal-local shrinkage priors together with Bayesian source selection to\nbalance information sharing and regularization. We show how Bayesian source\nselection allows for the extraction of the most useful data sources, while\ndiscounting biasing information that may lead to negative transfer. In this\nframework, both source selection and sparse regression are jointly accounted\nfor in prediction and inference via Bayesian model averaging. The structure of\nour model admits efficient posterior simulation via a Gibbs sampling algorithm\nallowing full posterior inference for the target regression coefficients,\nmaking BLAST both computationally practical and inferentially straightforward.\nOur method achieves more accurate posterior inference for the target than\nregularization approaches based on target data alone, while offering\ncompetitive predictive performance and superior uncertainty quantification\ncompared to current state-of-the-art transfer learning methods. We validate its\neffectiveness through extensive simulation studies and illustrate its\nanalytical properties when applied to a case study on the estimation of tumor\nmutational burden from gene expression, using data from The Cancer Genome Atlas\n(TCGA).",
      "published": "October 03, 2025",
      "categories": [
        "stat.ME",
        "stat.AP",
        "stat.CO"
      ],
      "pdf_link": "http://arxiv.org/pdf/2510.03449v1",
      "arxiv_url": "http://arxiv.org/abs/2510.03449v1"
    },
    {
      "title": "Exactly or Approximately Wasserstein Distributionally Robust Estimation\n  According to Wasserstein Radii Being Small or Large",
      "authors": [
        "Xiao Ding",
        "Enbin Song",
        "Dunbiao Niu",
        "Zhujun Cao",
        "Qingjiang Shi"
      ],
      "abstract": "This paper primarily considers the robust estimation problem under\nWasserstein distance constraints on the parameter and noise distributions in\nthe linear measurement model with additive noise, which can be formulated as an\ninfinite-dimensional nonconvex minimax problem. We prove that the existence of\na saddle point for this problem is equivalent to that for a finite-dimensional\nminimax problem, and give a counterexample demonstrating that the saddle point\nmay not exist. Motivated by this observation, we present a verifiable necessary\nand sufficient condition whose parameters can be derived from a convex problem\nand its dual. Additionally, we also introduce a simplified sufficient\ncondition, which intuitively indicates that when the Wasserstein radii are\nsmall enough, the saddle point always exists. In the absence of the saddle\npoint, we solve an finite-dimensional nonconvex minimax problem, obtained by\nrestricting the estimator to be linear. Its optimal value establishes an upper\nbound on the robust estimation problem, while its optimal solution yields a\nrobust linear estimator. Numerical experiments are also provided to validate\nour theoretical results.",
      "published": "October 02, 2025",
      "categories": [
        "eess.SP",
        "math.OC",
        "math.ST",
        "stat.TH"
      ],
      "pdf_link": "http://arxiv.org/pdf/2510.01763v2",
      "arxiv_url": "http://arxiv.org/abs/2510.01763v2"
    }
  ]
}