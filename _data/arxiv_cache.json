{
  "timestamp": 1748568202,
  "papers": [
    {
      "title": "GLAMP: An Approximate Message Passing Framework for Transfer Learning\n  with Applications to Lasso-based Estimators",
      "authors": [
        "Longlin Wang",
        "Yanke Song",
        "Kuanhao Jiang",
        "Pragya Sur"
      ],
      "abstract": "Approximate Message Passing (AMP) algorithms enable precise characterization\nof certain classes of random objects in the high-dimensional limit, and have\nfound widespread applications in fields such as statistics, deep learning,\ngenetics, and communications. However, existing AMP frameworks cannot\nsimultaneously handle matrix-valued iterates and non-separable denoising\nfunctions. This limitation prevents them from precisely characterizing\nestimators that draw information from multiple data sources with distribution\nshifts. In this work, we introduce Generalized Long Approximate Message Passing\n(GLAMP), a novel extension of AMP that addresses this limitation. We rigorously\nprove state evolution for GLAMP. GLAMP significantly broadens the scope of AMP,\nenabling the analysis of transfer learning estimators that were previously out\nof reach. We demonstrate the utility of GLAMP by precisely characterizing the\nrisk of three Lasso-based transfer learning estimators: the Stacked Lasso, the\nModel Averaging Estimator, and the Second Step Estimator. We also demonstrate\nthe remarkable finite sample accuracy of our theory via extensive simulations.",
      "published": "May 28, 2025",
      "categories": [
        "math.ST",
        "stat.ML",
        "stat.TH"
      ],
      "pdf_link": "http://arxiv.org/pdf/2505.22594v1",
      "arxiv_url": "http://arxiv.org/abs/2505.22594v1"
    }
  ]
}