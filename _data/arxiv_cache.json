{
  "timestamp": 1755566668,
  "papers": [
    {
      "title": "Deconfounding via Profiled Transfer Learning",
      "authors": [
        "Ziyuan Chen",
        "Yifan Jiang",
        "Jingyuan Liu",
        "Fang Yao"
      ],
      "abstract": "Unmeasured confounders are a major source of bias in regression-based effect\nestimation and causal inference. In this paper, we advocate a new profiled\ntransfer learning framework, ProTrans, to address confounding effects in the\ntarget dataset, when additional source datasets that possess similar\nconfounding structures are available. We introduce the concept of profiled\nresiduals to characterize the shared confounding patterns between source and\ntarget datasets. By incorporating these profiled residuals into the target\ndebiasing step, we effectively mitigates the latent confounding effects. We\nalso propose a source selection strategy to enhance robustness of ProTrans\nagainst noninformative sources. As a byproduct, ProTrans can also be utilized\nto estimate treatment effects when potential confounders exist, without the use\nof auxiliary features such as instrumental or proxy variables, which are often\nchallenging to select in practice. Theoretically, we prove that the resulting\nestimated model shift from sources to target is confounding-free without any\nassumptions imposed on the true confounding structure, and that the target\nparameter estimation achieves the minimax optimal rate under mild conditions.\nSimulated and real-world experiments validate the effectiveness of ProTrans and\nsupport the theoretical findings.",
      "published": "August 15, 2025",
      "categories": [
        "stat.ME"
      ],
      "pdf_link": "http://arxiv.org/pdf/2508.11622v1",
      "arxiv_url": "http://arxiv.org/abs/2508.11622v1"
    },
    {
      "title": "Holistic Bioprocess Development Across Scales Using Multi-Fidelity Batch\n  Bayesian Optimization",
      "authors": [
        "Adrian Martens",
        "Mathias Neufang",
        "Alessandro Butt\u00e9",
        "Moritz von Stosch",
        "Antonio del Rio Chanona",
        "Laura Marie Helleckes"
      ],
      "abstract": "Bioprocesses are central to modern biotechnology, enabling sustainable\nproduction in pharmaceuticals, specialty chemicals, cosmetics, and food.\nHowever, developing high-performing processes is costly and complex, requiring\niterative, multi-scale experimentation from microtiter plates to pilot\nreactors. Conventional Design of Experiments (DoE) approaches often struggle to\naddress process scale-up and the joint optimization of reaction conditions and\nbiocatalyst selection.\n  We propose a multi-fidelity batch Bayesian optimization framework to\naccelerate bioprocess development and reduce experimental costs. The method\nintegrates Gaussian Processes tailored for multi-fidelity modeling and\nmixed-variable optimization, guiding experiment selection across scales and\nbiocatalysts. A custom simulation of a Chinese Hamster Ovary bioprocess,\ncapturing non-linear and coupled scale-up dynamics, is used for benchmarking\nagainst multiple simulated industrial DoE baselines. Multiple case studies show\nhow the proposed workflow can achieve a reduction in experimental costs and\nincreased yield.\n  This work provides a data-efficient strategy for bioprocess optimization and\nhighlights future opportunities in transfer learning and uncertainty-aware\ndesign for sustainable biotechnology.",
      "published": "August 14, 2025",
      "categories": [
        "q-bio.QM",
        "stat.ML"
      ],
      "pdf_link": "http://arxiv.org/pdf/2508.10970v1",
      "arxiv_url": "http://arxiv.org/abs/2508.10970v1"
    },
    {
      "title": "Empirical Bayes for Data Integration",
      "authors": [
        "Paul Rognon-Vael",
        "David Rossell"
      ],
      "abstract": "We discuss the use of empirical Bayes for data integration, in the sense of\ntransfer learning. Our main interest is in settings where one wishes to learn\nstructure (e.g. feature selection) and one only has access to incomplete data\nfrom previous studies, such as summaries, estimates or lists of relevant\nfeatures. We discuss differences between full Bayes and empirical Bayes, and\ndevelop a computational framework for the latter. We discuss how empirical\nBayes attains consistent variable selection under weaker conditions (sparsity\nand betamin assumptions) than full Bayes and other standard criteria do, and\nhow it attains faster convergence rates. Our high-dimensional regression\nexamples show that fully Bayesian inference enjoys excellent properties, and\nthat data integration with empirical Bayes can offer moderate yet meaningful\nimprovements in practice.",
      "published": "August 10, 2025",
      "categories": [
        "stat.ME"
      ],
      "pdf_link": "http://arxiv.org/pdf/2508.08336v1",
      "arxiv_url": "http://arxiv.org/abs/2508.08336v1"
    },
    {
      "title": "Model Recycling Framework for Multi-Source Data-Free Supervised Transfer\n  Learning",
      "authors": [
        "Sijia Wang",
        "Ricardo Henao"
      ],
      "abstract": "Increasing concerns for data privacy and other difficulties associated with\nretrieving source data for model training have created the need for source-free\ntransfer learning, in which one only has access to pre-trained models instead\nof data from the original source domains. This setting introduces many\nchallenges, as many existing transfer learning methods typically rely on access\nto source data, which limits their direct applicability to scenarios where\nsource data is unavailable. Further, practical concerns make it more difficult,\nfor instance efficiently selecting models for transfer without information on\nsource data, and transferring without full access to the source models. So\nmotivated, we propose a model recycling framework for parameter-efficient\ntraining of models that identifies subsets of related source models to reuse in\nboth white-box and black-box settings. Consequently, our framework makes it\npossible for Model as a Service (MaaS) providers to build libraries of\nefficient pre-trained models, thus creating an opportunity for multi-source\ndata-free supervised transfer learning.",
      "published": "August 04, 2025",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_link": "http://arxiv.org/pdf/2508.02039v1",
      "arxiv_url": "http://arxiv.org/abs/2508.02039v1"
    }
  ]
}