{
  "last_fetch_timestamp": 1767750080,
  "papers": [
    {
      "arxiv_id": "2601.01642",
      "title": "Wasserstein Distributionally Robust Rare-Event Simulation",
      "authors": [
        "Dohyun Ahn",
        "Huiyi Chen",
        "Lewen Zheng"
      ],
      "abstract": "Standard rare-event simulation techniques require exact distributional specifications, which limits their effectiveness in the presence of distributional uncertainty. To address this, we develop a novel framework for estimating rare-event probabilities subject to such distributional model risk. Specifically, we focus on computing worst-case rare-event probabilities, defined as a distributionally robust bound against a Wasserstein ambiguity set centered at a specific nominal distribution. By exploiting a dual characterization of this bound, we propose Distributionally Robust Importance Sampling (DRIS), a computationally tractable methodology designed to substantially reduce the variance associated with estimating the dual components. The proposed method is simple to implement and requires low sampling costs. Most importantly, it achieves vanishing relative error, the strongest efficiency guarantee that is notoriously difficult to establish in rare-event simulation. Our numerical studies confirm the superior performance of DRIS against existing benchmarks.",
      "published": "January 04, 2026",
      "published_raw": "2026-01-04T19:15:22Z",
      "categories": [
        "stat.ME",
        "q-fin.CP",
        "stat.CO"
      ],
      "pdf_link": "https://arxiv.org/pdf/2601.01642v1",
      "arxiv_url": "https://arxiv.org/abs/2601.01642v1",
      "added_timestamp": 1767750080
    },
    {
      "arxiv_id": "2512.16748",
      "title": "Shift-Aware Gaussian-Supremum Validation for Wasserstein-DRO CVaR Portfolios",
      "authors": [
        "Derek Long"
      ],
      "abstract": "We study portfolio selection with a Conditional Value-at-Risk (CVaR) constraint under distribution shift and serial dependence. While Wasserstein distributionally robust optimization (DRO) offers tractable protection via an ambiguity ball around empirical data, choosing the ball radius is delicate: large radii are conservative, small radii risk violation under regime change. We propose a shift-aware Gaussian-supremum (GS) validation framework for Wasserstein-DRO CVaR portfolios, building on the work by Lam and Qian (2019). Phase I of the framework generates a candidate path by solving the exact reformulation of the robust CVaR constraint over a grid of Wasserstein radii. Phase II of the framework learns a target deployment law $Q$ by density-ratio reweighting of a time-ordered validation fold, computes weighted CVaR estimates, and calibrates a simultaneous upper confidence band via a block multiplier bootstrap to account for dependence. We select the least conservative feasible portfolio (or abstain if the effective sample size collapses). Theoretically, we extend the normalized GS validator to non-i.i.d. financial data: under weak dependence and regularity of the weighted scores, any portfolio passing our validator satisfies the CVaR limit under $Q$ with probability at least $1-\u03b2$; the Wasserstein term contributes a deterministic margin $(\u03b4/\u03b1)\\|x\\|_*$. Empirical results indicate improved return-risk trade-offs versus the naive baseline.",
      "published": "December 18, 2025",
      "published_raw": "2025-12-18T16:44:50Z",
      "categories": [
        "stat.ME"
      ],
      "pdf_link": "https://arxiv.org/pdf/2512.16748v1",
      "arxiv_url": "https://arxiv.org/abs/2512.16748v1",
      "added_timestamp": 1766193903
    },
    {
      "arxiv_id": "2512.13069",
      "title": "Multi-fidelity aerodynamic data fusion by autoencoder transfer learning",
      "authors": [
        "Javier Nieto-Centenero",
        "Esther Andr\u00e9s",
        "Rodrigo Castellanos"
      ],
      "abstract": "Accurate aerodynamic prediction often relies on high-fidelity simulations; however, their prohibitive computational costs severely limit their applicability in data-driven modeling. This limitation motivates the development of multi-fidelity strategies that leverage inexpensive low-fidelity information without compromising accuracy. Addressing this challenge, this work presents a multi-fidelity deep learning framework that combines autoencoder-based transfer learning with a newly developed Multi-Split Conformal Prediction (MSCP) strategy to achieve uncertainty-aware aerodynamic data fusion under extreme data scarcity. The methodology leverages abundant Low-Fidelity (LF) data to learn a compact latent physics representation, which acts as a frozen knowledge base for a decoder that is subsequently fine-tuned using scarce HF samples. Tested on surface-pressure distributions for NACA airfoils (2D) and a transonic wing (3D) databases, the model successfully corrects LF deviations and achieves high-accuracy pressure predictions using minimal HF training data. Furthermore, the MSCP framework produces robust, actionable uncertainty bands with pointwise coverage exceeding 95%. By combining extreme data efficiency with uncertainty quantification, this work offers a scalable and reliable solution for aerodynamic regression in data-scarce environments.",
      "published": "December 15, 2025",
      "published_raw": "2025-12-15T08:06:52Z",
      "categories": [
        "cs.LG",
        "physics.flu-dyn",
        "stat.ML"
      ],
      "pdf_link": "https://arxiv.org/pdf/2512.13069v1",
      "arxiv_url": "https://arxiv.org/abs/2512.13069v1",
      "added_timestamp": 1766090526
    },
    {
      "arxiv_id": "2512.12795",
      "title": "TRACER: Transfer Learning based Real-time Adaptation for Clinical Evolving Risk",
      "authors": [
        "Mengying Yan",
        "Ziye Tian",
        "Siqi Li",
        "Nan Liu",
        "Benjamin A. Goldstein",
        "Molei Liu",
        "Chuan Hong"
      ],
      "abstract": "Clinical decision support tools built on electronic health records often experience performance drift due to temporal population shifts, particularly when changes in the clinical environment initially affect only a subset of patients, resulting in a transition to mixed populations. Such case-mix changes commonly arise following system-level operational updates or the emergence of new diseases, such as COVID-19. We propose TRACER (Transfer Learning-based Real-time Adaptation for Clinical Evolving Risk), a framework that identifies encounter-level transition membership and adapts predictive models using transfer learning without full retraining. In simulation studies, TRACER outperformed static models trained on historical or contemporary data. In a real-world application predicting hospital admission following emergency department visits across the COVID-19 transition, TRACER improved both discrimination and calibration. TRACER provides a scalable approach for maintaining robust predictive performance under evolving and heterogeneous clinical conditions.",
      "published": "December 14, 2025",
      "published_raw": "2025-12-14T18:23:22Z",
      "categories": [
        "cs.LG",
        "stat.ME"
      ],
      "pdf_link": "https://arxiv.org/pdf/2512.12795v1",
      "arxiv_url": "https://arxiv.org/abs/2512.12795v1",
      "added_timestamp": 1766090526
    },
    {
      "arxiv_id": "2512.12550",
      "title": "Iterative Sampling Methods for Sinkhorn Distributionally Robust Optimization",
      "authors": [
        "Jie Wang"
      ],
      "abstract": "Distributionally robust optimization (DRO) has emerged as a powerful paradigm for reliable decision-making under uncertainty. This paper focuses on DRO with ambiguity sets defined via the Sinkhorn discrepancy: an entropy-regularized Wasserstein distance, referred to as Sinkhorn DRO. Existing work primarily addresses Sinkhorn DRO from a dual perspective, leveraging its formulation as a conditional stochastic optimization problem, for which many stochastic gradient methods are applicable. However, the theoretical analyses of such methods often rely on the boundedness of the loss function, and it is indirect to obtain the worst-case distribution associated with Sinkhorn DRO. In contrast, we study Sinkhorn DRO from the primal perspective, by reformulating it as a bilevel program with several infinite-dimensional lower-level subproblems over probability space. This formulation enables us to simultaneously obtain the optimal robust decision and the worst-case distribution, which is valuable in practical settings, such as generating stress-test scenarios or designing robust learning algorithms. We propose both double-loop and single-loop sampling-based algorithms with theoretical guarantees to solve this bilevel program. Finally, we demonstrate the effectiveness of our approach through a numerical study on adversarial classification.",
      "published": "December 14, 2025",
      "published_raw": "2025-12-14T04:42:51Z",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "pdf_link": "https://arxiv.org/pdf/2512.12550v1",
      "arxiv_url": "https://arxiv.org/abs/2512.12550v1",
      "added_timestamp": 1766090526
    },
    {
      "arxiv_id": "2512.08176",
      "title": "Worst-case generation via minimax optimization in Wasserstein space",
      "authors": [
        "Xiuyuan Cheng",
        "Yao Xie",
        "Linglingzhi Zhu",
        "Yunqin Zhu"
      ],
      "abstract": "Worst-case generation plays a critical role in evaluating robustness and stress-testing systems under distribution shifts, in applications ranging from machine learning models to power grids and medical prediction systems. We develop a generative modeling framework for worst-case generation for a pre-specified risk, based on min-max optimization over continuous probability distributions, namely the Wasserstein space. Unlike traditional discrete distributionally robust optimization approaches, which often suffer from scalability issues, limited generalization, and costly worst-case inference, our framework exploits the Brenier theorem to characterize the least favorable (worst-case) distribution as the pushforward of a transport map from a continuous reference measure, enabling a continuous and expressive notion of risk-induced generation beyond classical discrete DRO formulations. Based on the min-max formulation, we propose a Gradient Descent Ascent (GDA)-type scheme that updates the decision model and the transport map in a single loop, establishing global convergence guarantees under mild regularity assumptions and possibly without convexity-concavity. We also propose to parameterize the transport map using a neural network that can be trained simultaneously with the GDA iterations by matching the transported training samples, thereby achieving a simulation-free approach. The efficiency of the proposed method as a risk-induced worst-case generator is validated by numerical experiments on synthetic and image data.",
      "published": "December 09, 2025",
      "published_raw": "2025-12-09T02:11:08Z",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.OC"
      ],
      "pdf_link": "https://arxiv.org/pdf/2512.08176v1",
      "arxiv_url": "https://arxiv.org/abs/2512.08176v1",
      "added_timestamp": 1766090526
    },
    {
      "arxiv_id": "2512.06511",
      "title": "Diagnosis-based mortality prediction for intensive care unit patients via transfer learning",
      "authors": [
        "Mengqi Xu",
        "Subha Maity",
        "Joel Dubin"
      ],
      "abstract": "In the intensive care unit, the underlying causes of critical illness vary substantially across diagnoses, yet prediction models accounting for diagnostic heterogeneity have not been systematically studied. To address the gap, we evaluate transfer learning approaches for diagnosis-specific mortality prediction and apply both GLM- and XGBoost-based models to the eICU Collaborative Research Database. Our results demonstrate that transfer learning consistently outperforms models trained only on diagnosis-specific data and those using a well-known ICU severity-of-illness score, i.e., APACHE IVa, alone, while also achieving better calibration than models trained on the pooled data. Our findings also suggest that the Youden cutoff is a more appropriate decision threshold than the conventional 0.5 for binary outcomes, and that transfer learning maintains consistently high predictive performance across various cutoff criteria.",
      "published": "December 06, 2025",
      "published_raw": "2025-12-06T17:46:18Z",
      "categories": [
        "cs.LG",
        "stat.AP"
      ],
      "pdf_link": "https://arxiv.org/pdf/2512.06511v1",
      "arxiv_url": "https://arxiv.org/abs/2512.06511v1",
      "added_timestamp": 1766090526
    },
    {
      "arxiv_id": "2512.03208",
      "title": "Uncertainty Quantification for Large Language Model Reward Learning under Heterogeneous Human Feedback",
      "authors": [
        "Pangpang Liu",
        "Junwei Lu",
        "Will Wei Sun"
      ],
      "abstract": "We study estimation and statistical inference for reward models used in aligning large language models (LLMs). A key component of LLM alignment is reinforcement learning from human feedback (RLHF), where humans compare pairs of model-generated answers and their preferences are used to train a reward model. However, human feedback is inherently heterogeneous, creating significant challenges for reliable reward learning. To address this, we adopt a heterogeneous preference framework that jointly models the latent reward of answers and human rationality. This leads to a challenging biconvex optimization problem, which we solve via an alternating gradient descent algorithm. We establish theoretical guarantees for the resulting estimator, including its convergence and asymptotic distribution. These results enable the construction of confidence intervals for reward estimates. Leveraging these uncertainty quantification results, we conduct valid statistical comparisons between rewards and incorporate uncertainty into the best-of-$N$ (BoN) policy framework. Extensive simulations demonstrate the effectiveness of our method, and applications to real LLM data highlight the practical value of accounting for uncertainty in reward modeling for LLM alignment.",
      "published": "December 02, 2025",
      "published_raw": "2025-12-02T20:22:25Z",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "pdf_link": "https://arxiv.org/pdf/2512.03208v1",
      "arxiv_url": "https://arxiv.org/abs/2512.03208v1",
      "added_timestamp": 1766090526
    },
    {
      "arxiv_id": "2512.05139",
      "title": "Spatiotemporal Satellite Image Downscaling with Transfer Encoders and Autoregressive Generative Models",
      "authors": [
        "Yang Xiang",
        "Jingwen Zhong",
        "Yige Yan",
        "Petros Koutrakis",
        "Eric Garshick",
        "Meredith Franklin"
      ],
      "abstract": "We present a transfer-learning generative downscaling framework to reconstruct fine resolution satellite images from coarse scale inputs. Our approach combines a lightweight U-Net transfer encoder with a diffusion-based generative model. The simpler U-Net is first pretrained on a long time series of coarse resolution data to learn spatiotemporal representations; its encoder is then frozen and transferred to a larger downscaling model as physically meaningful latent features. Our application uses NASA's MERRA-2 reanalysis as the low resolution source domain (50 km) and the GEOS-5 Nature Run (G5NR) as the high resolution target (7 km). Our study area included a large area in Asia, which was made computationally tractable by splitting into two subregions and four seasons. We conducted domain similarity analysis using Wasserstein distances confirmed minimal distributional shift between MERRA-2 and G5NR, validating the safety of parameter frozen transfer. Across seasonal regional splits, our model achieved excellent performance (R2 = 0.65 to 0.94), outperforming comparison models including deterministic U-Nets, variational autoencoders, and prior transfer learning baselines. Out of data evaluations using semivariograms, ACF/PACF, and lag-based RMSE/R2 demonstrated that the predicted downscaled images preserved physically consistent spatial variability and temporal autocorrelation, enabling stable autoregressive reconstruction beyond the G5NR record. These results show that transfer enhanced diffusion models provide a robust and physically coherent solution for downscaling a long time series of coarse resolution images with limited training periods. This advancement has significant implications for improving environmental exposure assessment and long term environmental monitoring.",
      "published": "December 01, 2025",
      "published_raw": "2025-12-01T06:00:45Z",
      "categories": [
        "cs.CV",
        "cs.LG",
        "stat.ML"
      ],
      "pdf_link": "https://arxiv.org/pdf/2512.05139v1",
      "arxiv_url": "https://arxiv.org/abs/2512.05139v1",
      "added_timestamp": 1766090526
    },
    {
      "arxiv_id": "2512.00239",
      "title": "Self-Supervised Dynamical System Representations for Physiological Time-Series",
      "authors": [
        "Yenho Chen",
        "Maxwell A. Xu",
        "James M. Rehg",
        "Christopher J. Rozell"
      ],
      "abstract": "The effectiveness of self-supervised learning (SSL) for physiological time series depends on the ability of a pretraining objective to preserve information about the underlying physiological state while filtering out unrelated noise. However, existing strategies are limited due to reliance on heuristic principles or poorly constrained generative tasks. To address this limitation, we propose a pretraining framework that exploits the information structure of a dynamical systems generative model across multiple time-series. This framework reveals our key insight that class identity can be efficiently captured by extracting information about the generative variables related to the system parameters shared across similar time series samples, while noise unique to individual samples should be discarded. Building on this insight, we propose PULSE, a cross-reconstruction-based pretraining objective for physiological time series datasets that explicitly extracts system information while discarding non-transferrable sample-specific ones. We establish theory that provides sufficient conditions for the system information to be recovered, and empirically validate it using a synthetic dynamical systems experiment. Furthermore, we apply our method to diverse real-world datasets, demonstrating that PULSE learns representations that can broadly distinguish semantic classes, increase label efficiency, and improve transfer learning.",
      "published": "November 28, 2025",
      "published_raw": "2025-11-28T22:53:31Z",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_link": "https://arxiv.org/pdf/2512.00239v1",
      "arxiv_url": "https://arxiv.org/abs/2512.00239v1",
      "added_timestamp": 1766090526
    },
    {
      "arxiv_id": "2511.21221",
      "title": "Portfolio Optimization via Transfer Learning",
      "authors": [
        "Kexin Wang",
        "Xiaomeng Zhang",
        "Xinyu Zhang"
      ],
      "abstract": "Recognizing that asset markets generally exhibit shared informational characteristics, we develop a portfolio strategy based on transfer learning that leverages cross-market information to enhance the investment performance in the market of interest by forward validation. Our strategy asymptotically identifies and utilizes the informative datasets, selectively incorporating valid information while discarding the misleading information. This enables our strategy to achieve the maximum Sharpe ratio asymptotically. The promising performance is demonstrated by numerical studies and case studies of two portfolios: one consisting of stocks dual-listed in A-shares and H-shares, and another comprising equities from various industries of the United States.",
      "published": "November 26, 2025",
      "published_raw": "2025-11-26T09:52:06Z",
      "categories": [
        "q-fin.PM",
        "stat.AP"
      ],
      "pdf_link": "https://arxiv.org/pdf/2511.21221v1",
      "arxiv_url": "https://arxiv.org/abs/2511.21221v1",
      "added_timestamp": 1766090526
    },
    {
      "arxiv_id": "2511.20704",
      "title": "Pretraining Transformer-Based Models on Diffusion-Generated Synthetic Graphs for Alzheimer's Disease Prediction",
      "authors": [
        "Abolfazl Moslemi",
        "Hossein Peyvandi"
      ],
      "abstract": "Early and accurate detection of Alzheimer's disease (AD) is crucial for enabling timely intervention and improving outcomes. However, developing reliable machine learning (ML) models for AD diagnosis is challenging due to limited labeled data, multi-site heterogeneity, and class imbalance. We propose a Transformer-based diagnostic framework that combines diffusion-based synthetic data generation with graph representation learning and transfer learning. A class-conditional denoising diffusion probabilistic model (DDPM) is trained on the real-world NACC dataset to generate a large synthetic cohort that mirrors multimodal clinical and neuroimaging feature distributions while balancing diagnostic classes. Modality-specific Graph Transformer encoders are first pretrained on this synthetic data to learn robust, class-discriminative representations and are then frozen while a neural classifier is trained on embeddings from the original NACC data. We quantify distributional alignment between real and synthetic cohorts using metrics such as Maximum Mean Discrepancy (MMD), Frechet distance, and energy distance, and complement discrimination metrics with calibration and fixed-specificity sensitivity analyses. Empirically, our framework outperforms standard baselines, including early and late fusion deep neural networks and the multimodal graph-based model MaGNet, yielding higher AUC, accuracy, sensitivity, and specificity under subject-wise cross-validation on NACC. These results show that diffusion-based synthetic pretraining with Graph Transformers can improve generalization in low-sample, imbalanced clinical prediction settings.",
      "published": "November 24, 2025",
      "published_raw": "2025-11-24T19:34:53Z",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_link": "https://arxiv.org/pdf/2511.20704v1",
      "arxiv_url": "https://arxiv.org/abs/2511.20704v1",
      "added_timestamp": 1766090526
    },
    {
      "arxiv_id": "2511.16500",
      "title": "Scenario-based Regularization: A Tractable Framework for Distributionally Robust Stochastic Optimization",
      "authors": [
        "Diego Fonseca",
        "Mauricio Junca"
      ],
      "abstract": "We propose a flexible scenario-based regularized Sample Average Approximation (SBR-SAA) framework for stochastic optimization. This work is motivated by challenges in standard Wasserstein Distributionally Robust Optimization (WDRO), where out-of-sample performance, particularly tail risk, is sensitive to the choice of the p-norm, and formulations can be computationally intractable. Our method is inspired by the asymptotic expansion of the WDRO objective and introduces a regularizer that penalizes the (sub)gradient norm of the objective at a selected set of scenarios. This framework serves a dual purpose: (i) it provides a computationally tractable alternative to WDRO by using a representative subset of the data, and (ii) it can provide targeted robustness by incorporating user-defined adverse scenarios. We establish the theoretical properties of this framework by proving its equivalence to a decision-dependent WDRO problem, from which we derive finite sample guarantees and asymptotic consistency. We demonstrate the method's efficacy in two applications: (1) a multi-product newsvendor problem, where SBR-SAA serves as a tractable alternative to NP-hard WDRO, and (2) a mean-risk portfolio optimization problem, where it successfully uses historical crisis data to improve out-of-sample performance.",
      "published": "November 20, 2025",
      "published_raw": "2025-11-20T16:19:35Z",
      "categories": [
        "math.OC",
        "math.PR",
        "stat.CO"
      ],
      "pdf_link": "https://arxiv.org/pdf/2511.16500v1",
      "arxiv_url": "https://arxiv.org/abs/2511.16500v1",
      "added_timestamp": 1766090526
    },
    {
      "arxiv_id": "2511.15561",
      "title": "Variance-reduced extreme value index estimators using control variates in a semi-supervised setting",
      "authors": [
        "Louison Bocquet-Nouaille",
        "J\u00e9r\u00f4me Morio",
        "Benjamin Bobbia"
      ],
      "abstract": "The estimation of the Extreme Value Index (EVI) is fundamental in extreme value analysis but suffers from high variance due to reliance on only a few extreme observations. We propose a control variates based transfer learning approach in a semi-supervised framework, where a small set of coupled target and source observations is combined with abundant unpaired source data. By expressing the Hill estimator of the target EVI as a ratio of means, we apply approximate control variates to both numerator and denominator, with jointly optimized coefficients that guarantee variance reduction without introducing bias. We show theoretically and through simulations that the asymptotic relative variance reduction of the transferred Hill estimator is proportional to the tail dependence between the target and source variables and independent of their EVI values. Thus, substantial variance reduction can be achieved even without similarity in tail heaviness of the target and source distributions. The proposed approach can be extended to other EVI estimators expressed with ratio of means, as demonstrated on the moment estimator. The practical value of the proposed method is illustrated on multi-fidelity water surge and ice accretion datasets.",
      "published": "November 19, 2025",
      "published_raw": "2025-11-19T15:54:42Z",
      "categories": [
        "stat.ME",
        "math.ST"
      ],
      "pdf_link": "https://arxiv.org/pdf/2511.15561v1",
      "arxiv_url": "https://arxiv.org/abs/2511.15561v1",
      "added_timestamp": 1766090526
    },
    {
      "arxiv_id": "2511.15236",
      "title": "Testing relevant difference in high-dimensional linear regression with applications to detect transferability",
      "authors": [
        "Xu Liu"
      ],
      "abstract": "Most of researchers on testing a significance of coefficient $\\ubeta$ in high-dimensional linear regression models consider the classical hypothesis testing problem $H_0^{c}: \\ubeta=\\uzero \\mbox{ versus } H_1^{c}: \\ubeta \\neq \\uzero$. We take a different perspective and study the testing problem with the null hypothesis of no relevant difference between $\\ubeta$ and $\\uzero$, that is, $H_0: \\|\\ubeta\\|\\leq \u03b4_0 \\mbox{ versus } H_1: \\|\\ubeta\\|> \u03b4_0$, where $\u03b4_0$ is a prespecified small constant. This testing problem is motivated by the urgent requirement to detect the transferability of source data in the transfer learning framework. We propose a novel test procedure incorporating the estimation of the largest eigenvalue of a high-dimensional covariance matrix with the assistance of the random matrix theory. In the more challenging setting in the presence of high-dimensional nuisance parameters, we establish the asymptotic normality for the proposed test statistics under both the null and alternative hypotheses. By applying the proposed test approaches to detect the transferability of source data, the unified transfer learning models simultaneously achieve lower estimation and prediction errors with comparison to existing methods. We study the finite-sample properties of the new test by means of simulation studies and illustrate its performance by analyzing the GTEx data.",
      "published": "November 19, 2025",
      "published_raw": "2025-11-19T08:44:56Z",
      "categories": [
        "stat.ME"
      ],
      "pdf_link": "https://arxiv.org/pdf/2511.15236v1",
      "arxiv_url": "https://arxiv.org/abs/2511.15236v1",
      "added_timestamp": 1766090526
    }
  ]
}